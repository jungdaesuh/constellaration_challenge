# Gaussian process surrogate with physical law-corrected prior for multi-coupled PDEs defined on irregular geometry

#### Pucheng Tang [a], Hongqiao Wang [b] [,] [d], Qian Chen [c], Wenzhou Lin [c] [∗], Heng Yong [c] _a School of Artificial Intelligence_ _Wuhan University, Wuhan 430072, China_ _b School of Mathematics and Statistics_ _Central South University, Changsha 410083, China_ _c Institute of Applied Physics and Computational Mathematics_ _Beijing 100094, China_ _d Institute of Mathematics, Henan Academy of Sciences_ _Zhengzhou 450046, China_ **Abstract**

Parametric partial differential equations (PDEs) are fundamental mathematical tools for modeling complex physical systems, yet their numerical evaluation across parameter spaces remains
computationally intensive when using conventional high-fidelity solvers. To address this challenge, we propose a novel physical law-corrected prior Gaussian process (LC-prior GP) surrogate modeling framework that effectively integrates data-driven learning with underlying physical constraints to flexibly handle multi-coupled variables defined on complex geometries. The
proposed approach leverages proper orthogonal decomposition (POD) to parameterize highdimensional PDE solutions via their dominant modes and associated coefficients, thereby en
                                                        abling efficient Gaussian process (GP) surrogate modeling within a reduced-dimensional coeffi
cient space. A key contribution lies in the incorporation of physical laws together with a limited
number of parameter samples to correct the GP posterior mean, thus avoiding reliance on computationally expensive numerical solvers. Furthermore, interpolation functions are constructed
to describe the mapping from the full parameter space to the physics-based correction term.
This mapping is subsequently backpropagated to constrain the original GP surrogate, yielding
a more physically consistent conditional prior. To handle irregular geometries, the radial basis


1


function-finite difference (RBF-FD) method is incorporated during training set computation,
with its inherent differentiation matrices providing both computational efficiency and numerical accuracy for physical constraint optimization. The effectiveness of the proposed method
is demonstrated through numerical experiments involving reaction-diffusion model, miscible
flooding models and Navier-Stokes equations with multi-physics coupling defined on irregular domains. Additionally, the framework’s applicability to parameter estimation is verified
through multi parameters miscible flooding models and Korteweg-de Vries equation.
**Keywords** : Parametric partial differential equations; Gaussian process regression; Physical
laws; Surrogate model; Radial basis function finite difference; Parameter estimation

### **1 Introduction**


Parametric differential equations (DEs), including parametric ordinary differential equations (ODEs) and parametric partial differential equations (PDEs), are fundamental tools for
modeling a wide range of scientific and engineering phenomena, particularly in systems with
uncertain or complex dynamics [1]. These parameters may arise from physical properties, geometric configurations, environmental factors, or other system characteristics. The inclusion
of parameters in DEs is crucial for enabling uncertainty quantification, sensitivity analysis, and
optimization in real-world applications. For example, in fluid dynamics, parameters such as
viscosity and the Reynolds number govern flow behavior, whereas in structural mechanics, material stiffness and damping coefficients determine system stability. Moreover, parametric DEs
facilitate the study of how small input variations propagate through a system, supporting robust
design and control strategies. Techniques such as parameter estimation and inverse problems
rely on these models to infer unknown quantities from observed data. Neglecting parametric
dependencies may lead to model oversimplification, resulting in inaccurate predictions or suboptimal performance in critical applications. Accurate estimation of unknown parameter values
and quantification of their effects on specific systems typically require extensive computational
sampling, often involving thousands to millions of realizations. Traditional mesh-based numerical methods, while ensuring solution accuracy, incur prohibitive computational costs that
hinder their applicability to complex engineering scenarios [2–6]. In recent years, physicsinformed neural networks and neural operator methods under deep learning frameworks have
emerged as prominent research focuses [7, 8]. These approaches exhibit superior capabilities
over conventional mesh-based methods in handling complex dynamical systems with multiphysics coupling. However, when solving parametric DEs, neural operators demand large
training datasets consisting of parameter-solution pairs, which still require repeated calls to
computationally expensive high-fidelity numerical solvers. Consequently, these methods often
underperform in small-data regimes or scenarios where training data are scarce. Furthermore,
operator learning approaches demonstrate limited generalization capability and reduced accuracy for certain classes of nonlinear, multi-scale PDEs, particularly for complex problems
defined on irregular geometric domains [9–11].


2


To address these challenges, surrogate modeling techniques have emerged as a powerful
approach to reduce the computational cost of predictive modeling for large-scale, complex systems. Surrogate models are computationally efficient approximations of high-fidelity models
that can accurately capture system behavior while maintaining controlled accuracy. These datadriven models are constructed based on assumptions about the underlying functional form and
utilize system response data for training. Recent advances in machine learning have led to
widespread adoption of techniques such as neural network (NN) and Gaussian process (GP)
for constructing surrogate models [12]. For instance, Chen et al. [13] developed a data-free
surrogate model based on deep neural networks for solving PDEs, while Radaideh et al. [14]
integrated surrogate modeling with deep Gaussian processes. To further improve robustness,
physics-informed approaches have been incorporated into surrogate modeling frameworks.
Pestourie et al. [15] proposed physics-enhanced deep surrogates for PDEs that combines highfidelity solutions with low-fidelity solutions, achieving higher accuracy with limited training
data. Similarly, Li et al. [16] introduced an offline-online computational strategy that combines Markov chain Monte Carlo (MCMC) methods with physics-informed neural networks
for Bayesian inverse problems. Recent studies have demonstrated that GP/kernel-based methods can also be competitive. Mora et al. [8] leveraged the strengths of both deep neural networks and kernel methods, proposed an operator learning with Gaussian processes to learn the
mapping from input function space to solution space. However, the numerical experiments in
this article require a relatively high training cost, and the predictive performance on complex
geometries has not been demonstrated. Therefore, addressing problems flexibly under limited
sample availability still poses a significant challenge.
In recent years, as researchers address increasingly complex systems, traditional full-order
surrogate modeling approaches have exhibited growing computational inefficiencies. Consequently, the integration of reduced basis (RB) methods with surrogate modeling has emerged
as a dominant paradigm. RB methods, belonging to the meta-modeling family, are widely employed as surrogates for parameterized large-scale systems [17–20]. The core principle of RB
methods is to identify a low-dimensional subspace within the high-dimensional solution space
and project the governing equations onto this reduced space, thereby enabling computationally
efficient solutions. Among the most prevalent RB techniques is proper orthogonal decomposition (POD) [21, 22], also known as the Karhunen-Lo`eve expansion or principal component
analysis (PCA) in certain contexts. POD operates on a set of solution vectors (snapshots) to
derive an optimal basis for a reduced subspace. Its key advantage lies in its ability to truncate the basis optimally, retaining only the most energetically significant modes. Alternative
basis construction methods include proper generalized decomposition [23, 24], balanced truncation [25], and rational interpolation [26]. Recently, the combination of RB techniques with
data-driven models has shown promise for solving large-scale, complex systems. A major benefit of these hybrid methods is their independence from direct access to or modification of the
original high-fidelity model’s governing equations. For example, the hybrid POD-neural network approach proposed in [27, 28] employs a neural network to predict the low-dimensional


3


Figure 1: A Schematic of LC-prior GP for differential equations.


projection coefficients of the RB model. Song et al. [29] proposed a model reduction method
in which the Koopman operator is extracted from the training set via dynamic mode decomposition (DMD), and the features of a subset of training samples are linearly combined using
a nearest-neighbor weighting strategy to enable prediction. As a purely data-driven approach,
this method achieves accurate predictions while significantly reducing computational costs.
However, a significant limitation of these methods is their reliance on more model evaluations than intrusive approaches to initially construct a reliable surrogate model, particularly for
multi-coupled parametric systems.
Inspired by the aforementioned POD method, data-driven Gaussian process surrogate and
physics-informed method, we link these three approaches and propose a novel physical lawcorrected prior GP (LC-prior GP) as shown in Fig.1. Firstly, we concatenate the training data
into a snapshot matrix and apply POD to reduce the infinite-dimensional DEs solution space to
a low-dimensional basis and coefficient space that can accurately capture the most important
features of the original parametric DEs system. Subsequently, the GP surrogates model are
constructed to map parameters in DEs to the coefficients of the reduced basis function, significantly reducing the computational cost associated with directly training the surrogate models
in the original DEs solution space. By constructing the snapshot matrix, we aim to fully utilize
these known samples and achieve effective prediction of unknown parametric systems through
extraction and modeling of their key features - a process entirely driven by data. However, when
dealing with limited training samples or extrapolation predictions, the accuracy of such datadriven models faces significant limitations. To address this drawback, we propose a physical
law corrected method that leverages governing equations to refine predictions on small testing sets. Specifically, we optimize correction terms between surrogate model predictions and


4


physics-constrained optimal solutions. These correction terms are then characterized across the
entire parameter space using interpolation functions, which are backpropagated to the original
GP surrogate. This integration establishes a more physically reasonable prior expectation function that simultaneously satisfies both physical laws and training data constraints for standard
data-driven GP surrogate.
To ensure flexible application of our proposed method to irregular computational domains,
we employ the radial basis function finite difference (RBF-FD) method for forward simulations when generating training data for the surrogate model. RBF-FD method is a mesh-free
numerical discretization method that extended from traditional finite difference methods. In
contrast to conventional finite difference methods or finite element methods, RBF-FD does
not rely on structured grids. Instead, it constructs differentiation weights through radial basis
functions at local stencil, thereby achieving high-precision discretization of PDEs [30–32].
The core concept of RBF-FD lies in leveraging the interpolation properties of radial basis
functions to establish local approximation models over arbitrarily distributed nodes within the
computational domain. This approach is especially advantageous for irregular geometries, dynamic boundaries, and high-dimensional problems, circumventing the challenges associated
with mesh generation in complex domains using traditional grid-based methods. Furthermore,
RBF-FD exhibits high accuracy and flexible scalability, enabling the incorporation of various radial basis functions (e.g., Gaussian, multiquadric) to accommodate diverse numerical
requirements. Moreover, when solving parametric DEs using RBF-FD method, the evaluation
and differentiation matrices characterized by local stencil basis functions depend solely on the
equation’s domain definition and nodes positions, remaining independent of system parameters. Leveraging this advantage, our physical law correction of data-driven surrogate models
employs RBF-constructed matrices to approximate differential operators governing the equation solutions, significantly reducing both computational errors and time costs associated with
numerical differentiation methods.
In this framework, when addressing parameter estimation inverse problems, the surrogate
model eliminates the need for multiple queries to expensive numerical solvers. Instead, it uses
the existing surrogate model to predict the coefficients of the reduced basis, which can then be
employed to reconstruct solutions for DEs with different parameters and compute the likelihood
function. We apply the Markov Chain Monte Carlo (MCMC) method to obtain posterior samples of the parameters from the likelihood function, facilitating uncertainty quantification (UQ)
and measurement of the unknown parameters. Our approach integrates data-driven machine

                                                        learning techniques with physical laws within the RB framework, ensuring computational effi
ciency in low-dimensional spaces while maintaining the surrogate model’s accuracy with small
sample sizes.
The main contributions of this paper are summarized as follows:


  - We propose a novel approach that combines proper orthogonal decomposition and GP
surrogate for the construction of infinite-dimensional function surrogate models. By fully
utilizing the available data in both parametric representation and the surrogate model con

5


struction, this method significantly enhances the approximation accuracy of the function.


  - For the standard GP surrogate model, we propose a physical law correction for the prior
expectation function. This model enables joint modeling of multiple physical variables
while satisfying the constraints of the law in the context of DEs problems, that improving
the prediction capability of data-driven surrogates and enhancing their generalizability in
small sample settings.


  - During the computation of the training set, we choose the meshless RBF-FD. Its combination with the POD method enables flexible handling of irregular geometry. Additionally, since the evaluation and differentiation matrices are represented by radial basis
functions, repeated solving is unnecessary in the physical law correction part, greatly
improving computational efficiency and accuracy during optimization.


  - Building on the LC-prior GP, we infer the posterior distribution of unknown parameters
from noisy observational data within a Bayesian framework, enabling effective uncertainty quantification. The posterior samples obtained through our method provide more
accurate estimates of the parameters.


The structure of the paper is organized as follows: Section.2 formulates the problem setup
and surrogate model construction based on parametric DEs system. In Section.3, we introduce proper orthogonal decomposition for parameterizing infinite-dimensional solutions and
propose a physical law corrected prior function to improve the pure data-driven GP performance. We also briefly explains the parameter estimation process under LC-prior GP. Section.4
presents numerical examples, ranging from single-physical-quantity cases to multi-dimensional
parameters with coupled multiple physical quantities to demonstrate the effectiveness of proposed method. Section.4 concludes with remarks and a discussion of future directions.

### **2 Problem setting in parametric PDEs**


Parametric DEs are fundamental tools for modeling a wide range of phenomena in science
and engineering. They describe how quantities change over time or space and are essential
for understanding the underlying mechanisms of nature. Let Ω be the computational domain
with the Lipschitz continuous boundary ∂Ω. The general form for differential equation with
parameters can be expressed as:

F [�] **u** ( _**x**_ ; θ), ∇ **u**, ∇ [2] **u**, · · · ; θ [�] = **f** ( _**x**_ ; θ), _**x**_ ∈ Ω,
(1)
 G [�] **u** ( _**x**_ ; θ), ∇ **u**, ∇ [2] **u**, · · · ; θ [�] = **g** ( _**x**_ ; θ), _**x**_ ∈ ∂Ω.


where **u** ( _**x**_ ; θ) is the solution vector and θ = (θ 1, . . ., θ _q_ ) _[T]_ ∈ R _[q]_ is the vector of parameters. The operator function F and G are represents a linear or nonlinear function involving
**u** and differential operator acting on **u** defined in computational domain Ω and boundary


6


∂Ω respectively. **f** ( _**x**_ ; θ) and **g** ( _**x**_ ; θ) are source term functions. In the following, we denote
F ( **u**, ∇ **u**, ∇ [2] **u**, · · · ; θ) by the short-hand notation F ( **u** ; θ). We can define an operator from θ
to infinity-dimensional solution space of the DEs by :


M : θ ∈ R _[q]_ �→ **u** ( _**x**_ ; θ).

#### **2.1 RBF-FD method**


Radial basis function (RBF) methods represent a class of mesh-free techniques that can be
classified into several categories, including local RBF methods, global RBF methods, RBFFD methods and RBF spectral methods. This study focuses specifically on the local RBFFD approach integrated with the least squares technique. The local least squares RBF-FD
method enables spatial discretization of PDEs without requiring a predefined mesh structure.
This approach offers significant flexibility in handling complex geometries and is particularly
effective for solving problems in arbitrarily shaped domains. The methodological details of the
local RBF-FD scheme and its application to spatial discretization of parametric PDEs will be
systematically presented in later sections.
We begin by reviewing fundamental concepts of RBF interpolation, which form the basis
for the RBF-FD methodology. Consider a set of scattered nodes _**x**_ _i_ ∈ R _[d]_, _i_ = 1, 2, . . ., _n_
distributed in the neighborhood of a point _**x**_, where these nodes are completely independent
of any mesh or element structure. A localized RBF approximation of the function _u_ ( _**x**_ ) can be
constructed using the radial basis functions ϕ(∥ _**x**_ − _**x**_ _i_ ∥),



_u_ _h_ ( _**x**_ ) =



_n_


_c_ _i_ ϕ(∥ _**x**_ − _**x**_ _i_ ∥),

�

_i_ =1



where ϕ(∥· ∥) is some radial function, ∥· ∥ is the standard Euclidean norm, and _c_ _i_ are unknown
coefficients. Using the interpolation condition _u_ _h_ ( _**x**_ _i_ ) = _u_ ( _**x**_ _i_ ), we can obtain a linear system as:



ϕ(∥ _**x**_ 1 − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ 1 − _**x**_ _n_ ∥)

... ... ...

ϕ(∥ _**x**_ _n_ − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ _n_ − _**x**_ _n_ ∥)







_c_ 1

...

_c_ _n_



ϕ(∥ _**x**_ 1 − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ 1 − _**x**_ _n_ ∥)

... ... ...

ϕ(∥ _**x**_ _n_ − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ _n_ − _**x**_ _n_ ∥)


����������������������������������������������������������������������������������
_A_



_c_ 1

...

 _c_ _n_ 


����

**c**







=







_u_ ( _**x**_ 1 )

...

 _u_ ( _**x**_ _n_ )


��������

**u**



_u_ ( _**x**_ 1 )

...

_u_ ( _**x**_ _n_ )



.



Let **c** = ( _c_ 1, . . ., _c_ _n_ ) _[T]_ and **u** = ( _u_ ( _**x**_ 1 ), . . ., _u_ ( _**x**_ _n_ )) _[T]_, we have the compact form _A_ **c** = **u** .
To avoid the parameter optimization, we choose the piecewise smooth polyharmonic splines
(PHS) for simplicity. Since PHS-RBF is a conditionally positive definite function, the interpolation using pure RBFs cannot guarantee convergence and solvability. By augmenting the
approximation with a polynomial basis whose degree matches the order of conditional positive
definiteness and enforcing orthogonality between the RBF coefficients **c** and this polynomial


7


basis, we ensure that the quadratic form **c** _[T]_ _A_ **c** becomes strictly positive definite. This property
is crucial for establishing optimality results. The resulting RBF interpolation takes the form:


_n_ _m_


_u_ _h_ ( _**x**_ ) = _c_ _i_ ϕ(∥ _**x**_ − _**x**_ _i_ ∥) + β _k_ _p_ _k_ ( _**x**_ ),

� �

_i_ =1 _k_ =1

_n_ (2)


_c_ _i_ _p_ _k_ ( _**x**_ _i_ ) = 0, _k_ = 1, · · ·, _m_

�

 _i_ =1



_m_
�



β _k_ _p_ _k_ ( _**x**_ ),

_k_ =1



_u_ _h_ ( _**x**_ ) =



_n_
�



_c_ _i_ ϕ(∥ _**x**_ − _**x**_ _i_ ∥) +

_i_ =1



_n_ (2)


_c_ _i_ _p_ _k_ ( _**x**_ _i_ ) = 0, _k_ = 1, · · ·, _m_

�

_i_ =1



_n_
�



The dimension _m_ of the polynomial space is given by _m_ = � D _m_ _d_ + _d_ �, where D _m_ is the degree of
the polynomial and _d_ is the spatial dimension of R _[d]_ . The coefficients _c_ _i_ and β _k_ are determined
by the collocation method and the additional constraints. The numerical optimal solution can
be expressed as the solution of the following linear system:


ϕ(∥ _**x**_ 1 − _**x**_ 1 ∥)     - · · ϕ(∥ _**x**_ 1 − _**x**_ _n_ ∥) _p_ 1 ( _**x**_ 1 )     - · · _p_ _m_ ( _**x**_ 1 ) _c_ 1 _u_ ( _**x**_ 1 )

... ... ... ... ... ... ... ...

 ϕ(∥ _**x**_ _n_ − _**x**_ 1 ∥)                                                   - · · ϕ(∥ _**x**_ _n_ − _**x**_ _n_ ∥) _p_ 1 ( _**x**_ _n_ )                                                   - · · _p_ _m_ ( _**x**_ _n_ )   _c_ _n_  _u_ ( _**x**_ _n_ ) 
   =   .
~~~~ _p_ 1 ( _**x**_ 1 )                - · · _p_ 1 ( _**x**_ _n_ ) 0                - · · 0 ~~~~ ~~~~ β 1 ~~~~ 0 ~~~~


... ... ... ... ... ... ... ...

 _p_ _m_ ( _**x**_ 1 )                                       - · · _p_ _m_ ( _**x**_ _n_ ) 0                                       - · · 0   β _m_   0 



_c_ 1

...

_c_ _n_
β 1

...

β _m_





~~~~




ϕ(∥ _**x**_ 1 − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ 1 − _**x**_ _n_ ∥) _p_ 1 ( _**x**_ 1 ) - · · _p_ _m_ ( _**x**_ 1 )

... ... ... ... ... ...

ϕ(∥ _**x**_ _n_ − _**x**_ 1 ∥) - · · ϕ(∥ _**x**_ _n_ − _**x**_ _n_ ∥) _p_ 1 ( _**x**_ _n_ ) - · · _p_ _m_ ( _**x**_ _n_ )
_p_ 1 ( _**x**_ 1 )  - · · _p_ 1 ( _**x**_ _n_ ) 0  - · · 0

... ... ... ... ... ...

_p_ _m_ ( _**x**_ 1 )  - · · _p_ _m_ ( _**x**_ _n_ ) 0  - · · 0





~~~~






~~~~








_u_ ( _**x**_ 1 )

...

_u_ ( _**x**_ _n_ )



=





~~~~




0

...

0



.



ˆ
Let β = (β 1, · · ·, β _m_ ) _[T]_, we also have the compact form as _A_ [ˆ] _c_ = ˆ **u** :

_A_ _P_ **c** **u**
� _P_ _[T]_ **0** � �β� = �0� .



�



**c**
�β



�



**u**

=
0
�



�



.



����������
_A_ ˆ



����
**c** ˆ



����
**u** ˆ



In order to digitally discretize the problem Eq.(1), two sets of computational points are
distributed over computational domain Ω:


  - The interpolation point set _Y_ = { _**y**_ _i_ } _i_ _[N]_ =1 [for generating the cardinal functions.]


  - The evaluation point set _X_ = { _**x**_ _j_ } _[M]_ _j_ =1 [for sampling the PDE, and] _[ M]_ [ =] _[ qN]_ [.]


In the dataset _Y_, each data node _**y**_ _i_ corresponds to a local support domain which contributes
to the stencil _Y_ _s_ = { _**y**_ _i_ _[s]_ [}] _[n]_ _i_ =1 [.]



_m_


β _k_ _p_ _k_ ( _**y**_ ), if _**y**_ ∈ _X_ _s_

�

_k_ =1



_u_ _[s]_
_h_ [(] _**[y]**_ [)][ =]



_n_


_c_ _i_ ϕ(∥ _**y**_ − _**y**_ _i_ _[s]_ [∥][)][ +]

�

_i_ =1



To evaluate the RBF-FD approximation at a point _**x**_, we choose the stencil associated with
the point _**y**_ _i_ that is closest to _**x**_ . That is,


_s_ ( _**x**_ ) = arg min ∥ _**x**_ − _**y**_ _i_ ∥. (3)
_i_


8


(a) Interpolation point set _Y_ (b) Evaluation point set _X_


Figure 2: The interpolation point set _Y_ and evaluation point set _X_ .


Any point _**x**_ ∈ Ω is uniquely with one stencil _Y_ _s_ by Eq.(3). Then the local interpolation for
the solution of the PDE problem evaluated at the point _**x**_ with local stencil can be written as:



_m_
� β _k_ _[s]_ _[p]_ _[k]_ [(] _**[x]**_ [)]

_k_ =1



_u_ _[s]_
_h_ [(] _**[x]**_ [)][ =]



_n_


_c_ _[s]_
_i_ [ϕ][(][∥] _**[x]**_ [ −] _**[y]**_ _i_ _[s]_ [∥][)][ +]

�

_i_ =1



( _s_ )
_**c**_
= �ϕ(∥ _**x**_ − _**y**_ 1 _[s]_ [∥][)][,] . . ., ϕ(∥ _**x**_ − _**y**_ _n_ _[s]_ [∥][)][,] _p_ 1 ( _**x**_ ), - · ·, _p_ _m_ ( _**x**_ )� [�]

β [(] _[s]_ [)]



�



�



_A_ ( _s_ ) _P_ [(] _[s]_ [)]
= �ϕ(∥ _**x**_ − _**y**_ 1 _[s]_ [∥][)][,] . . ., ϕ(∥ _**x**_ − _**y**_ _n_ _[s]_ [∥][)][,] _p_ 1 ( _**x**_ ), - · ·, _p_ _m_ ( _**x**_ )� [�] ( _P_ [(] _[s]_ [)] ) _[T]_ **0**



−1
_**u**_ [(] _[s]_ [)]
_h_
**0**
� �



_**u**_ [(] _[s]_ [)]
= �Φ 1 _[s]_ [(] _**[x]**_ [)][,] Φ 2 _[s]_ [(] _**[x]**_ [)][,] . . ., Φ _n_ _[s]_ [(] _**[x]**_ [)][,] δ 1 _[s]_ [(] _**[x]**_ [)][,] δ 2 _[s]_ [(] _**[x]**_ [)][,] - · ·, δ _m_ _[s]_ [(] _**[x]**_ [)] � [�] **0** _h_



�



=



_n_


Φ _i_ _[s]_ [(] _**[x]**_ [)] _[u]_ _[h]_ [(] _**[y]**_ _i_ _[s]_ [)][,] where _**y**_ _i_ _[s]_ [∈] _[Y]_ _[s]_

�

_i_ =1



where the function Φ _[s]_
_i_ [(][·][) and][ δ] _k_ _[s]_ [(][·][) can by written as:]


_A_ ( _s_ ) _P_ [(] _[s]_ [)]
Φ _[s]_
_i_ [(] _**[x]**_ [)][ =][ ϕ][(][∥] _**[x]**_ [ −] _**[y]**_ _i_ _[s]_ [∥][)]
�( _P_ [(] _[s]_ [)] ) _[T]_ **0**


_A_ ( _s_ ) _P_ [(] _[s]_ [)]
δ _[s]_
_k_ [(] _**[x]**_ [)][ =] _[ p]_ _[k]_ [(] _**[x]**_ [)]
�( _P_ [(] _[s]_ [)] ) _[T]_ **0**



−1

, _i_ = 1, · · ·, _n_
�


−1

, _k_ = 1, · · ·, _m_
�



For ease of analysis, we define the global function of the whole computational domain:



Φ _i_ ( _**x**_ ) =



Φ _i_ _[s]_ [(] _**[x]**_ [)][,] _**y**_ _i_ ∈ _Y_ _s_
0 _**y**_ _i_ � _Y_ _s_ .



9


For any point from evaluation point set _X_, we can use the global basis function Φ _i_ (·) and Eq.(3)
to construct the interpolation function:



_u_ _h_ ( _**x**_ ) =


We construct a sparse global linear system:



_N_


Φ _i_ ( _**x**_ ) _u_ _h_ ( _**y**_ _i_ )

�

_i_ =1



Φ 1 ( _**x**_ 1 ) - · · Φ _N_ ( _**x**_ 1 )

... ... ...

Φ 1 ( _**x**_ _M_ ) - · · Φ _N_ ( _**x**_ _M_ )



Φ 1 ( _**x**_ 1 ) - · · Φ _N_ ( _**x**_ 1 )

... ... ...

Φ 1 ( _**x**_ _M_ ) - · · Φ _N_ ( _**x**_ _M_ )


��������������������������������������������������������
_E_ _h_ ( _X_, _Y_ )



_u_ _h_ ( _**y**_ 1 )

...

 _u_ _h_ ( _**y**_ _N_ )


����������
_u_ _h_ ( _Y_ )







_u_ _h_ ( _**y**_ 1 )

...

_u_ _h_ ( _**y**_ _N_ )







=



_u_ _h_ ( _**x**_ 1 )

...

 _u_ _h_ ( _**x**_ _M_ )


������������
_u_ _h_ ( _X_ )







_u_ _h_ ( _**x**_ 1 )

...

_u_ _h_ ( _**x**_ _M_ )



.



And we have the compact form: _u_ _h_ ( _X_ ) = _E_ _h_ ( _X_, _Y_ ) _u_ _h_ ( _Y_ ). The expression for the action of a
differential operator L on the RBF approximation _u_ _h_ ( _**x**_ ) as follow:



L _u_ _h_ ( _**x**_ ) =



_N_


LΦ _i_ ( _**x**_ ) _u_ _h_ ( _**y**_ _i_ ),

�

_i_ =1



In the same way of the global sampling set _X_ :


L _u_ _h_ ( _X_ ) = _D_ [L] _h_ [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [)][.]


The evaluation matrix _E_ _h_ ( _X_, _Y_ ) and differentiation matrix _D_ [L] _h_ [(] _[X]_ [,] _[ Y]_ [) are both] _[ M]_ [ ×] _[ N]_ [ sparse]
matrices. Using the radial basis function method for the spatial discretization of the Eq.(1)
system, we can obtain:

F ( _E_ _h_ ( _X_, _Y_ ) _u_ _h_ ( _Y_ ), _D_ [∇] _h_ [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [)][,] _[ D]_ [∇] _h_ [2] [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [)][,][ · · ·][ ;][ θ][)][ =] **[ f]** [(] _[X]_ [;][ θ][)][,] _X_ ∈ Ω
 G( _E_ _h_ ( _X_, _Y_ ) _u_ _h_ ( _Y_ ), _D_ [∇] _h_ [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [)][,] _[ D]_ [∇] _h_ [2] [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [)][,][ · · ·][ ;][ θ][)][ =] **[ g]** [(] _[X]_ [;][ θ][)][,] _X_ ∈ ∂Ω



In order to solve the solution _u_ _h_ ( _Y_ ), we can construct a linear system and use least squares
method. For the sake of intuitive expression, the boundary points are not distinguished here:


F (Φ 1 ( _**x**_ 1 ), ∇Φ 1 ( _**x**_ 1 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 1 ), ∇Φ _N_ ( _**x**_ 1 ), · · · ; θ) _u_ _h_ ( _**y**_ 1 ) **f** ( _**x**_ 1 ; θ)
F (Φ 1 ( _**x**_ 2 ), ∇Φ 1 ( _**x**_ 2 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 2 ), ∇Φ _N_ ( _**x**_ 2 ), · · · ; θ) _u_ _h_ ( _**y**_ 2 ) **f** ( _**x**_ 2 ; θ)


=

... ... ... ... ...

 F (Φ 1 ( _**x**_ _M_ ), ∇Φ 1 ( _**x**_ _M_ ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ _M_ ), ∇Φ _N_ ( _**x**_ _M_ ), · · · ; θ)   _u_ _h_ ( _**y**_ _N_ )   **f** ( _**x**_ _M_ ; θ) 



F (Φ 1 ( _**x**_ 1 ), ∇Φ 1 ( _**x**_ 1 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 1 ), ∇Φ _N_ ( _**x**_ 1 ), · · · ; θ)
F (Φ 1 ( _**x**_ 2 ), ∇Φ 1 ( _**x**_ 2 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 2 ), ∇Φ _N_ ( _**x**_ 2 ), · · · ; θ)

... ... ...

 F (Φ 1 ( _**x**_ _M_ ), ∇Φ 1 ( _**x**_ _M_ ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ _M_ ), ∇Φ _N_ ( _**x**_ _M_ ), · · · ; θ) 


����������������������������������������������������������������������������������������������������������������������������������������������������������������������������������
_M_ × _N_



F (Φ 1 ( _**x**_ 1 ), ∇Φ 1 ( _**x**_ 1 ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ 1 ), ∇Φ _N_ ( _**x**_ 1 ), · · · ; θ)
F (Φ 1 ( _**x**_ 2 ), ∇Φ 1 ( _**x**_ 2 ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ 2 ), ∇Φ _N_ ( _**x**_ 2 ), · · · ; θ)

... ... ...

F (Φ 1 ( _**x**_ _M_ ), ∇Φ 1 ( _**x**_ _M_ ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ _M_ ), ∇Φ _N_ ( _**x**_ _M_ ), · · · ; θ)







_u_ _h_ ( _**y**_ 1 )
_u_ _h_ ( _**y**_ 2 )

...

 _u_ _h_ ( _**y**_ _N_ ) 


����������������
_N_ ×1



_u_ _h_ ( _**y**_ 1 )
_u_ _h_ ( _**y**_ 2 )

...

_u_ _h_ ( _**y**_ _N_ )



=







**f** ( _**x**_ 1 ; θ)
**f** ( _**x**_ 2 ; θ)

...

 **f** ( _**x**_ _M_ ; θ) 


��������������������
_M_ ×1



**f** ( _**x**_ 1 ; θ)
**f** ( _**x**_ 2 ; θ)

...

**f** ( _**x**_ _M_ ; θ)



After calculating the interpolation point domain _u_ _h_ ( _Y_ ), the evaluation point domain can be
calculated by evaluation matrix _E_ _h_ ( _X_, _Y_ ): **u** ( _X_ ) = _E_ _h_ ( _X_, _Y_ ) _u_ _h_ ( _Y_ ).


10


The RBF-FD method is equally applicable for solving time-dependent dynamic PDEs. o
illustrate this capability, we examine the following generalized temporal equation form:


∂
_**x**_ ∈ Ω × (0, _T_ ],
∂ _t_ **[u]** [(] _**[x]**_ [,] _[ t]_ [;][ θ][)][ =][ F][ �] **[u]** [,][ ∇] **[u]** [,][ ∇] [2] **[u]** [,][ · · ·][ ;][ θ][�][,] (4)

 **u** ( _**x**_, 0; θ) = **u** 0 ( _**x**_ ; θ),



∂
_**x**_ ∈ Ω × (0, _T_ ],
∂ _t_ **[u]** [(] _**[x]**_ [,] _[ t]_ [;][ θ][)][ =][ F][ �] **[u]** [,][ ∇] **[u]** [,][ ∇] [2] **[u]** [,][ · · ·][ ;][ θ][�][,]



∂ _t_ (4)

**u** ( _**x**_, 0; θ) = **u** 0 ( _**x**_ ; θ),



The construction procedures for radial basis function, evaluation and differentiation matrices
remain rigorously consistent with the aforementioned methodology. The essential distinction
manifests primarily in the matrix assembly strategy of the linear solving system:


∂
_h_ [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [,] _[ t]_ _[n]_ [)][,] _[ D]_ [∇] _h_ [2] [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [,] _[ t]_ _[n]_ [)][,][ · · ·][ ;][ θ][)]
∂ _t_ _[E]_ _[h]_ [(] _[X]_ [,] _[ Y]_ [)] **[u]** [(] _[Y]_ [,] _[ t]_ _[n]_ [+][1] [;][ θ][)][ =][ F][ (] _[E]_ _[h]_ [(] _[X]_ [,] _[ Y]_ [)] _[u]_ _[h]_ [(] _[Y]_ [,] _[ t]_ _[n]_ [)][,] _[ D]_ [∇]



same as the matrix linear system:


Φ 1 ( _**x**_ 1 )  - · · Φ _N_ ( _**x**_ 1 ) ( **u** ( _**y**_
Φ 1 ( _**x**_ 2 )  - · · Φ _N_ ( _**x**_ 2 ) ( **u** ( _**y**_

... ... ...

Φ 1 ( _**x**_ _M_ )       - · · Φ _N_ ( _**x**_ _M_ ) ( **u** ( _**y**_



Φ 1 ( _**x**_ 1 ) - · · Φ _N_ ( _**x**_ 1 )
Φ 1 ( _**x**_ 2 ) - · · Φ _N_ ( _**x**_ 2 )

... ... ...

Φ 1 ( _**x**_ _M_ ) - · · Φ _N_ ( _**x**_ _M_ )


��������������������������������������������������������
_M_ × _N_



( **u** ( _**y**_ 1, _t_ _n_ +1 ) − **u** ( _**y**_ 1, _t_ _n_ ))/∆ _t_
( **u** ( _**y**_ 2, _t_ _n_ +1 ) − **u** ( _**y**_ 2, _t_ _n_ ))/∆ _t_

...

( **u** ( _**y**_ _N_, _t_ _n_ +1 ) − **u** ( _**y**_ _N_, _t_ _n_ ))/∆ _t_ 


������������������������������������������������������������������
_N_ ×1







Φ 1 ( _**x**_ 1 ) - · · Φ _N_ ( _**x**_ 1 )
Φ 1 ( _**x**_ 2 ) - · · Φ _N_ ( _**x**_ 2 )

... ... ...

Φ 1 ( _**x**_ _M_ ) - · · Φ _N_ ( _**x**_ _M_ )



( **u** ( _**y**_ 1, _t_ _n_ +1 ) − **u** ( _**y**_ 1, _t_ _n_ ))/∆ _t_
( **u** ( _**y**_ 2, _t_ _n_ +1 ) − **u** ( _**y**_ 2, _t_ _n_ ))/∆ _t_

...

( **u** ( _**y**_ _N_, _t_ _n_ +1 ) − **u** ( _**y**_ _N_, _t_ _n_ ))/∆ _t_



=



F ( **u** ( _**x**_ 1, _t_ _n_ ), ∇ **u** ( _**x**_ 1, _t_ _n_ ), · · · ; θ)
F ( **u** ( _**x**_ 2, _t_ _n_ ), ∇ **u** ( _**x**_ 2, _t_ _n_ ), · · · ; θ)

...

F ( **u** ( _**x**_ _M_, _t_ _n_ ), ∇ **u** ( _**x**_ _M_, _t_ _n_ ), · · · ; θ)


������������������������������������������������������������������������������
_M_ ×1







F ( **u** ( _**x**_ 1, _t_ _n_ ), ∇ **u** ( _**x**_ 1, _t_ _n_ ), · · · ; θ)
F ( **u** ( _**x**_ 2, _t_ _n_ ), ∇ **u** ( _**x**_ 2, _t_ _n_ ), · · · ; θ)

...

F ( **u** ( _**x**_ _M_, _t_ _n_ ), ∇ **u** ( _**x**_ _M_, _t_ _n_ ), · · · ; θ)



.



where ∆ _t_ is the iscretization step size in temporal domain [0, _T_ ].
Through temporal discretization and the construction of the aforementioned linear system, the RBF-FD method can effectively transform complex dynamic PDE into a system of
ODE in the temporal domain, which is then solved iteratively. By calculating the interpolation
point domain **u** ( _Y_, _t_ _n_ +1 ; θ), the evaluation point domain at the same time _t_ _n_ +1 can be calculated:
**u** ( _X_, _t_ _n_ +1 ; θ) = _E_ _h_ ( _X_, _Y_ ) **u** ( _Y_, _t_ _n_ +1 ; θ).
In various practical applications, such as parameter estimation, one often encounters a multitude of equations **u** ( _**x**_ ; θ) characterized by distinct unknown parameters θ. Once we have a
parameter, we can calculate its solutions by M RBF-FD : θ �→ **u** ( _**x**_ ; θ), but conventional methods
necessitate numerous queries to costly numerical solutions of DEs, leading to significant time
expenditures. Consequently, the development of a low-cost, high-accuracy surrogate model
becomes essential.

#### **2.2 Surrogate model**


Surrogate models serve as computationally efficient approximations of complex, resourceintensive simulations, finding widespread applications in scientific computing, engineering optimization, and machine learning. These data-driven approximations, constructed using techniques such as Gaussian process regression, support vector regression, or neural networks,
establish input-output relationships to enable rapid predictions for parameter sweeps, optimization, and uncertainty quantification.


11


Consider the scenario where direct numerical solutions of differential equations via M RBF-FD
in finite element analysis or computational fluid dynamics prove prohibitively expensive. A
well-constructed surrogate model M sur can provide approximate solutions with dramatically
reduced computational overhead while maintaining acceptable accuracy.
In constructing a surrogate, we assume the availability of training data D = {(θ _i_, **u** ( _**x**_ ; θ _i_ ))} _i_ _[N]_ =1 [.]
The goal is to learn a surrogate map:


M sur : θ �→ **u** ( _**x**_ ; θ),


where θ = (θ 1, · · ·, θ _q_ ) _[T]_ ∈ R _[q]_ . The surrogate M sur proves particularly valuable when full-scale
simulations become computationally intractable.

### **3 Law-corrected surrogate modeling with Gaussian process** **regression**


We propose an infinite-dimensional surrogate model that maps the parameters θ to the solution function **u** ( _**x**_ ; θ) within a Gaussian process regression framework, employing a novel
law-corrected prior. Our approach primarily relies on the following key components: 1. Parameterizing the solution function **u** ( _**x**_ ; θ) by sample features using POD; 2. Constructing finite
regression models from θ to **u** ( _**x**_ ; θ) using standard GP surrogate for each target solution function; 3. Incorporating physical laws to joint optimization of the prediction of entire physics
system and learning the conditional prior mean function for each original surrogate; 4. Applying our surrogate model to parameter estimation scenarios within a Bayesian framework.

#### **3.1 Parametric representation of the solution for PDEs**


Basis function expansion is a widely used technique for representing complex functions
across fields such as numerical analysis, signal processing, and differential equation solving

[33]. By linearly combining basis functions, various complex functions can be approximated
or reconstructed.

We construct the approximate DE solution, denoted as ˆ **u** ( _**x**_ ; θ), using _K_ orthogonal basis
functions:



ˆ
**u** ( _**x**_ ; θ) ≈ **u** ( _**x**_ ; θ) =



_K_


α _k_ (θ) ϕ _k_ ( _**x**_ ),

�

_k_ =1



where ϕ _k_ ( _**x**_ ) is the basis function and α _k_ (θ) is the coefficient of corresponding orthogonal basis.
Once the type of basis function and truncated value _K_ is determined, the function **u** ( _**x**_ ; θ) is
completely represented by _K_ coefficients. Different types of basis functions are suitable for different applications, and choosing the appropriate basis can significantly enhance computational
efficiency and accuracy.


12


**3.1.1** **Proper orthogonal decomposition**


In addressing complex physics problems, conventional basis function approaches often fail
to adequately capture the essential features of DEs, particularly when employing a restricted
number of basis functions to achieve dimensionality reduction objectives. To address this fundamental challenge, we employ Proper Orthogonal Decomposition (POD), a powerful technique that has found widespread application in processing high-dimensional computational data
to extract low-dimensional representations of dominant physical phenomena [34]. As a numerical dimensionality reduction technique, POD significantly reduces computational complexity
in intensive simulations such as computational fluid dynamics and structural analysis [35]. The
method distinguishes itself from traditional basis function approaches by eliminating the need
for a priori assumptions about basis function forms. Instead, POD derives optimal basis functions directly from system data through singular value decomposition of the snapshot matrix
constructed from training data. This data-driven approach yields superior dimensionality reduction performance while maintaining broad applicability across diverse physical systems.
The mathematical framework of POD shares conceptual similarities with principal component
analysis in machine learning, further enhancing its interdisciplinary utility.
Suppose a set of training data D = {(θ _n_, **u** ( _**x**_ ; θ _n_ ))} _n_ _[N]_ =1 [, which contains parameters][ θ][ ∈] [R] _[q]_ [ and]
_**x**_ = ( _x_ 1, · · ·, _x_ _D_ ) _[T]_ ∈ R _[D]_ represents the discretized spatial domain. By discretizing the solutions
across _D_ spatial points for each parameter θ _n_, we construct an _N_ × _D_ snapshot matrix _**U**_ by:



**u** ( _x_ 1 ; θ 1 ), . . ., **u** ( _x_ _D_ ; θ 1 )

... ... ...

 **u** ( _x_ 1 ; θ _N_ ), . . ., **u** ( _x_ _D_ ; θ _N_ ),







=




_**U**_ =







**u** ( _**x**_ ; θ 1 )

...

**u** ( _**x**_ ; θ _N_ )



where each row of _U_ represents the discrete solution **u** ( _**x**_ ; θ _n_ ) discretized across the spatial
domain for a given parameter θ _n_ and we compute the covariance matrix of snapshot matrix as:


1
_**C**_ =
_N_ − 1 _**[U]**_ _[T]_ _**[U]**_ [.]



We perform eigenvalue decomposition on the _C_ by _C_ ϕ _i_ = λ _i_ ϕ _i_, _i_ = 1, · · ·, _D_ to obtain
eigenvalues λ and eigenvectors ϕ. The eigenvectors ϕ _i_ represent the POD modes, and their
corresponding eigenvalues λ _i_ quantify the relative energy contribution of each mode to the
overall system. The larger the value, the greater contribution of this mode to the snapshot
matrix. So we sort them in descending order according to the size of the eigenvalues and select
the top-ranked _K_ eigenvectors as POD basis function to approximate solution as:


α 1 (θ 1 )    - · · α _K_ (θ 1 ) ϕ 1 ( _x_ 1 )    - · · ϕ 1 ( _x_ _D_ ) **u** ( _x_ 1 ; θ 1 ), . . ., **u** ( _x_ _D_ ; θ 1 )
α 1 (θ 2 )    - · · α _K_ (θ 2 ) ϕ 2 ( _x_ 1 )    - · · ϕ 2 ( _x_ _D_ ) **u** ( _x_ 1 ; θ 2 ), . . ., **u** ( _x_ _D_ ; θ 2 )


=

... ... ... ... ... ... ... ... ...

α 1 (θ _N_ )                               - · · α _K_ (θ _N_ ) ϕ _K_ ( _x_ 1 )                               - · · ϕ _K_ ( _x_ _D_ )  **u** ( _x_ 1 ; θ _N_ ), . . ., **u** ( _x_ _D_ ; θ _N_ )



α 1 (θ 1 ) - · · α _K_ (θ 1 )
α 1 (θ 2 ) - · · α _K_ (θ 2 )

... ... ...

α 1 (θ _N_ ) - · · α _K_ (θ _N_ )


��������������������������������������������������
_N_ × _K_



α 1 (θ 1 ) - · · α _K_ (θ 1 )
α 1 (θ 2 ) - · · α _K_ (θ 2 )

... ... ...

α 1 (θ _N_ ) - · · α _K_ (θ _N_ )







ϕ 1 ( _x_ 1 ) - · · ϕ 1 ( _x_ _D_ )
ϕ 2 ( _x_ 1 ) - · · ϕ 2 ( _x_ _D_ )

... ... ...

ϕ _K_ ( _x_ 1 ) - · · ϕ _K_ ( _x_ _D_ )


��������������������������������������������������
_K_ × _D_



ϕ 1 ( _x_ 1 ) - · · ϕ 1 ( _x_ _D_ )
ϕ 2 ( _x_ 1 ) - · · ϕ 2 ( _x_ _D_ )

... ... ...

ϕ _K_ ( _x_ 1 ) - · · ϕ _K_ ( _x_ _D_ )



=







**u** ( _x_ 1 ; θ 1 ), . . ., **u** ( _x_ _D_ ; θ 1 )
**u** ( _x_ 1 ; θ 2 ), . . ., **u** ( _x_ _D_ ; θ 2 )

... ... ...

 **u** ( _x_ 1 ; θ _N_ ), . . ., **u** ( _x_ _D_ ; θ _N_ )


������������������������������������������������������������������
_N_ × _D_



**u** ( _x_ 1 ; θ 1 ), . . ., **u** ( _x_ _D_ ; θ 1 )
**u** ( _x_ 1 ; θ 2 ), . . ., **u** ( _x_ _D_ ; θ 2 )

... ... ...

**u** ( _x_ 1 ; θ _N_ ), . . ., **u** ( _x_ _D_ ; θ _N_ )



13


We can have the compact form as:αϕ _K_ = _**U**_ .
After computing the coefficient matrix, we can effectively approximate the original snapshot matrix through a linear combination of the POD basis modes and their corresponding
coefficients by:



_K_ _K_
� _k_ =1 [α] _[k]_ [(][θ] [1] [)][ϕ] _[k]_ [(] _[x]_ [1] [)] - · · � _k_ =1 [α] _[k]_ [(][θ] [1] [)][ϕ] _[k]_ [(] _[x]_ _[D]_ [)]
_K_ _K_
� _k_ =1 [α] _[k]_ [(][θ] [2] [)][ϕ] _[k]_ [(] _[x]_ [1] [)] - · · � _k_ =1 [α] _[k]_ [(][θ] [2] [)][ϕ] _[k]_ [(] _[x]_ _[D]_ [)]

... ... ... .

_K_ _K_

� _k_ =1 [α] _[k]_ [(][θ] _[N]_ [)][ϕ] _[k]_ [(] _[x]_ [1] [)] - · · � _k_ =1 [α] _[k]_ [(][θ] _[N]_ [)][ϕ] _[k]_ [(] _[x]_ _[D]_ [)] 



=








_K_
� _k_ =1 [α] _[k]_ [(][θ] [1] [)][ϕ] _[k]_ [(] _**[x]**_ [)]
_K_
� _k_ =1 [α] _[k]_ [(][θ] [2] [)][ϕ] _[k]_ [(] _**[x]**_ [)]

...

_K_
� _k_ =1 [α] _[k]_ [(][θ] _[N]_ [)][ϕ] _[k]_ [(] _**[x]**_ [)]



≈




_**U**_ =







**u** ( _**x**_ ; θ 1 )
**u** ( _**x**_ ; θ 2 )

...

**u** ( _**x**_ ; θ _N_ )



where α are the coefficients representing the projection onto the _K_ -th basis function which need
be computed. Through POD, we map the D-dimensional solution space to a K-dimensional
space, where _K_ ≪ _D_, effectively achieving dimensionality reduction while ensuring the accuracy of the approximate solution. By fixing the modes ϕ _k_ as basis function, we can reconstruct
the solution corresponding to any parameter sample θ by learning surrogate model for the corresponding coefficients α(θ). The limitation of the POD method lies in the need for data to
construct the snapshot matrix. However, this issue can be effectively addressed within the
framework of machine learning.

#### **3.2 GP surrogate model**


The GPR is a widely used technique for surrogate modeling, offering a nonparametric perspective. Suppose we have a dataset obtained by numerical method :


D High = {(θ _i_, **u** ( _**x**_ ; θ _i_ ))} _i_ _[N]_ =1 [,]


where θ _i_ ∈ R _[q]_, **u** ( _**x**_ ; θ _i_ ) ∈ R _[D]_ and _N_ is the size of the dataset. The training dataset is denoted as
D High, as the input θ corresponds to high-dimensional equation solutions as outputs. To reduce
dimensionality, we map the discrete space of _D_ to a basis function coefficient space of a lower
dimension using RB technology:


POD : **u** ( _**x**_ ; θ _i_ ) ∈ R _[D]_ �→ α(θ _i_ ) ∈ R _[K]_, _K_ ≪ _D_,


where α(θ _i_ ) = (α 1 (θ _i_ ), · · ·, α _K_ (θ _i_ )) _[T]_ denoted by α _i_ and _K_ is the number of basis functions.
Thereby a low-dimensional training dataset is obtained by taking the basis function coefficients
as the corresponding outputs, denoted as D Low :


D Low = {(θ _i_, α _i_ )} _i_ _[N]_ =1 [,]


Our goal is to learn the function _f_ _k_ (·) : θ �→ α _k_ by constructing _K_ individual GPR models, each
sharing the same input θ but with different outputs α _k_ :


_f_ _k_ (θ) ∼GP [�] _m_ _k_ (θ), **k** _k_ (θ, θ [′] ) [�],


14


where _m_ _k_ (θ) is the mean function, often assumed to be a constant 0, and **k** _k_ (θ, θ [′] ) is the kernel
function. A commonly used kernel is the Radial Basis Function (RBF) kernel, expressed as:



**k** RBF = γ [2] exp − [∥][θ] _[i]_ [ −] [θ] _[j]_ [∥] [2]

2ℓ [2]

�



.
�



According to the definition of GP, the finite projection of _f_ _k_ (·) onto the training inputs θ,
namely _**f**_ _k_ = ( _f_ _k_ (θ 1 ), · · ·, _f_ _k_ (θ _N_ )) _[T]_, follows a multivariate Gaussian distribution,


_p_ ( _**f**_ _k_ |θ) = N( _**f**_ _k_ | **0**, **K** _k_ ),


where **K** _k_ represents the kernel matrix evaluated at θ, and each element is defined as [ **K** _k_ ] _i_, _j_ =
**k** _k_ (θ _i_, θ _j_ ). Let η _k_ = (γ _k_, ℓ _k_ ) denote the set of all hyper-parameters associated with **K** _k_ . By
integrating out _**f**_ _k_, the marginal likelihood can be obtained. To learn the model, we maximize
the log-likelihood to estimate the kernel parameters η _k_ for each α _k_ :



log _p_ (α _k_ |θ, η _k_ ) = − [1]




[1]

2 [log det] **[ K]** _[k]_ [(][θ][,][ θ] [′] [)][ −] _[N]_ 2




[1]

2 [α] _k_ _[T]_ **[K]** _[k]_ [(][θ][,][ θ] [′] [)] [−][1] [α] _[k]_ [ −] [1] 2



2 [log 2][π.]



According to the GP prior, given a new input θ [∗], the posterior (or predictive) distribution of
the output _f_ (θ [∗] ) is a conditional Gaussian distribution:


∗ ∗ 2
_p_ [�] _f_ _k_ (θ [∗] )|θ [∗], D Low � = N� _f_ _k_ (θ )| µ _k_ (θ ), σ _k_ [(][θ] [∗] [)][�][,]


where the posterior mean and variance are given by:


µ _k_ (θ [∗] ) = E[ _f_ _k_ (θ [∗] )|θ [∗], D Low ] = _m_ _k_ (θ [∗] ) + **k** _[T]_ _k_ ∗ **[K]** [−] _k_ [1] �α _k_ − _m_ _k_ (θ)�

σ [2] _k_ [(][θ] [∗] [)][ =][ Var[] _[f]_ _[k]_ [(][θ] [∗] [)][|][θ] [∗] [,][ D] [Low] []][ =] **[ k]** _[k]_ [(][θ] [∗] [,][ θ] [∗] [)][ −] **[k]** [⊤] _k_ ∗ **[K]** [−] _k_ [1] **[k]** _[k]_ [∗]


where **k** _k_ ∗ = ( **k** _k_ (θ [∗], θ 1 ), · · ·, **k** _k_ (θ [∗], θ _N_ )) _[T]_ represents the kernel evaluations between θ [∗] and the
training input.
By iterating this process, we independently learn the surrogate model _f_ _k_ (·) : R _[q]_ → R
for each basis function coefficient α _k_ (θ). The solution **u** ( _**x**_ ; θ [∗] ) is then predicted as the linear
combination of the _K_ surrogate models, expressed as:



ˆ
**u** ( _**x**_ ; θ [∗] ) = M GP ( _**x**_ ; θ [∗] ) =



_K_


µ _k_ (θ [∗] )ϕ _k_ ( _**x**_ ). (5)

�

_k_ =1


#### **3.3 Prior correction with physical laws**

Since the GP surrogate model is data-driven, it tends to have limited prediction capability
within the parameter space Θ, but outside the training data points. To address this, we propose embedding physical laws from DEs directly into the surrogate model. By incorporating


15


Figure 3: A basic strategy for gaining θ law, the blue points are θ obs for training _f_ _k_ (·), and orange points
are θ law for learning physical law correction function ω _k_ (·).


global DE constraints as prior knowledge, we enhance the GPR model performance, improving its ability to predict. This approach is independent of the data-driven nature of the GPR
model and does not require additional training data from expensive numerical simulations. The
new correction function ω(θ| _Law_ ) is introduced to adjust the GP predicted coefficients, ensuring they align with the physical laws of the system, where _Law_ is defined by the governing
parametric PDE of the physical system law as Eq.(1).
The GP surrogate models for the coefficients of the DE solution is constructed with mean
functions _m_ _k_ (θ) of prior. A straightforward idea is that we can learn a better mean function of
prior ˜ _m_ _k_ (θ| _Law_ ) with considering the physical laws based on the original surrogate model. We
denote the novel prior mean function as:


˜
_m_ _k_ (θ| _Law_ ) = _m_ _k_ (θ) + ω _k_ (θ| _Law_ ),


where ˜ _m_ _k_ (θ| _Law_ ) is physical law-corrected prior, _m_ _k_ (θ) is prior mean function of _f_ _k_ (·) which is
general supposed to constant 0 and the ω _k_ (θ| _Law_ ) is correction function that needs to be learned
by physical law.
The reason for incorporating physical laws to refine the model is to enhance the surrogate’s
predictive capability for data outside the training set. Let θ obs ⊂{θ _i_ |(θ _i_, α _i_ ) ∈D Low } denote
the training set used for learning the data-driven GP model. Then under this framework, the
physical law-corrected prior function can be further reformulated as:



˜
_m_ _k_ (θ| _Law_ ) =



0, θ ∈ θ obs,

ω _k_ (θ| _Law_ ), θ ∈ Θ \ θ obs,




By introducing a new prior function, it is possible to construct a novel LC-prior GP surrogate _f_ [˜] _k_ (·) that simultaneously leverages data-driven and physical informed as :


_f_ ˜ _k_ (θ) ∼GP [�] _m_ ˜ _k_ (θ| _Law_ ), **k** _k_ (θ, θ [′] ) [�]


16


For any given parameter θ [∗], we can subsequently derive the conditional posterior mean under
the physical law by:

˜
µ _k_ (θ [∗] ) = E[ _f_ _k_ (θ [∗] )|θ [∗], D Low, _Law_ ]

= ˜ _m_ _k_ (θ [∗] | _Law_ ) + **k** _[T]_ _k_ ∗ **[K]** [−] _k_ [1] �α _k_ − _m_ _k_ (θ)�

= ω _k_ (θ [∗] | _Law_ ) + **k** _[T]_ _k_ ∗ **[K]** [−] _k_ [1] [α] _[k]_

Similar to the approach in the preceding section, we can likewise reconstruct the function
**u** ( _**x**_ ; θ [∗] ). The Eq.(5) can now be rewritten as:



ˆ
**u** ( _**x**_ ; θ [∗] ) = M LC ( _**x**_ ; θ [∗] ) =



_K_


µ˜ _k_ (θ [∗] )ϕ _k_ ( _**x**_ )

�

_k_ =1



=


=



_K_
�

_k_ =1



_K_


ω _k_ (θ [∗] )ϕ _k_ ( _**x**_ ) + M GP ( _**x**_ ; θ [∗] )

�

_k_ =1



�µ _k_ (θ ∗ ) + ω _k_ (θ ∗ )�ϕ _k_ ( _**x**_ )



(6)



In order to learn the mapping relationship from parameters θ to correction coefficients ω _k_ (·),
it is necessary to additionally extract a subset of parameter samples from the entire parameter
space for physical law correction. So in the parameter space Θ, we extract some parameters
θ law outside the data-driven GP surrogate training set θ obs, that used to learn the correction
coefficients by physical law loss function. The Fig.3 shows a basic strategy for gaining θ law .
The physical law loss function to learn the ω _k_ (·) in Eq.(1) as:


L _oss_ = ||F (M LC ( _**x**_ ; θ law )) − **f** ( _**x**_ ; θ law )|| [2] _l_ 2 [+][ λ][ ∗||G][(][M] [LC] [(] _**[x]**_ [;][ θ] [law] [))][ −] **[g]** [(] _**[x]**_ [;][ θ] [law] [)][||] _l_ [2] 2 [,] (7)


where λ is a weight that controls the proportion of boundary conditions. We perform a global
search within a range of [− _**c**_ - σ _k_ (θ law ), _**c**_ - σ _k_ (θ law )] to optimize ω, where σ _k_ (θ law ) is the standard
deviation of each surrogate _f_ _k_ (·), _**c**_ is a constant that controls the size of the local interval.
In the optimization of this loss function, if uesint numerical methods to recompute the differential function F in every iteration, it would not only incur prohibitive computational costs
but also degrade accuracy due to numerical errors, which would adversely affect the training
and learning of our surrogate model. Benefiting from the RBF-FD method’s characteristics
for the specific problem, we can pre-store the evaluation matrix _E_ _h_ ( _X_, _Y_ ) and differentiation
matrices _D_ [∇] _h_ [(] _[X]_ [,] _[ Y]_ [)][,] _[ D]_ [∇] _h_ [2] [(] _[X]_ [,] _[ Y]_ [), etc. in sparse matrix format during the forward solving process.]
Since these matrices are independent of the equation parameters θ, they require no repeated
solving. This advantageous feature enables that we can efficiently compute the loss function
by reconstructing the RBF-FD linear system using the predicted solutions from the surrogate
model. Here we denote M LC (·) compactly as ˆ **u** ( _X_ ; θ), we can obtain:

∥F [�] **u** ˆ ( _X_ ), _D_ [∇] _h_ [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [)][,] _[ D]_ _h_ [∇] [2] [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [)][,][ · · ·][ ;][ θ][�] [−] **[f]** [(] _[X]_ [;][ θ][)][ ∥] _l_ [2] 2 _**x**_ ∈ Ω
 ∥G [�] **u** ˆ ( _X_ ), _D_ [∇] _h_ [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [)][,] _[ D]_ _h_ [∇] [2] [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [)][,][ · · ·][ ;][ θ][�] [−] **[g]** [(] _[X]_ [;][ θ][)][ ∥] _l_ [2] 2 _**x**_ ∈ ∂Ω


17


Assume point sets _X_ and _Y_ consist of _m_ and _n_ discrete points located within the computational
domain boundary respectively, the left terms of linear system within the Ω follow as:



**u** ˆ ( _**y**_ _n_ +1 ; θ)

...

**u** ˆ ( _**y**_ _N_ ; θ)



**u** ˆ ( _**y**_ 1 ; θ)

...

**u** ˆ ( _**y**_ _n_ ; θ)







**u** ( _**y**_ 1 ; θ)

...

**u** ˆ ( _**y**_ _n_ ; θ)


**u** ˆ ( _**y**_ _n_ +1 ; θ)

...

 **u** ˆ ( _**y**_ _N_ ; θ) 


������������������������
_N_ ×1



_L_ _f_ =



F (Φ 1 ( _**x**_ 1 ), ∇Φ 1 ( _**x**_ 1 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 1 ), ∇Φ _N_ ( _**x**_ 1 ), · · · ; θ)
F (Φ 1 ( _**x**_ 2 ), ∇Φ 1 ( _**x**_ 2 ), · · · ; θ)  - · · F (Φ _N_ ( _**x**_ 2 ), ∇Φ _N_ ( _**x**_ 2 ), · · · ; θ)

... ... ...

 F (Φ 1 ( _**x**_ _m_ ), ∇Φ 1 ( _**x**_ _m_ ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ _m_ ), ∇Φ _N_ ( _**x**_ _m_ ), · · · ; θ) 


������������������������������������������������������������������������������������������������������������������������������������������������������������������������������
_m_ × _N_







F (Φ 1 ( _**x**_ 1 ), ∇Φ 1 ( _**x**_ 1 ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ 1 ), ∇Φ _N_ ( _**x**_ 1 ), · · · ; θ)
F (Φ 1 ( _**x**_ 2 ), ∇Φ 1 ( _**x**_ 2 ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ 2 ), ∇Φ _N_ ( _**x**_ 2 ), · · · ; θ)

... ... ...

F (Φ 1 ( _**x**_ _m_ ), ∇Φ 1 ( _**x**_ _m_ ), · · · ; θ) - · · F (Φ _N_ ( _**x**_ _m_ ), ∇Φ _N_ ( _**x**_ _m_ ), · · · ; θ)



the linear system on the boundary ∂Ω can likewise be expressed as:



**u** ˆ ( _**y**_ _n_ +1 ; θ)

...

**u** ˆ ( _**y**_ _N_ ; θ)



**u** ˆ ( _**y**_ 1 ; θ)

...

**u** ˆ ( _**y**_ _n_ ; θ)







**u** ( _**y**_ 1 ; θ)

...

ˆ
**u** ( _**y**_ _n_ ; θ) 

~~~~


**u** ˆ ( _**y**_ _n_ +1 ; θ)

...

 **u** ˆ ( _**y**_ _N_ ; θ) 


������������������������
_N_ ×1



_L_ _g_ =







G(Φ 1 ( _**x**_ _m_ +1 ), ∇Φ 1 ( _**x**_ _m_ +1 ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _m_ +1 ), ∇Φ _N_ ( _**x**_ _m_ +1 ), · · · ; θ)
G(Φ 1 ( _**x**_ _m_ +2 ), ∇Φ 1 ( _**x**_ _m_ +2 ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _m_ +2 ), ∇Φ _N_ ( _**x**_ _m_ +2 ), · · · ; θ)

... ... ...

 G(Φ 1 ( _**x**_ _M_ ), ∇Φ 1 ( _**x**_ _M_ ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _M_ ), ∇Φ _N_ ( _**x**_ _M_ ), · · · ; θ) 


��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������������
( _M_ − _m_ )× _N_



G(Φ 1 ( _**x**_ _m_ +1 ), ∇Φ 1 ( _**x**_ _m_ +1 ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _m_ +1 ), ∇Φ _N_ ( _**x**_ _m_ +1 ), · · · ; θ)
G(Φ 1 ( _**x**_ _m_ +2 ), ∇Φ 1 ( _**x**_ _m_ +2 ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _m_ +2 ), ∇Φ _N_ ( _**x**_ _m_ +2 ), · · · ; θ)

... ... ...

G(Φ 1 ( _**x**_ _M_ ), ∇Φ 1 ( _**x**_ _M_ ), · · · ; θ) - · · G(Φ _N_ ( _**x**_ _M_ ), ∇Φ _N_ ( _**x**_ _M_ ), · · · ; θ)



where ˆ **u** ( _Y_ ; θ) = [�] **u** ˆ ( _**y**_ 1 ; θ), · · ·, ˆ **u** ( _**y**_ _N_ ; θ) [�] _[T]_ can be calculated by the Moore-Penrose pseudoinverse of evaluation matrix: ˆ **u** ( _Y_ ; θ) = _E_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [;][ θ][).]
Let the _M_ × _N_ matrix be denoted as _L_ = ( _L_ _f_, _L_ _g_ ) _[T]_, where _L_ _f_ represents discrete points within
the computational domain Ω and _L_ _g_ represents points on the boundary ∂Ω. With the right terms
_R_ = ( _R_ _f_, _R_ _g_ ) _[T]_, where _R_ _f_ = [�] **f** ( _**x**_ 1 ; θ), · · ·, **f** ( _**x**_ _m_ ; θ) [�] _[T]_, and _R_ _g_ = [�] **g** ( _**x**_ _m_ +1 ; θ), · · ·, **g** ( _**x**_ _M_ ; θ) [�] _[T]_ .
Then the physics loss function Eq.(7) can be rewritten:


L _oss_ = ∥ _L_ _f_ _E_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [;][ θ][)][ −] _[R]_ _[ f]_ [∥] _l_ [2] 2 [+][ λ][ ∗∥] _[L]_ _[g]_ _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] **[u]** [(] _[X]_ [;][ θ][)][ −] _[R]_ _[g]_ [∥] _l_ [2] 2


By optima the above loss function for each physics corrected point θ law, we can obtain the
optimization result of ω at all discrete corrected points. By organizing the parameters and their
corresponding correction coefficients, we obtain the physics corrected training set, denoted as
D law = {(θ law( _i_ ), ω ( _i_ ) )} _i_ _[M]_ =1 [, where][ ω] [(] _[i]_ [)] [ =][ (][ω] [1(] _[i]_ [)] [,][ · · ·][, ω] _[K]_ [(] _[i]_ [)] [)] _[T]_ [ corresponding the GP surrogate] _[ f]_ _[k]_ [(][·][)]
respectively. To further characterize the mapping relationship between the entire parameter
space and the correction coefficients, we learn their connection through interpolation functions
and the physics corrected training set. Similar to the GP surrogate learning in previous sections,
we now need to independently learn _K_ interpolation functions _s_ _k_ (·) : θ �→ ω _k_ based on the


18


**Algorithm 1** LC-prior GP
**Input:** D High = {(θ _i_, **u** ( _**x**_ ; θ _i_ ))} _i_ _[N]_ =1 [; the number of basis functions] _[ K]_ [; prediction target][ θ] [∗]

**Output:** Surrogate approximate solution M LC ( _**x**_ ; θ [∗] ).

1: Parameterize the output solutions in D High by POD method and obtain the low-dimensional
dataset D low = {(θ _i_, α _i_ )} _i_ _[N]_ =1 [.]
2: Construct GP surrogates for the _K_ basis coefficients α _k_ : _f_ _k_ (·) ∼GP _k_ (·) with dataset D low .
3: Select _M_ physical correction parameters {θ law( _i_ ) } _i_ _[M]_ =1 [∈] [Ω][.]
4: **for** _i_ = 1 **to** _M_ **do**
5: Predict (µ _k_ (·), σ [2] _k_ [(][·][)) for each][ θ] [law] [ using the GP surrogate] _[ f]_ _[k]_ [(][·][)]
6: In the bounds [− _**c**_ σ [2] (·), _**c**_ σ [2] (·)], optimize the correction coefficient ω _k_ using the physical law loss function in Eq. (7)
7: **end for**
8: Train a corrected model for each basis function coefficients _s_ _k_ (·) : θ �→ ω _k_ using the new
training data D law = {(θ law( _i_ ), ω ( _i_ ) )} _i_ _[M]_ =1 [and interpolation functions.]
9: Renew the prior mean using _s_ _k_ (·) to get the LC-prior GP: _f_ [˜] _k_ (·) ∼N(˜µ _k_ (·), ˜σ [2] _k_ [(][·][)).]
10: Compute the approximate solution: M LC ( _**x**_ ; θ [∗] ) = [�] _k_ _[K]_ =1 [µ][˜] _[k]_ [(][θ] [∗] [)][ϕ] _[k]_ [(] _**[x]**_ [), where the posterior]
mean ˜µ _k_ (θ [∗] ) = _s_ _k_ (θ [∗] ) + µ _k_ (θ [∗] ).


number of POD modes. In this training part, we are correcting the posterior mean of the learned
GP surrogate, so there is no need to solve **u** ( _**x**_ ; θ) using numerical methods to generate a new
training set, all the corrections are based on the learned model _f_ _k_ (·) and all the information
comes from the physical constraints of the specific DE . The overall schematic of our LC-prior
GP method is shown in Fig.1.

#### **3.4 Prediction**


The complete surrogate consists of two parts: the data-driven GPR model _f_ _k_ (·) : θ �→ α _k_
and the physical law corrected model _s_ _k_ (·) : θ �→ ω _k_, both models have the same input θ.
We pass the prediction _s_ _k_ (·) of the corrected model back to the GPR model as the physics
constraints to renew the prior constant 0. So the LC-prior GP _f_ [˜] _k_ (·) is also a Gaussian process
with a law-corrected prior: _f_ ˜ _k_ (θ) ∼GP [�] _m_ ˜ _k_ (θ), **k** _k_ (θ, θ ′ )�.


Given a new parameter θ [∗], the conditional distribution can be written:


_f_ ˜ _k_ (θ [∗] ) ∼N(˜µ _k_ (θ [∗] ), ˜σ [2] _k_ [(][θ] [∗] [))][,]


where ˜µ _k_ (·), ˜σ [2] _k_ [(][·][) are conditional mean and std:]


˜
µ _k_ (θ [∗] ) = E[ _f_ _k_ (θ [∗] )|θ [∗], D Low, _Law_ ] = _s_ _k_ (θ [∗] ) + **k** _[T]_ _k_ ∗ **[K]** [−] _k_ [1] [α] _[k]_ [ =] _[ s]_ _[k]_ [(][θ] [∗] [)][ +][ µ] _[k]_ [(][θ] [∗] [)][,]


σ˜ [2] _k_ [(][θ] [∗] [)][ =][ Var[] _[f]_ _[k]_ [(][θ] [∗] [)][|][θ] [∗] [,][ D] [Low] [,] _[ Law]_ []][ =] **[ k]** _[k]_ [(][θ] [∗] [,][ θ] [∗] [)][ −] **[k]** [⊤] _k_ ∗ **[K]** [−] _k_ [1] **[k]** _[k]_ [∗] [.]


19


And we can reconstruct solution of the equation **u** ( _**x**_ ; θ [∗] ) in domain for any given parameters
by LC-prior GP:



_K_
�

_k_ =1



M LC ( _**x**_ ; θ [∗] ) =


=



_K_


˜
µ _k_ (θ [∗] )ϕ _k_ ( _**x**_ ) =

�

_k_ =1



_K_


_s_ _k_ (θ [∗] )ϕ _k_ ( _**x**_ ) + M GP ( _**x**_ ; θ [∗] )

�

_k_ =1



�µ _k_ (θ ∗ ) + _s_ _k_ (θ ∗ )�ϕ _k_ ( _**x**_ )



where ˜µ _k_ (·) is the predicted posterior mean of LC-prior GP _f_ [˜] _k_ (·). The overall framework of
LC-prior GP can be defined in Algorithm 1.

#### **3.5 Extension to multi-coupled PDE systems**


In many real-world scenarios, the governing equations are not isolated single-physics PDEs
but rather multi-coupled systems involving strongly interacting physical processes. Extending the LC-prior GP framework to such settings requires careful treatment of the correlations
among multiple dependent variables. Let us denote the multi-coupled PDE system as:

F [�] **u** [(1)], **u** [(2)], · · ·, **u** [(] _[m]_ [)] ; θ [�] = **f** ( _**x**_ ; θ), _**x**_ ∈ Ω,
(8)
 G [�] **u** [(1)], **u** [(2)], · · ·, **u** [(] _[m]_ [)] ; θ [�] = **g** ( _**x**_ ; θ), _**x**_ ∈ ∂Ω.


where **u** [(] _[ j]_ [)] ( _**x**_ ; θ) represents the _j_ -th physical variable in the coupled system, and θ is the vector
of parameters. Suppose we have already obtained a set of training set {(θ _i_, **u** [(1)] _i_ [,][ · · ·][,] **[ u]** [(] _i_ _[m]_ [)] [)][}] _i_ _[N]_ =1
using the RBF-FD method and follow the same procedure as in the single-physics case, each
solution field is projected onto a reduced basis constructed via POD from snapshot data:



**u** [(] _[j]_ [)] ( _**x**_ ; θ) ≈



_K_ _j_
� α [(] _k_ _[ j]_ [)] [(][θ][)][ϕ] [(] _k_ _[j]_ [)] [(] _**[x]**_ [)][,] _j_ = 1, · · ·, _m_ .

_k_ =1



where _K_ _j_ is the number of POD modes for the _j_ -th physical variable, ϕ [(] _[ j]_ [)] = (ϕ [(] 1 _[ j]_ [)] [,][ · · ·][, ϕ] [(] _K_ _[ j]_ _j_ [)] [)] _[T]_

denote the modes and α [(] _[ j]_ [)] = (α [(] 1 _[ j]_ [)] [,][ · · ·][, α] [(] _K_ _[ j]_ _j_ [)] [)] _[T]_ [ are the corresponding reduced coe][ffi][cients. For]
each coefficient, a Gaussian process surrogate is trained:


_f_ _k_ [(] _[j]_ [)] [(][θ][)][ ∼GP][�] _[m]_ [(] _k_ _[ j]_ [)] [(][θ][)][,] **[ k]** [(] _k_ _[ j]_ [)] [(][θ][,][ θ] ′ )�, _j_ = 1, · · ·, _m_ .


with the pure data-driven surrogate M [(] GP _[ j]_ [)] [for each variable] **[ u]** [(] _[ j]_ [)] [(] _**[x]**_ [;][ θ][).]
A naive extension would treat these surrogates independently, but this ignores the crossvariable couplings encoded in the governing equations. To address this, the LC-prior GP
framework is adapted by introducing joint correction coefficient ω [(] _k_ _[ j]_ [)] [(][θ][|] _[Law]_ [) for each GP re-]
gression model to construct physical law corrected surrogate. These coefficients are optimized
simultaneously under the constraint of the entire physical coupled system:


L _oss_ = ||F [�] M [(1)] LC [,][ · · ·][,][ M] [(] LC _[m]_ [)] [;][ θ][�] [−] **[f]** [(] _**[x]**_ [;][ θ][)][||] _l_ [2] 2 [+][ λ][ ∗||G][�][M] LC [(1)] [,][ · · ·][,][ M] LC [(] _[m]_ [)] [;][ θ][�] [−] **[g]** [(] _**[x]**_ [;][ θ][)][||] _l_ [2] 2 [,][ (9)]


20


where M [(] _[ j]_ [)]
LC [denotes the law-corrected surrogate reconstruction of the corresponding variable.]
Consistent with the previous formulation, we employ a small set of parameter samples to perform optimization based on the above loss function together with the trained GP surrogates, and
learn the global correction mapping _s_ [(] _k_ _[j]_ [)] [(][·][) :][ θ][ �→] [ω] [(] _k_ _[ j]_ [)] [for each surrogate model in the parameter]
space through interpolation functions. The interpolation functions are then backpropagated to
the original GP surrogates as the new prior mean functions, yielding the LC-prior GP _f_ [˜] _k_ [(] _[j]_ [)] [(][·][).]
For given θ [∗], any physical variable **u** [(] _[j]_ [)] ( _**x**_ ; θ [∗] ) in multi-coupled system can be efficiently
predicted through the LC-prior GP:



_K_ _j_
�

_k_ =1



M [(] _[ j]_ [)]
LC [(] _**[x]**_ [;][ θ] [∗] [)][ =]


=



_K_ _j_
� E[ _f_ [˜] _k_ [(] _[ j]_ [)] [(][θ] [∗] [)][|][θ] [∗] [,] _[ Law]_ []][ϕ] _[k]_ [(] _**[x]**_ [)][ =]

_k_ =1



_K_ _j_
� _s_ [(] _k_ _[ j]_ [)] [(][θ] [∗] [)][ϕ] _[k]_ [(] _**[x]**_ [)][ +][ M] [(] GP _[j]_ [)] [(] _**[x]**_ [;][ θ] [∗] [)][,] _j_ = 1, · · ·, _m_ .

_k_ =1



( _j_ )
�µ _k_ [(][θ] [∗] [)][ +] _[ s]_ [(] _k_ _[j]_ [)] [(][θ] [∗] [)][�][ϕ] _[k]_ [(] _**[x]**_ [)]



This joint optimization ensures that correction terms are consistent across all variables
and enforce the interdependencies dictated by the PDE system. In this way, the multi-output
LC-prior GP explicitly encodes both the data-driven correlations and the physical couplings,
thereby yielding predictions that are physically coherent and numerically stable.

#### **3.6 Parameter estimation by LC-prior GP**


Inferring unknown parameters θ from indirect observations _**y**_ is a critical application. Typically, the observed data _**y**_ is contaminated with noise, often modeled as _**y**_ = M true ( _**x**_ ; θ) + ϵ,
where M true ( _**x**_ ; θ) represents the true model out put and ϵ is the noise term. There are two general frameworks for parameter estimation: (1) deterministic methods for point estimation, and
(2) Bayesian inverse methods for posterior estimation. Here we propose to infer the unknown
parameters in Bayesian framework with use of our LC-prior GP surrogate.
In Bayesian setting, the prior belief about the parameter θ is encoded in the probability
distribution π prior (θ). Here we use a uniform distribution as prior for highlighting the action of
likelihood function. Our aim is to infer the distribution of θ conditioned on the given data _**y**_, the
LC-prior GP surrogate M LC and physical law _Law_, the posterior distribution π(θ| _**y**_, M LC, _Law_ ).
By the Bayes’ rule, we have


π(θ| _**y**_, M LC, _Law_ ) ∝ _P_ ϵ � _**y**_ −M LC ( _**x**_ ; θ)� ∗ π( _Law_ |M LC, _**y**_ ) ∗ π prior (θ), (10)


where _P_ ϵ � _**y**_ −M LC ( _**x**_ ; θ)� is the likelihood π( _**y**_, M LC |θ) and π( _Law_ |M LC, _**y**_ ) is the conditional
distribution of physical law defined by the loss function Eq. (7)


_P_ ϵ � _**y**_ −M LC ( _**x**_ ; θ)� ∝ exp(−β 1 ∗|| _y_ −M LC ( _**x**_ ; θ)|| 2 _l_ 2 [)][,]


π( _Law_ |M LC ) ∝ exp(−β 2 ∗||L _oss_ || [2] _l_ 2 [)][,]


21


fidelity to the DEs can be measured by π( _Law_ |M LC ), while fidelity to the data can be measured
by _P_ ϵ � _**y**_ −M LC ( _**x**_ ; θ)�. Weights for data and equation information are assumed in β = (β 1, β 2 ).
The unnormalized posterior samples can be easily sampled using MCMC method such as
Metropolis-Hastings (MH) algorithm, Gibbs sampling and DRAM et al. Here we use MH sampling algorithm to draw samples from the unnormalized posterior Eq. (10). The MetropolisHastings (MH) algorithm is one of the most popular MCMC methods. An MH step of invariant
distribution π(θ) and proposal distribution π _q_ (θ ′ |θ _i_ ) involves sampling a candidate value θ ′ given
′ _i_ ′
the current value θ _[i]_ according to π _q_ (θ |θ ). The Markov chain then moves towards θ with acceptance probability



′ _i_ ′
′ | _y_ ) _q_ (θ |θ )
A(θ [(] _[i]_ [)], θ ) = min 1, [π][(][θ] ′ _i_
� π(θ _[i]_ | _y_ ) _q_ (θ |θ )



,
�



otherwise it remains at θ _[i]_ In our work, we draw from π(θ| _y_, F ) using MH sampling:


1. initialize θ [1]


2. For _i_ = 1 to _N_

′ ′ _i_
(a) Sample θ from the proposal distribution π _q_ (θ |θ ).



′ π(θ ′ | _y_ )π _q_ (θ _i_ |θ ′ )
(b) Compute A(θ _[i]_, θ ) = min �1, π(θ _[i]_ | _y_ )π _q_ (θ ~~[′]~~ |θ _[i]_ )



and sample _u_ from U[0, 1].
�



′ ′ ′ _i_
(c) If _u_ < A(θ _[i]_, θ ) then accept θ . Otherwise, set θ = θ .


Bayesian methods offer advantages by explicitly accounting for parameter uncertainty,
making them more robust in the presence of noisy or sparse data. By providing a full posterior
distribution, they enable probabilistic interpretation and decision-making under uncertainty.
Both deterministic and Bayesian methods can be computationally intensive, especially for highdimensional models or when the likelihood function is complex. The surrogate model we construct mitigates the challenge of high computational cost by replacing the map M RBF-FD (·) with
M LC (·).

### **4 Numerical examples**


To validate the performance of LC-prior GP method, we present five numerical examples
based on our proposed method. In the numerical examples of the Reaction-diffusion equation,
miscible flooding model and the Navier-Stokes model, we primarily demonstrate the surrogate
performance of the LC-prior GP method on the irregular computational domain. To illustrate
the accuracy of this approach, we compare it with the standard GP method and the DMDwiNN method proposed in reference [29], further highlighting the performance of the LC-prior
GP method under small-sample settings. Additionally, within the framework of our surrogate model, we achieve highly efficient parameter estimation applications in miscible flooding
model and the Korteweg-de Vries equation using given noisy observations.


22


Table 1: The relative errors of GP method, LC-prior GP method and DMD-wiNN.


GP LC-prior GP DMD-wiNN


t=0.5 0.0763 **0** . **0211** 0.0275

t=1.0 0.1127 **0** . **0307** 0.0408

All discrete time 0.0839 **0** . **0233** 0.0304


Figure 4: Relative errors between the mean solutions obtained by the RBF-FD and the LC-prior GP with
different _N_ law and POD modes _K_ (left), and the optimization time with physical law correction under the
corresponding conditions (right).

#### 4.1 Reaction-di ff usion model



In this subsection, we consider a reaction-diffusion equation, which involves a parameter ϵ
and with a homogeneous Dirichlet boundary condition:


_u_ _t_ = ϵ [2] ∆ _u_ − _F_ [′] ( _u_ ) + _f_ 0 ( _x_, _y_, _t_ ), in (0, _T_ ) × Ω,
_F_ ( _u_ ) = [1] 4 [(] _[u]_ [2] [ −] [1)] [2] [,] in (0, _T_ ) × Ω, (11)

_u_ (0, ·) = _u_ 0, in Ω,





_u_ _t_ = ϵ [2] ∆ _u_ − _F_ [′] ( _u_ ) + _f_ 0 ( _x_, _y_, _t_ ), in (0, _T_ ) × Ω,
_F_ ( _u_ ) = [1] [(] _[u]_ [2] [ −] [1)] [2] [,] in (0, _T_ ) × Ω,



_F_ ( _u_ ) = [1] 4 [(] _[u]_ [2] [ −] [1)] [2] [,] in (0, _T_ ) × Ω, (11)

_u_ (0, ·) = _u_ 0, in Ω,



For the example in this subsection, Eq.(11) with _f_ 0 ( _x_, _y_, _t_ ) = 0 is the classical Allen-Cahn
equation, which involves a parameter ϵ related to the interface thickness. We set _T_ = 1 and
computational domain is Ω= [−1, 1] × [−1, 1] ∈ R [2] . The initial value is given by



_u_ 0 ( _x_, _y_ ) =



1
1, if ( _x_ [2] + _y_ [2] ) 2 ≤ [1] 8 [(3][ +][ 3 sin(5][γ][))][,]


0, elsewhere,




23


Figure 5: The first column presents the mean solutions of the RBF-FD method at _t_ =0.5, while columns
two to four display the results at _t_ =0.5 for GP, LC-prior GP and DMD-wiNN methods, respectively. The
first row shows the mean solutions, and the second row provides the corresponding errors.


with



arccos
�



2π − arccos
�



, _y_ ≥ 0,
�



γ =







_x_
~~√~~ _x_ [2] + _y_ [2]



_x_
~~√~~ _x_ [2] + _y_ [2]



, _y_ < 0.
�



The discrete scheme we used in the temporal direction is


_u_ _[n]_ [+][1] − τϵ [2] ∆ _u_ _[n]_ [+][1] = _u_ _[n]_ − τ _f_ ( _u_ _[n]_ ).


Here θ = ϵ. The training set with 3 sample parameters is uniformly generated from the
interval π prior (ϵ) ∼U[0, 1] . For the testing set, we randomly generate 200 samples from
the uniform distribution π prior (ϵ). In this numerical example, only the surrogate model for
_u_ ( _x_, _y_, _t_ ; θ) needs to be constructed.
In this section, to illustrate the impact of the number of parameter points used for physical
law correction and the number of POD modes on the final modeling accuracy, we compare
the relative errors of the LC-prior GP and the RBF-FD method on the test set under different conditions, as shown in Fig.4(left). Here, _N_ law denotes the number of samples used
to learn the physical-law prior function, obtained by uniformly sampling within the interval π prior (ϵ) ∼U[0, 1], and _K_ epresents the first _K_ POD modes extracted from the eigendecomposition of the snapshot matrix. When _K_ = 2, the number of selected modes is too
small to provide a good approximation of the solution space of the original function; under this
condition, even with additional physical law corrections, the surrogate performance cannot be


24


Figure 6: The first column presents the mean solutions of the RBF-FD method at _t_ =1, while columns
two to four display the results at _t_ =1 for GP, LC-prior GP and DMD-wiNN methods, respectively. The
first row shows the mean solutions, and the second row provides the corresponding errors.


significantly improved. In contrast, when the number of modes is increased to _K_ = 3, the
overall system is well captured, and appropriately increasing the number of correction samples
_N_ law further reduces the prediction error of the final model. It is worth noting that when the
number of modes is further increased to _K_ = 4, the relative prediction errors are nearly the
same as those with _K_ = 3. This indicates that the surrogate performance of the LC-prior GP
is closely tied to the approximation capability of the POD method: once the approximation is
sufficiently accurate, adding more modes does not further enhance performance. Fig.4(right)
presents the corresponding optimization time, whose computational cost grows with both _N_ law
and _K_ . Based on these results, one can set an error threshold such that when the POD approximation error falls below this threshold, there is no need to keep increasing _K_ and incurring
additional computational costs, while still maintaining a high-quality surrogate model.
In order to present more concrete results, we choose _N_ law = 7 and _K_ = 3 for the surrogate
model construction. The Table.1 presents the relative errors between three methods and the
RBF-FD full-order solution at _t_ = 0.5, _t_ = 1 and all discrete time instants. Although the
conventional GP method performs poorly on the test set, the physical law-corrected method
demonstrates relative errors smaller than those of the DMD-wiNN method with equivalent
sample size, confirming the effectiveness of our approach. Fig.5 and Fig.6 show the means
and errors of the test set solutions at _t_ = 0.5 and _t_ = 1 respectively. The first column shows
the full-order solution obtained by RBF-FD, followed by the GP and LC-prior GP surrogate
model results in the second and third columns, and the DMD-wiNN method’s result in the
fourth column. The first row presents the means of solutions for testing set, while the second
row shows their corresponding errors between the full-order solution.


25


Table 2: The relative errors with different number of physical law corrected samples


GP LC-prior GP


_N_ law = 4 0.3105 0.1732
_N_ law = 9 0.3105 **0** . **0952**


Figure 7: Two different strategies to chose the physical law correction points. The blue points are GP
surrogate training data θ obs and orange points are correction points θ law

#### **4.2 The KdV Equation**


The Korteweg–de Vries equation is a nonlinear partial differential equation arising in the
study of a number of different physical systems, e.g., water waves, plasma physics, anharmonic
lattices, and elastic rods [36]. It describes the long time evolution of small-but-finite amplitude
dispersive waves:







_u_ _t_ + θ 1 _u_ [∂] _[u]_



∂ [3] _u_

[∂] _[u]_ [=][ 0][,] in[0, _T_ ] × Ω

∂ _x_ [+][ θ] [2] ∂ _x_ [3]




[1] ~~√~~ ~~_c_~~ 1

2 _[sech]_ [2] [(] 2



√ ~~_c_~~ 2

[2]

2 _[sech]_ [2] [(] 2



_u_ ( _x_, 0) = _[c]_ [1]



~~_c_~~ 1

2 [(] _[x]_ [ −] _[L]_ [1] [))][ +] _[ c]_ 2 [2]



(12)

~~_c_~~ 2

in[0, _T_ ] × Ω
2 [(] _[x]_ [ −] _[L]_ [2] [))][,]



where θ = (θ 1, θ 2 ) and Ω ∈ R [1] we construct surrogate models for the nonlinear term θ 1 and
the dispersion term θ 2 to _u_ ( _x_, _t_ ; θ), with π prior (θ 1 ) ∼U(2, 8) and π prior (θ 2 ) ∼U(0.2, 2). This
is a highly nonlinear PDE, so 16 training set parameter θ obs are uniformly selected in prior
distribution.

In order to illustrate the accuracy of parameter estimation, here we supposed a true parameters θ [∗] = (6, 1). And given noise observation _**y**_ = M RBF-FD (θ [∗] ) + ϵ, where ϵ ∼N(0, σ [2] ) to get
posterior samples under our proposed framework.
In this study, we investigate how the number of correction points selected based on physical
laws affects the performance of the surrogate model, and further extend the analysis to a twodimensional parameter space. Specifically, the number of θ law used for correction is recorded
as _N_ law, we compare two point-selection strategies: _N_ law = 4 and _N_ law = 9, as shown in Fig.7,


26


Table 3: MCMC results for the KdV Equation with θ [∗] = (6, 1)


Method _N_ law = 9 _N_ law = 4


σ [2] obs [=][ 0][.][1] LC-prior GP **(6.08** ± **1.16, 1.16** ± **0.67)** (4.61 ± 4.43, 1.67 ± 3.14)
GP (5.32 ± 2.40, 1.22 ± 1.69) —
σ ~~[2]~~ obs [=][ 0][.][2] LC-prior GP **(5.95** ± **1.11, 1.09** ± **0.62)** (4.37 ± 4.12, 1.24 ± 3.02)
GP (6.43 ± 2.52, 1.93 ± 1.71) —


Figure 8: The prediction of _u_ ( _x_, _t_ ; θ [∗] ) for KdV equation by different methods. The top-left figure shows
the ground truth, the top-right figure represents the standard GP surrogate, and the second row displays
LC-prior GP with different number of the physical law corrected points


and evaluate their impact on the accuracy of the surrogate model and the MCMC posterior
distribution. Fig.8 shows that, compared with the GP surrogate model driven solely by data, our
method greatly reduces the error in the surrogate model. As the number of the θ law increases, the
performance of the LC-prior GP improves, but the optimization cost for ω also increases. The
specific relative errors are presented in Table 2, since the standard GP method does not require
θ law modification, the errors for standard GP only need calculation once. The experimental


27


results indicate that appropriately increasing the number of correction points can substantially
enhance the robustness of LC-prior GP predictions.
For the parameters estimation in the KdV equation, results are reflected in the posterior
samples of the MCMC, as shown in Table 3. The posterior samples obtained using LC-prior
GP with _N_ law = 9 exhibit a more accurate mean and smaller standard deviation compared to
both the standard GP and _N_ law = 4. All the experimental results above demonstrate that our
method provides a more competitive performance, even when dealing with nonlinear PDEs that
describe complex physical phenomena, as reflected in both the surrogate model errors and the
posterior distribution of samples.

#### **4.3 Incompressible miscible flooding model**



In this subsection, we test a model with multiple parameters. The incompressible miscible
flooding in the porous media is widely used in the engineering fields, such as the reservoir
simulation and the exploration of the underground water and oil. The classical equations are
given as follows:


∇· **u** = _q_, in (0, _T_ ) × Ω,
**u** = − µ( [κ] _c_ ) [∇] _[p]_ [,] in (0, _T_ ) × Ω, (13)

 ϕ _c_ _t_ + **u**                              - ∇ _c_ = ∇· ( **D** ( **u** )∇ _c_ ), in (0, _T_ ) × Ω,



∇· **u** = _q_, in (0, _T_ ) × Ω,
**u** = − µ( [κ] _c_ ) [∇] _[p]_ [,] in (0, _T_ ) × Ω,

ϕ _c_ _t_ + **u** - ∇ _c_ = ∇· ( **D** ( **u** )∇ _c_ ), in (0, _T_ ) × Ω,



(13)



where Ω ∈ R _[d]_ with _d_ = 2. The parameter κ represents the permeability, µ represents the
viscosity, and ϕ is the porosity. The unknown functions **u**, _p_ and _c_ are the velocity, pressure,
and concentration, respectively.
By replacing the velocity **u** with the pressure _p_, Eq.(13) is equivalent to:
 ϕ− _c_ µ _t_ ( [κ] − _c_ ) [∆] µ _[p]_ (κ _c_ [ =] ) [∇] _[ q][p]_ [,][ · ∇] _[c]_ [ =][ ∇·][ (] **[D]** [(] **[u]** [)][∇] _[c]_ [)][,] in (0in (0,, _T T_ )) × × Ω Ω,, (14)


with


**D** ( **u** ) = _d_ _m_ _I_ + | **u** | [�] _d_ _l_ _E_ ( **u** ) + _d_ _t_ ( _I_ − _E_ ( **u** )) [�],


where _I_ is the identity matrix, _d_ _m_ is the effective diffusion coefficient, _d_ _l_ is the longitudinal
dispersion coefficient, _d_ _t_ is the transverse dispersion coefficient, and _E_ ( **u** ) is the tensor that
projects onto the velocity **u** = ( _u_ 1, . . ., _u_ _d_ ) direction with each component defined as follows:


( _E_ ( **u** )) _i_, _j_ = _[u]_ _[i]_ _[u]_ _[j]_ [,] 1 ≤ _i_, _j_ ≤ _d_ .

| **u** | [2]


28


Table 4: The relative errors of GP method, LC-prior GP method and DMD-wiNN.


GP LC-prior GP DMD-wiNN


Two parameters example 0.3269 **0** . **1744** 0.2112
Three parameters example 0.2056 **0** . **1229** 0.1367


Figure 9: The strategy to chose training set and physics correction set (left) and some testing set for a
brief illustration (right).


For this example, we consider **D** ( **u** ) = _d_ _m_ _I_ and set _T_ = 0.1. We select an irregular region in
the two-dimensional space Ω. The radius of this region satisfies the following requirement:


_r_ _a_ = 1 + [sin][(][7][γ][)][ +][ sin][(][γ][)], γ ∈ [0, 2π].

10



The uniform division of the DistMesh is used for the spatial division. The computation domain
is [−1.5, 1.5] × [−1.5, 1.5], and the distance between the longest two points after the division is
_h_ = 0.04. The discrete numerical scheme of the temporal direction is as follows:

− [κ]

µ( _c_ ) [∆] _[p]_ _[n]_ [+][1] [ =] _[ q]_ _[n]_ [+][1] [,]

 ϕ _[c]_ _[n]_ [+][1] τ [−] _[c]_ _[n]_ − µ(κ _c_ ) [∇] _[p]_ _[n]_ [ · ∇] _[c]_ _[n]_ [+][1] [ −] _[d]_ _[m]_ [∆] _[c]_ _[n]_ [+][1] [ =][ 0][.]



− [κ]

µ( _c_ ) [∆] _[p]_ _[n]_ [+][1] [ =] _[ q]_ _[n]_ [+][1] [,]
ϕ _[c]_ _[n]_ [+][1] [−] _[c]_ _[n]_ − κ [∇] _[p]_ _[n]_




[−] _[c]_ _[n]_ − κ

τ µ( _c_ ) [∇] _[p]_ _[n]_ [ · ∇] _[c]_ _[n]_ [+][1] [ −] _[d]_ _[m]_ [∆] _[c]_ _[n]_ [+][1] [ =][ 0][.]



**4.3.1** **Two parameters example**


In this experiment, we aim to examine whether applying physics law corrections beyond
the training set can further improve the LC-prior GP’s extrapolation performance on the test
set, so the parameters κ and ϕ in Eq.(14) are uncertain. Here θ = (κ, ϕ). The training set is
constructed by selecting the central points from the prior distributions, i.e., κ = {−1, 1} from
π prior (κ) ∼U[−3, 3] and ϕ = {−2, 2} from π prior (ϕ) ∼U[−6, 6], yielding 4 training samples
from their Cartesian product, while the testing set contains 20 points randomly scattered in the


29


Figure 10: The first column presents the mean solutions of the RBF-FD method at _T_ =0.1, while columns
two to four display the results at _T_ =0.1 for GP, LC-prior GP and DMD-wiNN methods, respectively.
The first row shows the mean solutions, and the second row provides the corresponding errors.


Table 5: MCMC results for two parameters example with θ [∗] = (0.64, 4.97)


Method (Mean ± Std) of posterior


σ [2] obs [=][ 0][.][1] LC-prior GP **(0.68** ± **0.31, 5.12** ± **0.63)**
GP (2.21 ± 0.54, 5.30 ± 0.55)
σ ~~[2]~~ obs [=][ 0][.][2] LC-prior GP **(0.98** ± **0.33, 4.72** ± **0.48)**
GP (1.55 ± 0.51, 3.90 ± 0.35)


same intervals. Thus, the size of the training set is 4 and the size of the testing set is 400. For
the physics corrected set, 5 equally spaced points were sampled from each of the uniform prior
distribution, yielding a total of 25 data points. Fig.9 shows a concise schematic representation.
For this numerical example, Eq.(14) involves two unknown functions: pressure _p_ and concentration _c_ . Therefore, it is necessary to construct surrogate models for both _p_ ( _x_, _y_, _t_ ; θ) and
_c_ ( _x_, _y_, _t_ ; θ) simultaneously to enable subsequent correction through the physics corrected loss
function. During the parametric representation stage, the first three POD modes are selected as
the basis functions for both variables.

The Fig.10 visualizes the results of a two-parameter miscible flooding model at _t_ = 0.1,
comparing the performance of three methods across 400 test samples under the specified problem setup. The first row displays the mean solutions of each method on the test set, while the
second row presents their corresponding errors against the RBF-FD full-order mean solutions.
The proposed LC-prior GP method achieves relative errors on the order of 10 [−][3] . Additional
details are provided in the accompanying Table.4, which compares the aggregated errors of


30


Figure 11: The first column presents one physical law-corrected sample’s solutions of the RBF-FD
method at _T_ =0.1, while columns two to four display the results at _T_ =0.1 for GP, LC-prior GP and POD,
respectively. The first row shows the mean solutions, and the second row provides the corresponding

errors.


Table 6: MCMC results for three parameters example with θ [∗] = (1.45, −2.21, −4.68)


Method (Mean ± Std) of posterior


σ [2] obs [=][ 0][.][1] LC-prior GP **(1.59** ± **0.46, -2.41** ± **0.70, -4.47** ± **1.15)**
GP (1.31 ± 0.25, -1.08 ± 0.65, -1.57 ± 1.30)
σ ~~[2]~~ obs [=][ 0][.][2] LC-prior GP **(1.78** ± **0.26, -2.01** ± **0.27, -4.72** ± **0.43)**
GP (0.91 ± 0.28, 2.83 ± 0.49, -5.58 ± 0.62)


surrogate models at ten discrete time points within the interval [0, _T_ ]. Notably, the LC-prior GP
method demonstrates superior prediction accuracy over both standard GP method and DMDwiNN method.

In parameter estimation applications, to further evaluate the model’s robustness, we randomly selected a sample of testing set with θ [∗] = (0.64, 4.97) and added white Gaussian noise at
varying intensity levels to simulate observed measurements for MCMC sampling. The specific
results of the posterior distribution’s mean and standard deviation are presented in the Table.5.
The posterior samples based on the LC-prior GP surrogate demonstrate more accurate mean
estimates compared to those from the standard GP surrogate under both noise conditions.


31


Figure 12: The first column presents the mean solutions of the RBF-FD method at _T_ =0.1, while columns
two to four display the results at _T_ =0.1 for GP, LC-prior GP and DMD-wiNN methods, respectively.
The first row shows the mean solutions, and the second row provides the corresponding errors.


**4.3.2** **Three parameters example**


In this subsection, we further test the parameterization problem based on Eq.(14), where
the three parameters κ, µ, and ϕ are all uncertain. Thus, θ = (κ, µ, ϕ). For this example, the
size of the training set is 8 with evenly scattered 2 points in intervals π prior (κ) ∼U[−3, 3] for
κ, π prior (ϕ) ∼U[−6, 6] for ϕ and π prior (µ) ∼U[−10, 10] for µ. The size of the testing set is
8000 with evenly scattered 20 points and size of the physics corrected set is 64 with evenly
scattered 4 points in the same intervals. Here, the temporal and spatial discretization schemes
and the number of POD modes remain the same as in two parameters example, except for the
additional parameter µ.
To demonstrate the effectiveness of the physics-informed correction, we selected the sample
with the largest error in the physics-constrained set and visualized its correction performance.
Figure.11 presents the detailed results: column 1 shows the full-order RBF-FD solution as
reference; column 2 displays the standard GP method’s prediction, which exhibits poor performance (error ∼ 10 [−][2] ) under purely data-driven conditions; column 3 presents our proposed LCprior GP method and column 4 provides the POD approximation obtained from the full-order
solution and basis functions, representing the theoretical accuracy upper bound in our framework. The results clearly show that after optimization with the physics-informed loss function,
the prediction accuracy improves significantly, achieving a error of 10 [−][5] that approaches the
parameterization-determined accuracy limit.
The mean solutions for the numerical reference solution, GP surrogate, LC-prior GP surrogate and DMD-wiNN method at _t_ = 0.1 are shown in Fig.12. The LC-prior GP method
achieves a error of 10 [−][3], representing an order-of-magnitude improvement over the standard


32


Table 7: The relative errors of GP method, LC-prior GP method and DMD-wiNN.


GP LC-prior GP DMD-wiNN


**u** in the x-direction 0.0309 **0** . **0202** 0.0320

**u** in the y-direction 0.0265 **0** . **0163** 0.0264


GP approach. However, using partial datasets to characterize correction terms across the entire
parameter space introduces inherent prediction errors. This explains why individual corrected
samples may outperform the aggregated test set results. As quantified in Table.4 through relative error computations at all temporal discretization points, our proposed method maintains
best performance even for complex multi-parameter systems with irregular computational domains. The parameter estimation in this example, we follow the same workflow as described
in the previous section, with detailed numerical results presented in the accompanying Table.6.
Under our proposed framework, the posterior samples for systems with multi-physics coupling
and multiple parameters demonstrate more competitive performance compared to purely datadriven surrogate models.

#### **4.4 Incompressible Navier-Stokes model**


In practice, many fluid phenomena can be described using the incompressible NavierStokes model. Thus, in this subsection, we mainly consider the incompressible Navier-Stokes
model, where the parameters are viscosity and density. The incompressible Navier-Stokes
model with the Dirichlet boundary condition is as follows:







**u** _t_ + ( **u** - ∇) **u** − µ∆ **u** + [∇] ρ _[p]_ [=][ 0][,] in (0, _T_ ) × Ω,

∇· **u** = 0, in (0, _T_ ) × Ω,

**u** (0, ·) = **u** 0,



(15)



where Ω ∈ R [2], **u** = [ _u_, _v_ ] is the velocity, µ is the viscosity, _p_ is the pressure, and ρ is the
density. _u_ and _v_ represent the velocity components in the x- and y-directions, respectively. So
the Eq.(15) can be rewritten by:


2

∂ _u_ ∂ _u_ [∂] [2] _[u]_ + [1] ∂ _p_

∂ _t_ [+] _[ u]_ [∂] ∂ _[u]_ _x_ [+] _[ v]_ [∂] ∂ _[u]_ _y_ [−] [µ] � ∂ _x_ [2] [+] ∂ _y_ [2] � ρ ∂ _x_ [=][ 0][,]

∂ _v_ ∂ 2 _v_ ∂ _p_

+ [1]

[+][ ∂] [2] _[v]_

∂ _t_ [+] _[ u]_ ∂ [∂] _[v]_ _x_ [+] _[ v]_ [∂] ∂ _y_ _[v]_ [−] [µ] � ∂ _x_ [2] ∂ _y_ [2] � ρ ∂ _y_ [=][ 0][,]

in (0, _T_ ) × Ω. (16)

∂ _u_
∂ _x_ [+][ ∂] ∂ _[v]_ _y_ [=][ 0][,]

_u_ ( _x_, _y_, 0) = **u** 0 ( _x_, _y_ ),

 _v_ ( _x_, _y_, 0) = **u** 0 ( _x_, _y_ ),



+ [1]
� ρ

+ [1]
� ρ



∂ _u_

∂ _t_ [+] _[ u]_ [∂] ∂ _[u]_ _x_



2
∂ _u_ [∂] [2] _[u]_

[+]
∂ _x_ [2] ∂ _y_ [2]

∂ 2 _v_

[+][ ∂] [2] _[v]_
∂ _x_ [2] ∂ [2]



∂ _y_ [2]



∂ _y_ [2]



∂ _u_




[1] ∂ _p_

ρ ∂ _x_ [=][ 0][,]



∂ _v_



∂ _v_

∂ _t_ [+] _[ u]_ ∂ [∂] _[v]_ _x_




[∂] _[u]_

∂ _x_ [+] _[ v]_ [∂] ∂ _[u]_ _y_

[∂] _[v]_

∂ _x_ [+] _[ v]_ [∂] ∂ _[v]_



2

[∂] _[u]_ ∂ _u_

∂ _y_ [−] [µ] � ∂ _x_ [2]

∂ 2 _v_

[∂] _[v]_

∂ _y_ [−] [µ] � ∂ _x_ [2]



in (0, _T_ ) × Ω. (16)




[1] ∂ _p_

ρ ∂ _y_



∂ _y_ [=][ 0][,]



∂ _u_
∂ _x_ [+][ ∂] ∂ _[v]_



∂ _y_ [=][ 0][,]



_u_ ( _x_, _y_, 0) = **u** 0 ( _x_, _y_ ),

_v_ ( _x_, _y_, 0) = **u** 0 ( _x_, _y_ ),



33


Figure 13: The first column presents the mean solutions of the RBF-FD method at **u** in the x-direction
and _T_ =0.1, while columns two to four display the results at _T_ =0.1 for GP, LC-prior GP and DMDwiNN methods, respectively. The first row shows the mean solutions, and the second row provides the
corresponding errors.


For this example, we set _T_ = 0.1 and select an irregular region in two-dimensional space.
The radius of the region satisfies the same requirement in Eq.(13) of the last Subsection. The
uniform division of the DistMesh is used for the spatial division, the computation domain is

[−1.5, 1.5] × [−1.5, 1.5], and the distance between the longest two points after the division is
_h_ = 0.04. The initial value is given by:



π π

, π _x_ sin
2 [(] _[x]_ [2] [ +] _[ y]_ [2] [)] � � 2



π
**u** 0 ( _x_, _y_ ) = −π _y_ sin
� � 2



π ′

, _x_, _y_ ∈ Ω.
2 [(] _[x]_ [2] [ +] _[ y]_ [2] [)] ��



The discrete numerical scheme of the temporal direction is as follows:


**u** _[n]_ [+][1] τ− **u** _[n]_ + ( **u** _[n]_   - ∇) **u** _[n]_ − ν∆ **u** _[n]_ [+][1] + [∇] _[p]_ ρ _[n]_ [+][1] = 0,

 _p_ _[n]_ [+][1] τ− _p_ _[n]_ + ∇· **u** _[n]_ [+][1] = 0,



τ− **u** _[n]_ + ( **u** _[n]_ - ∇) **u** _[n]_ − ν∆ **u** _[n]_ [+][1] + [∇] _[p]_ _[n]_ [+][1]



**u** _[n]_ [+][1] − **u** _[n]_



_p_ _[n]_ [+][1] − _p_ _[n]_



ρ = 0,



τ− _p_ + ∇· **u** _[n]_ [+][1] = 0,



Here parameters µ and ρ are uncertainty, θ = (µ, ρ). The training set is evenly dispersed by
3 points in intervals π prior (µ) ∼U[0, 1] and π prior (ρ) ∼U[0.1, 1], the physical law corrected set
is randomly dispersed by 5 points and the testing set is randomly dispersed by 20 points in the
intervals. Thus, the size of the training set is 9, the corrected set contains 16 points (excluding
the overlapping 9 points between the corrected set and the training set), and the testing set
contains 400 points.
The Navier-Stokes model, as a classical system of partial differential equations governing
viscous fluid motion, involve complex coupling among multiple physical quantities. Therefore, in this section, we must simultaneously construct surrogate models for all three solution


34


Figure 14: The second derivatives of **u** in the x-direction and _T_ =0.1 obtained by the RBF-FD(the first
and third). The LC-prior GP results are the second one and the fourth one.


Figure 15: The first column presents the mean solutions of the RBF-FD method at **u** in the y-direction
and _T_ =0.1, while columns two to four display the results at _T_ =0.1 for GP, LC-prior GP and DMDwiNN methods, respectively. The first row shows the mean solutions, and the second row provides the
corresponding errors.


fields: the velocity components _u_ ( _x_, _y_, _t_ ; θ) and _v_ ( _x_, _y_, _t_ ; θ) in both coordinate directions and the
pressure field _p_ ( _x_, _y_, _t_ ; θ). For each physical quantity, we parameterize the solutions using basis
functions constructed from the first 4 POD modes.
In this section, we compute both the mean solution of the velocity field **u** on the test set
and evaluate its second-order derivative characteristics using the evaluation and differentiation
matrices derived from the RBF-FD method. Fig.13 and Fig.15 present the computational results for the x-direction and y-direction at _t_ = 0.1 respectively. The first column displays the
full-order reference solution obtained through numerical methods, while columns 2 through 4
show the mean solutions generated by the GP, LC-prior GP and DMD-wiNN methods along
with their corresponding errors relative to the reference solution. Within this complex system,
while the DMD-wiNN method demonstrates significantly increased errors compared to previous cases, our proposed method maintains exceptionally small error magnitudes, conclusively


35


Figure 16: The second derivatives of **u** in the y-direction and _T_ =0.1 obtained by the RBF-FD(the first
and third). The LC-prior GP results are the second one and the fourth one.


demonstrating its generalization capability. Table.7 provides a comprehensive error analysis,
quantifying the aggregate relative errors of the surrogate model across all temporal discretization points within [0, _T_ ].
Moreover, the precomputed evaluation matrix _E_ _h_ ( _X_, _Y_ ) and differentiation matrices such as
_D_ [L] _h_ _[x]_ [(] _[X]_ [,] _[ Y]_ [) and] _[ D]_ [L] _h_ _[xx]_ [(] _[X]_ [,] _[ Y]_ [) obtained during training set generation enable e][ffi][cient and accurate]
computation of higher-order derivatives for the target functions. Let us denote the surrogate
model predictions by ˆ _u_ and ˆ _v_, so the second-order derivative can be calculated by:


∂ [2] _u_ ˆ ∂ [2] _v_ ˆ

[=] _[ D]_ _h_ [L] _[xx]_ [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] _[u]_ [,] [=] _[ D]_ _h_ [L] _[xx]_ [(] _[X]_ [,] _[ Y]_ [)] _[E]_ _h_ [†] [(] _[Y]_ [,] _[ X]_ [)ˆ] _[v]_ [.]
∂ _x_ [2] ∂ _x_ [2]


Similarly, we can compute ∂ [2] _u_ ˆ/∂ _y_ [2] and ∂ [2] _v_ ˆ/∂ _y_ [2] in the same manner. Fig.14 and Fig.16
present the second-order derivative results of **u** along different spatial directions, where the
first and third columns display the full-order reference solutions, while the second and fourth
columns show the predictions obtained using the LC-prior GP method. These results demonstrate that the proposed method maintains high accuracy in approximating multiple target quantities, even for complex computational domains with coupled multi-physics interactions.

### **5 Conclusion**


We present a method for improving computation efficiency and accuracy of numerical estimations for both parametric DEs solutions and unknown parameters using a GP surrogate with
a physical law-corrected prior. This approach is applicable to both PDEs defined on complex
geometries or multi-physics interaction system. The method employs the POD method to map
the high-dimensional solution space of the DEs into a low-dimensional basis coefficient space.
The GP surrogate is then trained to predict the basis coefficients from the DE parameters. To
enhance the model further, we introduce a correction mechanism that incorporates physical
law constraints, refining the GP surrogate’s prior. This physics-informed machine learning
approach leverages both solution training data and the underlying physical model, offering improved performance.


36


Additionally, we use the learned forward model to infer the posterior distribution of unknown parameters from noisy observations. By combining data-driven modeling with physical
law constraints, our method effectively handles complex equations and demonstrates robust
performance even in the presence of noisy observations with varying magnitudes.
Despite its strengths, there are limitations and opportunities for future work. First, the current approach samples correction model points uniformly from the coefficient model’s samples,
whereas a more effective strategy may involve selecting points with high GP surrogate prediction variance in the parameter space. Second, when using POD to parameterize the DE, the
number of discrete grids in the predicted DE solution is constrained to the number of grids in
the sample data. This limitation is absent when using conventional orthogonal bases. Future
work will explore methods to reconstruct more sophisticated discrete solutions within the POD
framework.

### **Acknowledgment**


Heng Yong acknowledges the support of National Safety Academic Fund (NSAF) under
Grant No.U2230208 and the National Natural Science Foundation of China (NSFC) under
Grant Number 12331010. Hongqiao Wang acknowledges the support of National Natural Science Foundation of China (NSFC) under Grant Number 12271562. The work was supported
by the Major Scientific and Technological Innovation Platform Project of Human Province
(2024JC1003). This work was carried out in part using computing resources at the High Performance Computing Center of Central South University.

### **References**


[1] WE Schiesser. Time Delay ODE/PDE Models: Applications in Biomedical Science and

Engineering. CRC Press, 2019.


[2] Xinhai Chen, Jie Liu, Shengguo Li, Peizhen Xie, Lihua Chi, and Qinglin Wang. Tamm:
A new topology-aware mapping method for parallel applications on the tianhe-2a supercomputer. In Algorithms and Architectures for Parallel Processing: 18th International

                                        Conference, ICA3PP 2018, Guangzhou, China, November 15 17, 2018, Proceedings, Part
I 18, pages 242–256. Springer, 2018.


[3] Ameya D Jagtap, Ehsan Kharazmi, and George Em Karniadakis. Conservative physicsinformed neural networks on discrete domains for conservation laws: Applications to forward and inverse problems. Computer Methods in Applied Mechanics and Engineering,
365:113028, 2020.


37


[4] Guofei Pang and George Em Karniadakis. Physics-informed learning machines for partial
differential equations: Gaussian processes versus neural networks. Emerging frontiers in
nonlinear science, pages 323–343, 2020.


[5] John David Anderson and John Wendt. Computational fluid dynamics, volume 206.
Springer, 1995.


[6] Siddhartha Mishra. A machine learning framework for data driven acceleration of computations of differential equations. arXiv preprint arXiv:1807.09519, 2018.


[7] Pau Batlle, Matthieu Darcy, Bamdad Hosseini, and Houman Owhadi. Kernel methods are
competitive for operator learning. Journal of Computational Physics, 496:112549, 2024.


[8] Carlos Mora, Amin Yousefpour, Shirin Hosseinmardi, Houman Owhadi, and Ramin
Bostanabad. Operator learning with gaussian processes. Computer Methods in Applied
Mechanics and Engineering, 434:117581, 2025.


[9] Samuel Rudy, Alessandro Alla, Steven L Brunton, and J Nathan Kutz. Data-driven identification of parametric partial differential equations. SIAM Journal on Applied Dynamical
Systems, 18(2):643–660, 2019.


[10] Tapas Tripura and Souvik Chakraborty. Wavelet neural operator for solving parametric
partial differential equations in computational mechanics problems. Computer Methods
in Applied Mechanics and Engineering, 404:115783, 2023.


[11] Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. Science advances,
7(40):eabi8605, 2021.


[12] Marc C Kennedy and Anthony O’Hagan. Bayesian calibration of computer models.
Journal of the Royal Statistical Society: Series B (Statistical Methodology), 63(3):425–
464, 2001.


[13] Xinhai Chen, Rongliang Chen, Qian Wan, Rui Xu, and Jie Liu. An improved data-free
surrogate model for solving partial differential equations using deep neural networks.
Scientifc reports,i 11(1):19507, 2021.


[14] Majdi I Radaideh and Tomasz Kozlowski. Surrogate modeling of advanced computer
simulations using deep gaussian processes. Reliability Engineering & System Safety,
195:106731, 2020.


[15] Rapha¨el Pestourie, Youssef Mroueh, Chris Rackauckas, Payel Das, and Steven G Johnson. Physics-enhanced deep surrogates for partial differential equations. Nature Machine
Intelligence, 5(12):1458–1465, 2023.


38


[16] Yongchao Li, Yanyan Wang, and Liang Yan. Surrogate modeling for bayesian inverse
problems based on physics-informed neural networks. Journal of Computational Physics,
475:111841, 2023.


[17] David J Lucia, Philip S Beran, and Walter A Silva. Reduced-order modeling: new approaches for computational physics. Progress in aerospace sciences, 40(1-2):51–117,
2004.


[18] Olivier Ezvan, Anas Batou, Christian Soize, and Laurent Gagliardini. Multilevel
model reduction for uncertainty quantification in computational structural dynamics.
Computational Mechanics, 59(2):219–246, 2017.


[19] Jan Hesthaven, Cecilia Pagliantini, and Nicol`o Ripamonti. Adaptive symplectic model
order reduction of parametric particle-based vlasov–poisson equation. Mathematics of
Computation, 93(347):1153–1202, 2024.


[20] Federico Pichi, Beatriz Moya, and Jan S Hesthaven. A graph convolutional autoencoder
approach to model order reduction for parametrized pdes. Journal of Computational
Physics, 501:112762, 2024.


[21] Akhil Nekkanti and Oliver T Schmidt. Gappy spectral proper orthogonal decomposition.

Journal of Computational Physics, 478:111950, 2023.


[22] Stefania Fresca and Andrea Manzoni. Pod-dl-rom: Enhancing deep learning-based reduced order models for nonlinear parametrized pdes by proper orthogonal decomposition.
Computer Methods in Applied Mechanics and Engineering, 388:114181, 2022.


[23] JP Moitinho de Almeida. A basis for bounding the errors of proper generalised decomposition solutions in solid mechanics. International Journal for Numerical Methods in
Engineering, 94(10):961–984, 2013.


[24] Francisco Chinesta, Amine Ammar, Adrien Leygue, and Roland Keunings. An overview
of the proper generalized decomposition with applications in computational rheology.
Journal of Non-Newtonian Fluid Mechanics, 166(11):578–592, 2011.


[25] Bruce Moore. Principal component analysis in linear systems: Controllability, observability, and model reduction. IEEE transactions on automatic control, 26(1):17–32, 1981.


[26] Ulrike Baur, Christopher Beattie, Peter Benner, and Serkan Gugercin. Interpolatory
projection methods for parameterized model reduction. SIAM Journal on Scientifci
Computing, 33(5):2489–2518, 2011.


[27] Jan S Hesthaven and Stefano Ubbiali. Non-intrusive reduced order modeling of nonlinear
problems using neural networks. Journal of Computational Physics, 363:55–78, 2018.


39


[28] Kyung Hyun Park, Sang Ook Jun, Sung Min Baek, Maeng Hyo Cho, Kwan Jung Yee, and
Dong Ho Lee. Reduced-order model with an artificial neural network for aerostructural
design optimization. Journal of Aircraft, 50(4):1106–1116, 2013.


[29] Huailing Song, Yuming Ba, Dongqin Chen, and Qiuqi Li. A model reduction method for
parametric dynamical systems defined on complex geometries. Journal of Computational
Physics, 506:112923, 2024.


[30] Victor Bayona, Miguel Moscoso, Manuel Carretero, and Manuel Kindelan. Rbf-fd formulas and convergence properties. Journal of Computational Physics, 229(22):8281–8295,
2010.


[31] Varun Shankar. The overlapped radial basis function-finite difference (rbf-fd) method: A
generalization of rbf-fd. Journal of Computational Physics, 342:211–228, 2017.


[32] Victor Bayona, Natasha Flyer, Bengt Fornberg, and Gregory A Barnett. On the role of
polynomials in rbf-fd approximations: Ii. numerical solution of elliptic pdes. Journal of
Computational Physics, 332:257–273, 2017.


[33] Robert Schaback and Holger Wendland. Using compactly supported radial basis functions
to solve partial differential equations. WIT Transactions on Modelling and Simulation,
23, 2024.


[34] Gal Berkooz, Philip Holmes, and John L Lumley. The proper orthogonal decomposition
in the analysis of turbulent flows. Annual review of fuid mechanics,l 25(1):539–575,
1993.


[35] Ngoc Cuong Nguyen and Andrew Rohskopf. Proper orthogonal descriptors for efficient
and accurate interatomic potentials. Journal of Computational Physics, 480:112030, 2023.


[36] Robert M Miura. The korteweg–devries equation: a survey of results. SIAM review,
18(3):412–459, 1976.


40


