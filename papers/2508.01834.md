# **Efficient optimization of expensive black-box** **simulators via marginal means, with** **application to neutrino detector design**

#### Hwanwoo Kim [∗], Simon Mak [∗] [†], Ann-Kathrin Schuetz [‡], Alan Poon [‡] [§] August 8, 2025

**Abstract**

With advances in scientific computing, computer experiments are increasingly
used for optimizing complex systems. However, for modern applications, e.g., the
optimization of nuclear physics detectors, each experiment run can require hundreds
of CPU hours, making the optimization of its black-box simulator _f_ over a highdimensional space _X_ a challenging task. Given limited runs at inputs **x** 1 _, · · ·,_ **x** _n_ _∈X_,
the best solution from these evaluated inputs can be far from optimal, particularly as
dimensionality increases. Existing black-box methods, however, largely employ this
“pick-the-winner” (PW) solution, which leads to mediocre optimization performance.
To address this, we propose a new Black-box Optimization via Marginal Means
(BOMM) approach. The key idea is a new estimator of a global optimizer **x** _[∗]_ that
leverages the so-called marginal mean functions, which can be efficiently inferred
with limited runs in high dimensions. Unlike PW, this estimator can select solutions
beyond evaluated inputs for improved optimization performance. Assuming _f_ follows
a generalized additive model with unknown link function and under mild conditions,
we prove that the BOMM estimator not only is consistent for optimization, but also
has an optimization rate that tempers the “curse-of-dimensionality” faced by existing
methods, thus enabling better performance as dimensionality increases. We present a
practical framework for implementing BOMM using the transformed Gaussian process
surrogate model in Lin and Joseph [2020]. Finally, we demonstrate the effectiveness of
BOMM in numerical experiments and an application on neutrino detector optimization
in nuclear physics.


_Keywords_ : Bayesian Optimization, Black-Box Optimization, Computer Experiments, Detector Design, Gaussian Process, Uncertainty Quantification.


∗ Department of Statistical Science, Duke University

  - SM and HK are supported by NSF CSSI 2004571, NSF DMS 2210729, 2316012 and DE-SC0024477.

  - Nuclear Science Division, Lawrence Berkeley National Laboratory
§ Lawrence Berkeley National Laboratory is operated by the University of California under the U.S.
Department of Energy Federal Prime Agreement DE-AC02-05CH11231.


1


### **1 Introduction**

Scientific computing is undergoing rapid development. With recent progress, complex


phenomena, e.g., rocket engines [Mak et al., 2018], universe expansion [Kaufman et al., 2011]


and particle collisions [Ji et al., 2024a,b], can now be reliably simulated via virtual simulation.


These “computer experiments” [Gramacy, 2020; Deng et al., 2025] offer an appealing


alternative to physical experiments [Wu and Hamada, 2009], which may be impractical


or infeasible in modern applications. However, such virtual experiments often incur high


computational costs that hamper their use for scientific decision-making, particularly for


optimizing the simulated response surface _f_ ( _·_ ) over a design space _X_ . We face this bottleneck


in our motivating application of designing complex detectors for neutrinoless double-beta


decay searches [Dolinski et al., 2019]. Such a decay mechanism provides important insight


into the fundamental matter-antimatter asymmetry in the Universe [Canetti et al., 2012],


but its detection requires careful detector optimization to suppress cosmogenic backgrounds.


While virtual simulators provide an appealing strategy for detector optimization, the


simulation of a single detector design can require hundreds of CPU hours, which makes its


optimization a highly challenging task.


A proven solution is probabilistic surrogate modeling [Overstall and Woods, 2016]. The


idea is to run the computer experiment at designed input points **x** 1 _, · · ·,_ **x** _n_ _∈X ⊂_ R _[d]_,


then use the simulated data [ _f_ ( **x** _i_ )] _[n]_ _i_ =1 [to fit a probabilistic model that predicts] _[ f]_ [ with]


uncertainty at untested inputs. A popular surrogate choice is the Gaussian process (GP;


Rasmussen and Williams, 2006; Stein, 2012), which provides flexible probabilistic modeling


with closed-form predictive equations. This not only permits efficient exploration of _f_ over


the design space _X_, but also facilitates timely downstream scientific decision-making, e.g.,


optimization [Miller and Mak, 2025; Kim and Sanz-Alonso, 2025] and inverse problems


[Ehlers et al., 2025; Kim et al., 2024]. Recent developments on GP surrogates include


the use of deeper architectures [Sauer et al., 2023; Montagna and Tokdar, 2016] and the


2


incorporation of domain physics [Ding et al., 2025; Golchi et al., 2015].


We consider the specific task of minimizing [1] the expensive black-box function _f_ :


**x** _[∗]_ _∈_ Argmin _f_ ( **x** ) _,_ (1)
**x** _∈X_


which is critical for many facets of decision-making via computer experiments, including


system optimization [Paulson and Tsay, 2025] and control [Miller et al., 2024]. Here,


Argmin denotes the set of input points that minimize _f_ . Existing “black-box optimization”


approaches can be classified as sequential or one-shot methods. Sequential methods perform


sequential (or batch-sequential) evaluations of _f_, where each input **x** _n_ is adaptively selected


using evaluation data from previous inputs **x** 1 _, · · ·,_ **x** _n−_ 1 . Such methods have received much


attention in the Bayesian optimization literature; see, e.g., Jones et al. [1998]; Chen et al.


[2024]; Frazier et al. [2008]. However, for expensive computer simulators, the high cost of a


single run can be a barrier for sequential methods. For example, in our detector optimization


application, a high-fidelity simulation for a single detector design can require hundreds


of CPU hours, which prevents any adaptive iterations when a decision needs to be made


promptly. In such a scenario, _one-shot_ methods that simultaneously perform all runs may


be more feasible. One-shot methods are facilitated by the rise of distributed computing,


which permits the simultaneous evaluation of _f_ at many inputs via multi-core processing.


We will focus on such one-shot methods here, as motivated by our application.


Existing one-shot black-box optimization approaches broadly fall into two categories


[Thomaser et al., 2022]. The first adopts the simple but intuitive strategy of picking the


best solution ˆ **x** _[∗]_ _n_ [=] arg min _f_ ( **x** ) amongst the evaluated inputs. This was coined the
**x** _∈{_ **x** 1 _,···,_ **x** _n_ _}_

“pick-the-winner” (PW) approach in Wu et al. [1990] and Mak and Wu [2019], and is broadly


used in practice. Given limited runs over a high-dimensional space _X_, however, the evaluated


inputs can be far from optimal, in which case PW may yield mediocre performance. The


1
Here, one can easily maximize _f_ by minimizing the modified objective _−f_ .


3


second strategy is to first fit a surrogate model _f_ [ˆ] _n_ ( _·_ ) from data, then “infer” **x** _[∗]_, i.e., infer an


optimal solution from (1), via its minimizer ˆ **x** _[∗]_ _n_ [=] [ arg min] _f_ ˆ _n_ ( **x** ). While this surrogate-based
**x** _∈X_

approach may yield improvements over PW when the surrogate fits well globally, this is


by no means guaranteed; when this fit is poor, such approaches may perform worse than


PW. For high-dimensional spaces _X_, surrogate-based approaches may further face a “curse

of-dimensionality” [Bellman, 1966], in that the surrogate fit becomes increasingly poor as


dimension _d_ increases. This is well-known for GP surrogates, which have an _L_ _∞_ -prediction


rate of _O_ ( _n_ _[−][ν/d]_ ) using the Mat´ern kernel [Stein, 2012] with smoothness parameter _ν >_ 0;


see Wu and Schaback [1993]; Wendland [2004]. This exponential dependence of sample size


_n_ on _d_ can result in rapid deterioration of surrogate (and thus optimization) performance as


dimensionality increases [Ding et al., 2019]. A similar curse-of-dimensionality is also present


for sequential Bayesian optimization methods [Bull, 2011; Kim et al., 2025].


To address this, we propose a new Black-box Optimization via Marginal Means (BOMM)


approach for one-shot black-box optimization. The key idea is to construct a new BOMM


estimator ˆ **x** _[∗]_ _n_ [of an optimizer] **[ x]** _[∗]_ [that depends on the so-called marginal mean functions.]


In contrast to PW, our BOMM estimator can select solutions beyond evaluated inputs to


improve black-box optimization with limited data. In contrast to surrogate-based approaches,


which require the challenging task of a good surrogate fit over the _full_ domain _X_, the


marginal mean functions in BOMM can be effectively estimated in high dimensions with


limited runs. Assuming _f_ follows a generalized additive model [Hastie and Tibshirani,


1990] with unknown link function and under mild regularity conditions, we prove that the


ˆ
BOMM optimality gap _|f_ ( **x** _[∗]_ _n_ [)] _[−]_ _[f]_ [(] **[x]** _[∗]_ [)] _[|]_ [ not only converges to zero, but does so at a rate with]


considerably less dependence on dimensionality than existing methods, thus tempering the


curse-of-dimensionality and facilitating good performance as _d_ increases. We then present


a methodological framework, which leverages the transformed approximate additive GP


model in Lin and Joseph [2020] for an effective implementation of BOMM. Finally, we


demonstrate the effectiveness of BOMM over the state-of-the-art in a suite of numerical


4


experiments and for our motivating application of neutrino detector optimization.


There are important practical considerations when inferring an optimal solution beyond


evaluated points. Despite its limitations, one appeal of PW is its reliability: its inferred


solution ˆ **x** _[∗]_ _n_ [is naturally validated by an evaluated point. This is desirable in applications]


where final design decisions are made promptly after inference. For complex scientific


applications (e.g., detector design), however, the inferred solution ˆ **x** _[∗]_ _n_ [is typically but one]


step in the design process; such a solution is then further investigated and validated by


scientists prior to design decisions. For such problems, the validation of ˆ **x** _[∗]_ _n_ [within the]


black-box optimization procedure is not essential, and the improvement gained from inferring


beyond evaluated points can be highly beneficial with limited runs, as we show later.


This paper is organized as follows. Section 2 provides background on GPs, existing one

shot black-box methods, and their potential limitations in motivating experiments. Section


3 presents the proposed BOMM estimator and proves its optimization consistency and


associated rate. Section 4 outlines a comprehensive methodological framework for effective


implementation. Sections 5 and 6 investigate the performance of BOMM in numerical


experiments and an application on detector optimization. Section 7 concludes the paper.

### **2 Background and Motivation**


We first give a brief review of GPs, then outline existing one-shot black-box optimization


methods and their potential limitations in a motivating experiment.

#### **2.1 Gaussian process modeling**


Let _f_ : _X →_ R be the black-box function to optimize, where _X_ is its design space. In


what follows, we presume _X_ to be a rectangular domain of the form _X_ = [�] _[d]_ _l_ =1 [[] _[L]_ _[l]_ _[, U]_ _[l]_ [], where]


_L_ _l_ and _U_ _l_ are the lower and upper limits for the _l_ -th input variable. Given the black-box


nature of _f_, one can adopt a Gaussian process (GP; Rasmussen and Williams, 2006) prior on


5


_f_ : _f_ ( _·_ ) _∼_ GP _{µ, k_ ( _·, ·_ ) _}_ . Here, _µ_ is a mean parameter that can be estimated from data, and


_k_ ( _·, ·_ ) is a kernel function that controls sample path smoothness. Common kernel choices


include the squared-exponential and the Mat´ern kernels [Stein, 2012; Gramacy, 2020].


Next, suppose the expensive computer simulator is evaluated at _n_ designed input points


**x** 1 _, · · ·,_ **x** _n_, yielding data **f** _n_ = [ _f_ ( **x** 1 ) _, · · ·, f_ ( **x** _n_ )]. In what follows, we presume that the


simulator is deterministic, in that it returns the same output _f_ ( **x** ) given the same input


**x** . This is commonly assumed in the computer experiments literature, particularly when


_f_ solves a deterministic partial differential equation system. One can easily account for


Gaussian simulation noise by incorporating a nugget term in the predictive equations below;


see Peng and Wu [2014]. Conditional on data **f** _n_, the predictive distribution of _f_ ( **x** new ) at


an untested point **x** new can be shown to be [ _f_ ( **x** new ) _|_ **f** _n_ ] _∼N{f_ [ˆ] _n_ ( **x** new ) _, σ_ _n_ [2] [(] **[x]** [new] [)] _[}]_ [, where:]


ˆ
_f_ _n_ ( **x** new ) = _µ_ + **k** _[T]_ _n_ [(] **[x]** [new] [)] **[K]** _[−]_ _n_ [1] [(] **[f]** _[n]_ _[−]_ _[µ]_ **[1]** [)] _[,]_ (2)

_σ_ _n_ [2] [(] **[x]** [new] [) =] _[ k]_ [(] **[x]** [new] _[,]_ **[ x]** [new] [)] _[ −]_ **[k]** _[T]_ _n_ [(] **[x]** [new] [)] **[K]** _[−]_ _n_ [1] **[k]** _[n]_ [(] **[x]** [new] [)] _[.]_


Here, **K** _n_ = [ _k_ ( **x** _i_ _,_ **x** _j_ )] _[n]_ _i,j_ =1 [and] **[ k]** _[n]_ [(] **[x]** [new] [) = [] _[k]_ [(] **[x]** [new] _[,]_ **[ x]** _[i]_ [)]] _[n]_ _i_ =1 [. Equation] [ (2)] [ provides the basis]


for efficient probabilistic surrogate modeling of _f_ ( _·_ ) over the input space _X_ .

#### **2.2 Existing one-shot black-box optimization methods**


As mentioned in the Introduction, existing one-shot black-box optimization methods can


be broadly categorized as pick-the-winner and surrogate-based approaches; these approaches


differ in how they “estimate” [2] an optimal solution **x** _[∗]_ . PW-based approaches [Wu et al.,


1990] are simple but intuitive: they select the best observed solution ˆ **x** _[∗]_ _n_ [=] arg min _f_ ( **x** )
**x** _∈{_ **x** 1 _,···,_ **x** _n_ _}_

amongst the evaluated points. The PW estimator of **x** _[∗]_ is commonly used in practice. One


reason is that such an estimator is “robust” [Mak and Wu, 2019], in that it does not select


points on which _f_ has not been evaluated. However, given a limited sample size _n_ (due to


2 For black-box optimization, the quality of an estimator ˆ **x** _∗n_ [for an optimal solution] **[ x]** _[∗]_ [is typically gauged]
by its optimality gap _f_ (ˆ **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [).]


6


the costly nature of _f_ ), the evaluated design points can be far from optimal, meaning the


PW estimator may yield mediocre optimization performance.


**x** _[∗]_ .
Surrogate-based optimization (SBO) approaches employ an alternate estimator of


One first uses the collected data on _f_ to train a surrogate model _f_ [ˆ] _n_ ( _·_ ), then selects the


ˆ
optimizer of this surrogate ˆ **x** _[∗]_ _n_ [=] [ arg min] _f_ _n_ ( **x** ) as its estimate of **x** _[∗]_ . When the trained
**x** _∈X_

surrogate _f_ [ˆ] _n_ fits well globally, surrogate-based approaches can provide improved optimization


over PW [Thomaser et al., 2022]; when this is not the case, however, such approaches may


perform worse than PW. This phenomenon is exacerbated when _X_ is high-dimensional,


where surrogate quality can deteriorate quickly given a limited sample size _n_ [Ding et al.,


2019]. This “curse-of-dimensionality” is well-known for GP surrogates: for a GP with


an isotropic Mat´ern kernel _k_ [Stein, 2012] and smoothness parameter _ν >_ 0 (we call this


the “Mat´ern- _ν_ GP” later), one can show [Wu and Schaback, 1993; Wendland, 2004] that


its _L_ _∞_ -prediction rate is _∥f −_ _f_ [ˆ] _n_ _∥_ _∞_ = _O_ ( _n_ _[−][ν/d]_ ) with optimally selected design points,


where _f_ is in the reproducing kernel Hilbert space (RKHS; Aronszajn, 1950) for kernel


_k_, denoted _F_ . The optimality gap using such a surrogate thus follows a similar rate of


ˆ
_|f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)] _[|]_ [ =] _[ O]_ [(] _[n]_ _[−][ν/d]_ [) for] _[ f][ ∈F]_ [. The exponential dependence of sample size] _[ n]_ [ on]


dimension _d_ in this rate suggests that the performance of SBO methods can quickly worsen


as dimension increases.

#### **2.3 Motivating experiments**


To highlight these limitations of PW-based and surrogate-based approaches for one-shot


black-box optimization, we explore two motivating experiments in the challenging setting


with limited runs in a (moderately) high-dimensional space. We consider two test functions


in the computer experiments literature [Surjanovic and Bingham, 2013]: the six-hump


camel function in _d_ = 6 dimensions, and the wing weight function in _d_ = 10 dimensions;


their specific forms are provided in Appendix G. For each function, we take a one-shot


7


design of _n_ = 10 _d_ points from a maximin Latin hypercube design [Morris and Mitchell,


1995] scaled to _X_ . For SBO, we consider two surrogate choices: a GP surrogate using


the square-exponential kernel (SBO-SqExp), and a deep GP [Sauer et al., 2023] surrogate


(SBO-DGP). Experimental details are provided later in Section 5.


Figure 1 shows the box

plots of the log-optimality gaps


ˆ
log _|f_ ( **x** _[∗]_ _n_ [)] _[−][f]_ [(] **[x]** _[∗]_ [)] _[|]_ [ for each method]


over 20 replications. There are sev

eral observations to note. First, the


simple PW estimator yields large


optimality gaps for the 10-d wing



weight experiment. This is not sur

prising, as the limited evaluated


points are likely far from optimal,



**Figure 1:** _Log-optimality gaps of the compared methods for_
_the six-hump camel and wing weight functions. Boxplots_
_show experiment variability over 20 replications for each_
_method._



particularly on a _d_ = 10-dimensional space _X_ . Second, the surrogate-based optimizers


perform better than PW for the wing weight function, but worse for the six-hump camel


function. A plausible reason is that the latter function is more complex over its domain:


given a small sample size, a good global surrogate fit becomes more challenging, resulting in


worse surrogate-based-optimization performance. This reliance on a good global surrogate


fit can make SBO methods unreliable, particularly with limited runs in moderate-to-high


dimensions. To foreshadow, the proposed BOMM addresses these limitations (see Figure 1)


via a new estimator for **x** _[∗]_ that relies on marginal mean functions, which can be effectively


estimated from limited data in high dimensions; we explore this next.


8


### **3 Black-box Optimization via Marginal Means**

We present next our BOMM framework, which employs a new estimator for **x** _[∗]_ using


marginal mean functions. We first outline its estimation framework, then prove its opti

mization consistency and associated rate under mild regularity conditions. Such a rate


tempers the curse-of-dimensionality noted earlier for existing black-box methods, enabling


better optimization performance as _d_ increases. A methodological framework for robust


implementation is presented later in Section 4.

#### **3.1 The BOMM estimator**


Suppose the black-box function _f_ follows the general model:


_f_ ( **x** ) = _ϕ ◦_ _h_ ( **x** ) _,_ _h_ ( **x** ) = _h_ 1 ( _x_ 1 ) + _· · ·_ + _h_ _d_ ( _x_ _d_ ) + _ζ_ ( **x** ) _,_ (3)


where _ϕ_ _◦_ _h_ ( **x** ) = _ϕ{h_ ( **x** ) _}_ denotes the composition of functions _ϕ_ and _h_ . Here, _ϕ_ is a strictly


monotone (and thus invertible) link function to be estimated from data, _h_ 1 ( _x_ 1 ) _, · · ·, h_ _d_ ( _x_ _d_ )


are additive functions on each input, and _ζ_ ( **x** ) accounts for “mild” deviations from additivity


for _h_ ( **x** ); more on this later. Without loss of generality, we presume in the following that _ϕ_


is strictly monotonically increasing, as one can account for the monotonically decreasing


case by reversing the sign on _h_ ( **x** ).


Note that, with _ζ_ ( **x** ) = 0, the model (3) reduces to a _generalized additive model_ (GAM;


Hastie and Tibshirani, 1990) with unknown link function. GAMs are widely used in the


statistical learning literature [Hastie et al., 2009; Rudin et al., 2022] due to its flexible


modeling framework and interpretability. A key appeal of a GAM is that it provides some


relief from the curse-of-dimensionality for high-dimensional regression [Stone, 1986], by


leveraging an additive structure after link transformation. Its form can further be justified


via the well-known Kolmogorov-Arnold representation theorem (see, e.g., Tikhomirov, 1991).


9


The inclusion of _ζ_ ( **x** ) enhances model flexibility by accounting for potential deviations


from additivity in _h_ ( **x** ). This use of a carefully specified transformation for near-additive


modeling has a long history in statistics, going back to the Box-Cox transformation [Box and


Cox, 1964] and ANOVA modeling [Wu and Hamada, 2009]. We adopt later a probabilistic


modeling framework for (3) using the transformed approximate additive GP in Lin and


Joseph [2020] to guide BOMM optimization.


Next, define the so-called transformed _marginal mean_ functions of _f_ :


_m_ _l_ ( _x_ _l_ ) = _ϕ_ _[−]_ [1] _◦_ _f_ ( **x** ) _d_ **x** _−l_ _,_ _l_ = 1 _, · · ·, d._ (4)
� _X_ _−l_


Here, **x** _−l_ refers to all variables in **x** except _x_ _l_, and _X_ _−l_ denotes its domain. For an input _l_,


such a function marginalizes the transformed response surface _ϕ_ _[−]_ [1] _◦_ _f_ over the remaining


_d −_ 1 inputs. Given data **f** _n_ on _f_ at design points, let ˆ _m_ _l_ ( _x_ _l_ ) denote the estimator of this


marginal mean function for input _l_ ; we will discuss how to construct such an estimator later.


The BOMM estimator ˆ **x** _[∗]_ _n_ [= (ˆ] _[x]_ _[∗]_ _n,_ 1 _[,][ · · ·][,]_ [ ˆ] _[x]_ _[∗]_ _n,d_ [) for] **[ x]** _[∗]_ [then takes the following form:]


_x_ ˆ _[∗]_ _n,l_ [= argmin] _m_ ˆ _l_ ( _x_ _l_ ) _,_ _l_ = 1 _, · · ·, d._ (5)
_x_ _l_


In words, the _l_ -th element of the BOMM estimator is taken as the minimizer of the estimated


marginal mean function ˆ _m_ _l_ ( _x_ _l_ ) for the _l_ -th input.


One way to intuit this estimator is as follows. Suppose _f_ follows a GAM (i.e., the model


in (3) with _ζ_ ( **x** ) = 0), and suppose its link function _ϕ_ and additive functions _h_ 1 _, · · ·, h_ _d_


are known. Then the solution ˜ **x** _[∗]_ = ( _x_ ˜ _[∗]_ 1 _[,][ · · ·][,]_ [ ˜] _[x]_ _[∗]_ _d_ [) defined as] [ ˜] _[x]_ _[∗]_ _l_ [=] [ argmin] _m_ _l_ ( _x_ _l_ ) must be

_x_ _l_

a global optimizer of _f_ ; this follows from the fact that _ϕ_ is monotonically increasing and


_h_ ( **x** ) is additive. Given this constructive form for **x** _[∗]_, the BOMM estimator (5) targets the


estimation of such a solution via the _estimated_ marginal mean functions ˆ _m_ _l_ . When such


embedded near-additive structure is present, these marginal functions can be estimated


10


efficiently even in high dimensions [Horowitz and Mammen, 2007]; BOMM exploits this for


efficient black-box optimization with limited data.


The BOMM estimator is motivated by a related problem of parameter design optimization


for quality improvement [Wu and Hamada, 2009]. The latter targets the optimization of a


physical system, e.g., the mean yield of a plot of land, under different control inputs with


varying discrete levels. The goal is to identify a near-optimal input setting with limited


physical experiment runs. Wu et al. [1990] coined the term “pick-the-winner” as the simple


strategy that selects the best observed setting within the limited runs. Taguchi [1986]


instead advocates for an alternate “analysis of marginal means” (AM) strategy. For each


input _l_, the AM estimator selects the level that minimizes its marginal mean over such an


input. The intuition is that such marginal effects in one dimension can be estimated more


efficiently than the minimum over the full _d_ -dimensional domain. Not surprisingly, when _f_


is near-additive (i.e., it has few interactions), AM is markedly more efficient than PW for


system optimization with limited runs [Mak and Wu, 2019]. Our BOMM estimator extends


this for _continuous_ black-box optimization, coupled with a flexible _generalized_ additive


modeling framework (3) that relaxes the near-additivity requirement on _f_ .


It is also useful to contrast our approach with the earlier surrogate-based one-shot


approaches, which directly optimize a standard surrogate model trained on data **f** _n_ . As


noted earlier, such approaches may yield mediocre performance given small sample sizes in


high dimensions, when the surrogate fits poorly over the full space _X_ . Instead of relying on


the full fitted surrogate, BOMM instead leverages the estimated marginal mean functions,


which can be more easily inferred in high dimensions with limited data. As we see later,


this can improve theoretical and empirical optimization performance by tempering the


curse-of-dimensionality. A key reason lies in (i) the reduced function space for GAMs


(and its generalization in (3) ; see Theorem 4) compared to (ii) the highly nonparametric


function spaces typically considered for surrogate modeling. Functions in the reduced space


(i) permit efficient inference on marginal mean functions and its use for effective black-box


11


optimization, whereas functions in (ii) do not permit the exploitation of such structure.


Given the modeling flexibility of GAMs [Hastie et al., 2009; Lin and Joseph, 2020], this


reduced space does not appear to be overly restrictive in our target problems and enables


improved black-box optimization with limited data, as we see in later numerical experiments.

#### **3.2 Optimization consistency and rate**


We first investigate the convergence properties of BOMM. We will show that its optimality


ˆ
gap _|f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)] _[|]_ [ converges at a rate of] _[ O]_ _[P]_ [(] _[n]_ _[−][k/]_ [(4] _[k]_ [+2)] [) when] _[ f]_ [ follows a GAM. Here,] _[ k]_


is the degree of differentiability on the link function _ϕ_ and the additive functions _h_ 1 _, · · ·, h_ _d_ .


This considerably reduces the impact of dimensionality compared to the earlier _O_ ( _n_ _[−][ν/d]_ )


rate for surrogate-based approaches that use a Mat´ern- _ν_ GP, thus facilitating effective


optimization in high dimensions. As before, suppose the domain is _X_ = [�] _[d]_ _l_ =1 [[] _[L]_ _[l]_ _[, U]_ _[l]_ [].]


We make the following set of assumptions for theoretical analysis:


**Assumption 1.** _The objective_ _f_ _is in the form of a GAM (i.e., model_ (3) _with_ _ζ_ ( **x** ) = 0 _),_


_with its link function_ _ϕ_ _and additive functions_ _h_ 1 _, · · ·, h_ _d_ _k_ _-times continuously differentiable_


_with k ≥_ 2 _. Further assume:_


[ _ϕ_ [(] _[k]_ [)] ( _z_ )] [2] _dz < ∞,_ [ _h_ [(] _l_ _[k]_ [)] [(] _[x]_ _[l]_ [)]] [2] _[ dx]_ _[l]_ _[ <][ ∞][,]_ _for l_ = 1 _, · · ·, d,_ (6)
� �


_where ϕ_ [(] _[k]_ [)] _is the k-th derivative of ϕ, and the same for h_ [(] _l_ _[k]_ [)] _[.]_


**Assumption 2.** _The link function ϕ is strictly monotone increasing._


**Assumption 3.** _Design points {_ **x** 1 _, · · ·,_ **x** _n_ _} are sampled i.i.d. from_ Uniform( _X_ ) _._


Assumption 1 provides necessary smoothness conditions on _ϕ_ and _h_ 1 _, · · ·, h_ _d_, following


Horowitz and Mammen [2007]. Assumption 2 follows from the discussion in Section 3.1.


Assumption 3 is a typical design assumption for theoretical analysis.


12


In the following analysis, we adopt the inference approach in Horowitz and Mammen


[2007] for estimating _ϕ_ and _h_ 1 _, · · ·, h_ _d_ in a GAM (i.e., model (3) with _ζ_ ( **x** ) = 0). There,


these functions are jointly estimated via the constrained regularized least squares problem:



_n_
� _{f_ ( **x** _i_ ) _−_ _ϕ_ [ _h_ 1 ( _x_ _i,_ 1 ) + _· · ·_ + _h_ _d_ ( _x_ _i,d_ )] _}_ [2]


_i_ =1



� � �
_ϕ,_ _h_ 1 _, · · ·,_ _h_ _d_ = arg min
� � _ϕ,h_ 1 _,···,h_ _d_


under the constraints:



1

_n_



(7)



+ _λ_ [2] _n_



_ν_ 1 _/_ 2 _ν_ 2 _/_ 2 [�]

[ _ϕ_ [(] _[k]_ [)] ( _z_ )] [2] _dz_ + [ _ϕ_ _[′]_ ( _z_ )] [2] _dz_
� �� �
���



_,_



_d_
�


_l_ =1




[ _h_ [(] _l_ _[k]_ [)] [(] _[x]_ _[l]_ [)]] [2] _[dx]_ _[l]_ [ +] [ _h_ _[′]_ _l_ [(] _[x]_ _[l]_ [)]] [2] _[dx]_ _[l]_
�� �



= 1 _,_ _ϕ_ _[′]_ ( _z_ ) _>_ 0 _,_ (8)
�



where _ν_ 1 _>_ 0 and _ν_ 2 _>_ 0 are fixed constants with _ν_ 2 _≥_ _ν_ 1 . Here, the second term in (7)


provides regularization on the smoothness of _ϕ_ with penalty _λ_ _n_, and the first constraint in


(8) provides similar regularity on the additive functions _h_ 1 _, · · ·, h_ _d_ . The second constraint


in (8) ensures the estimated _ϕ_ is strictly monotone increasing. Following Horowitz and


Mammen [2007], we adopt the following assumption on the penalty _λ_ _n_ :


**Assumption 4.** _λ_ _n_ = _O_ _P_ � _n_ _[−][k/]_ [(2] _[k]_ [+1)] [�] _and λ_ _[−]_ _n_ [1] [=] _[ O]_ _[P]_ � _n_ _[k/]_ [(2] _[k]_ [+1)] [�] _._


With this, we now investigate the optimization performance of the BOMM estimator


ˆ
**x** _[∗]_ _n_ [= (] _[x]_ [ˆ] _[∗]_ _n,_ 1 _[,][ · · ·][,]_ [ ˆ] _[x]_ _[∗]_ _n,d_ [) in] [ (5)] [, where] [ ˆ] _[m]_ _[l]_ [ follows from] [ (4)] [ with] _[ ϕ]_ [ and] _[ f]_ [ set as] [ ˆ] _[ϕ]_ [ and] [ ˆ] _[f]_ [(] **[x]** [) =]


ˆ ˆ
_ϕ{h_ 1 ( _x_ 1 ) + _· · ·_ + ˆ _h_ _d_ ( _x_ _d_ ) _}_, respectively. As _f_ is presumed to be a GAM, this reduces to


ˆ

ˆ
_x_ _[∗]_ _n,l_ [= argmin] _h_ _l_ ( _x_ _l_ ). The following theorem establishes its optimization rate:
_x_ _l_


**Theorem 1.** _Under Assumptions 1 – 4 above, the BOMM estimator_ ˆ **x** _[∗]_ _n_ _[in]_ [ (5)] _[ using the]_


_inference approach in_ (7) _and_ (8) _yields the following optimization rate:_


_|f_ (ˆ **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)] _[|]_ [ =] _[ O]_ _[P]_ � _n_ _[−][k/]_ [(4] _[k]_ [+2)] [�] _,_ (9)


13


_where constants in O_ _P_ _may depend on f and dimension d._


The proof of this theorem is provided in Appendix A.


Several useful insights can be gleaned from this theorem. First, as sample size _n →∞_,


the optimality gap between the BOMM estimator ˆ **x** _[∗]_ _n_ [and a global minimum] **[ x]** _[∗]_ [approaches]


zero, which proves the consistency of BOMM for global optimization. Second, as the


degree of smoothness _k_ increases for the link and additive functions, the optimization rate


in (9) also improves, which is not surprising. Finally and most importantly, the term in


this rate relating to sample size _n_, namely _n_ _[−][k/]_ [(4] _[k]_ [+2)], does not depend on dimension _d_ .


This is in contrast to the _O_ ( _n_ _[−][ν/d]_ ) optimization rate (discussed earlier in Section 2.2) for


surrogate-based approaches using the Mat´ern- _ν_ GP, which deteriorates considerably as


dimension _d_ increases. In this sense, BOMM can temper such a curse-of-dimensionality for


existing black-box optimization methods. We show later that this translates to improved


practical optimization performance over existing methods, for our target setting with limited


runs in moderate-to-high dimensions.

### **4 Practical Implementation**


With this theoretical foundation, we now present a practical framework for robust


implementation of BOMM. We first leverage the transformed approximate additive GP in


Lin and Joseph [2020] for probabilistic inference on the desired marginal mean functions


to perform BOMM. We then propose a modification of BOMM, called BOMM+, for the


setting where _h_ ( **x** ) may deviate from additivity. Finally, we provide convergence analysis


for this GP-based implementation of BOMM and BOMM+.

#### **4.1 GP-based BOMM**


In what follows, we employ a (i) GP-based framework for inferring the model components


in (3) . There are three reasons why this may be preferable to the (ii) optimization-based


14


           approach in (7) (8) . First, (ii) is largely used for theoretical analysis, and can be tricky


to implement well as many hyperparameters need to be tuned. Second, (i) permits the


_probabilistic_ inference of marginal mean functions, which we will leverage for a robust


implementation of BOMM. Finally, the required smoothness conditions in (7) - (8) can be


imposed within (i) via a careful selection of GP kernels, as discussed next. We thus expect


(i) to have a comparable optimization rate as shown for (ii) in Theorem 1, although we


prove just its consistency in Section 4.3 for reasons discussed later.


To infer the model components in (3), we adopt the transformed approximate additive


GP (TAAG) in Lin and Joseph [2020], which models _f_ as:


_f_ ( **x** ) = _ϕ_ _λ_ _{A_ ( **x** ) + _Z_ ( **x** ) _},_ _A_ ( **x** ) _∼_ GP _{µ, k_ _A_ ( _·, ·_ ) _},_ _Z_ ( **x** ) _∼_ GP _{_ 0 _, k_ _Z_ ( _·, ·_ ) _},_



_d_


_w_ _l_ = 1 _,_

�


_l_ =1



_k_ _A_ ( **x** _,_ **y** ) = _σ_ [2] (1 _−_ _η_ ) _r_ _A_ ( **x** _−_ **y** ) _,_ _r_ _A_ ( _**ω**_ ) =


_k_ _Z_ ( **x** _,_ **y** ) = _σ_ [2] _ηr_ _Z_ ( **x** _−_ **y** ) _,_



_d_
� _w_ _l_ _r_ _A,l_ ( _ω_ _l_ ) _,_


_l_ =1



(10)



where _ϕ_ _λ_ is a link function parametrized by _λ_, and _A_ ( **x** ) and _Z_ ( **x** ) are independent GPs.


Here, _A_ ( **x** ) models the additive part of _h_ ( **x** ) in (3) via the additive kernel _r_ _A_ in (10),


where each additive term _r_ _A,l_ can be specified as a squared-exponential kernel or a Mat´ern


kernel that controls smoothness of the additive function _h_ _l_ in (3) . Next, _Z_ ( **x** ) models the


residual non-additive part of _h_ ( **x** ) in (3), namely _ζ_ ( **x** ), via a zero-mean GP, where _r_ _Z_ is a


non-additive kernel of choice. The parameter _η ∈_ [0 _,_ 1] controls the degree of non-additivity


in _h_ ( **x** ): a near-zero value suggests that this function is near-additive, whereas a large value


indicates considerable non-additivity. Finally, the parameter _σ_ [2] _>_ 0 serves as a global


variance parameter on both _A_ ( **x** ) and _Z_ ( **x** ).


For the link function _ϕ_ _λ_, one choice (as adopted in Lin and Joseph, 2020) is the well

known one-parameter Box-Cox transformation [Box and Cox, 1964]. This can be defined as


_ϕ_ _[−]_ _λ_ [1] [(] _[z]_ [) = (1] _[ −]_ _[z]_ _[λ]_ [)] _[/λ]_ [ for] _[ λ <]_ [ 0,] _[ ϕ]_ _[−]_ _λ_ [1] [(] _[z]_ [) =] [ log] _[ z]_ [ for] _[ λ]_ [ = 0, and (] _[z]_ _[λ]_ _[ −]_ [1)] _[/λ]_ [ for] _[ λ >]_ [ 0, where]


15


the parameter _λ ∈_ R is fit from data. Compared to its standard definition, the sign is


flipped for the case of _λ <_ 0 to ensure _ϕ_ _λ_ is monotonically increasing; this does not affect its


modeling capabilities. To use this transform, the black-box function _f_ needs to be strictly


positive. This can be achieved in practice by adding an appropriately large constant on _f_,


which does not affect its optimization. While one can employ a more flexible transformation


choice (e.g., the two-parameter transform in Yeo and Johnson, 2000), we find that the above


Box-Cox transformation works quite well in later experiments.


With this, the marginal mean functions _{m_ _l_ ( _x_ _l_ ) _}_ _[d]_ _l_ =1 [can then be inferred as follows.]


Suppose we know the model parameters _λ_, _µ_, _σ_ [2], _η_ and **w** = ( _w_ 1 _, · · ·, w_ _d_ ), along with the


kernel length-scale parameters for _r_ _A_ and _r_ _Z_ (denoted as _**θ**_ _A_ and _**θ**_ _Z_, respectively); these will


be estimated from data later. Denote the above parameter set by **Θ** . Recall from (4) that



_m_ _l_ ( _x_ _l_ ) = �



_X_ _−l_ _[ϕ]_ _λ_ _[−]_ [1] _[◦]_ _[f]_ [(] **[x]** [)] _[ d]_ **[x]** _[−][l]_ [. Conditional on observed data] **[ f]** _[n]_ [, the following proposition]



shows that the posterior distribution of _m_ _l_ ( _·_ ) follows a Gaussian process:


**Proposition 2.** _Adopt the modeling framework in_ (10) _, and suppose model parameters_ **Θ**


_are known. Conditional on data_ **f** _n_ = [ _f_ ( **x** 1 ) _, · · ·, f_ ( **x** _n_ )] _, the marginal mean function_ _m_ _l_ ( _·_ )


_has the posterior distribution m_ _l_ ( _·_ ) _|_ **f** _n_ _∼_ GP _{µ_ _n,l_ ( _·_ ) _, k_ _n,l_ ( _·, ·_ ) _}, where:_



_µ_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] **[x]** [)] _[d]_ **[x]** _[−][l]_ _[,]_ _k_ _n,l_ ( _x_ _l_ _, x_ _[′]_ _l_ [) =]
_X_ _−l_ �



_µ_ _n,l_ ( _x_ _l_ ) =
�



_X_ _−l_



_k_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] **[x]** _[,]_ **[ x]** _[′]_ [)] _[d]_ **[x]** _[−][l]_ _[d]_ **[x]** _−_ _[′]_ _l_ _[.]_ (11)

� _X_ _−l_



_Here,_ _µ_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] _[·]_ [)] _[ and]_ _[ k]_ _[n,ϕ]_ _[−]_ _λ_ [1] _[◦][f]_ [(] _[·][,][ ·]_ [)] _[ are the posterior mean and covariance functions of]_ _[ ϕ]_ _λ_ _[−]_ [1] _[◦]_ _[f]_


_conditional on_ **f** _n_ _, given by:_


_µ_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] **[x]** [) =] _[ µ]_ [ + ((1] _[ −]_ _[η]_ [)] **[r]** _[n,A]_ [(] **[x]** [) +] _[ η]_ **[r]** _[n,Z]_ [(] **[x]** [))] _[⊤]_ [((1] _[ −]_ _[η]_ [)] **[R]** _[n,A]_ [ +] _[ η]_ **[R]** _[n,Z]_ [)] _[−]_ [1] [ �] _ϕ_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** � _,_


˜ ˜
_k_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] **[x]** _[,]_ **[ x]** _[′]_ [) =] _[ σ]_ [2] [ �] 1 _−_ **r** _n_ ( **x** ) _[⊤]_ ((1 _−_ _η_ ) **R** _n,A_ + _η_ **R** _n,Z_ ) _[−]_ [1] **r** _n_ ( **x** _[′]_ )� _,_


(12)


_where_ **r** _n,A_ ( **x** ) = [ _r_ _A_ ( **x** _i_ _−_ **x** )] _[n]_ _i_ =1 _[,]_ **[ r]** _[n,Z]_ [(] **[x]** [) = [] _[r]_ _[Z]_ [(] **[x]** _[i]_ _[−]_ **[x]** [)]] _[n]_ _i_ =1 _[,]_ [ ˜] **[r]** _[n]_ [(] **[x]** [) = (1] _[ −]_ _[η]_ [)] **[r]** _[n,A]_ [(] **[x]** [) +] _[ η]_ **[r]** _[n,Z]_ [(] **[x]** [)] _[,]_


**R** _n,A_ = [ _r_ _A_ ( **x** _i_ _−_ **x** _j_ )] _[n]_ _i,j_ =1 _[and]_ **[ R]** _[n,Z]_ [= [] _[r]_ _[Z]_ [(] **[x]** _[i]_ _[−]_ **[x]** _[j]_ [)]] _[n]_ _i,j_ =1 _[.]_


16


The proof of this proposition is provided in Appendix C.


With this, the GP-based BOMM estimator then takes the form:


**x** ˆ _[∗]_ _n_ [:= (ˆ] _[x]_ _[∗]_ _n,_ 1 _[,][ · · ·][,]_ [ ˆ] _[x]_ _[∗]_ _n,d_ [)] _[,]_ _x_ ˆ _[∗]_ _n,l_ [= arg min] _µ_ _n,l_ ( _x_ _l_ ) _,_ _l_ = 1 _, · · ·, d._ (13)
_x_ _l_


This can be further simplified when _{r_ _A,l_ _}_ _[d]_ _l_ =1 [and] _[ r]_ _[Z]_ [ follow the squared-exponential form:]


_̸_


_̸_



2 [�]
�


_̸_


_̸_



2 [�]
�


_̸_


_̸_



_r_ _A,l_ ( _x_ _l_ _, x_ _[′]_ _l_ [) = exp]


_̸_


_̸_



_′_
_−_ _x_ _l_ _−_ _x_ _l_
_θ_
� _A,l_

�


_̸_


_̸_



_,_ _r_ _Z_ ( **x** _,_ **x** _[′]_ ) = exp


_̸_


_̸_



_−_

�


_̸_


_̸_



_d_
�


_l_ =1


_̸_


_̸_



_x_ _l_ _−_ _x_ _l_


_θ_

� _Z,l_


_̸_


_̸_



_x_ _l_ _−_ _x_ _l_


_θ_

� _Z,l_


_̸_


_̸_



(14)


_̸_


_̸_



where _**θ**_ _A_ = ( _θ_ _A,_ 1 _, · · ·, θ_ _A,d_ ) and _**θ**_ _Z_ = ( _θ_ _Z,_ 1 _, · · ·, θ_ _Z,d_ ) are their length-scale parameters.


With such kernels, the following proposition gives a closed-form objective for (13):


**Proposition 3.** _Adopt the same conditions as Proposition 2. Under the squared-exponential_


_kernels in_ (14) _, the BOMM estimator in_ (13) _reduces to:_


_̸_


_̸_



�


_̸_


_̸_



�


_̸_


_̸_



2 [�]
�


_̸_


_̸_



2 [��]
�


_̸_


_̸_



_n_
� _p_ _i,l_ _q_ _i_


_i_ =1


_̸_


_̸_



_−_ � _x_ _l_ _−θ_ _Z,l_ _x_ _i,l_


_̸_


_̸_



_θ_
_Z,l_


_̸_


_̸_



_x_ ˆ _[∗]_ _n,l_ [= arg min]
_x_ _l_


_̸_


_̸_



�


_̸_


_̸_



(1 _−_ _η_ ) _w_ _l_ Vol( _X_ _−l_ )


_̸_


_̸_



_n_
� _q_ _i_ exp


_i_ =1


_̸_


_̸_



_−_ � _x_ _l_ _−θ_ _A,l_ _x_ _i,l_


_̸_


_̸_



_θ_
_A,l_


_̸_


_̸_



_d−_ 1
+ _π_ 2 _η_


_̸_


_̸_



_,_


_̸_


_̸_



(15)


_where_ **x** _i_ = ( _x_ _i,_ 1 _, · · ·, x_ _i,d_ ) _is the i-th design point. Here, p_ _i,l_ _and_ **q** = [ _q_ 1 _, · · ·, q_ _n_ ] _follow:_


_p_ _i,l_ = � _θ_ _Z,j_ �˜Φ _i,j_ ( _U_ _j_ ) _−_ ˜Φ _i,j_ ( _L_ _j_ )� _,_ **q** = ((1 _−_ _η_ ) **R** _n,A_ + _η_ **R** _n,Z_ ) _[−]_ [1] [ �] _ϕ_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** � _,_

_j_ = _̸_ _l_


_̸_



_̸_


_where_ Φ [˜] _i,j_ _is the c.d.f. of N_ ( _x_ _i,j_ _, θ_ _Z,j_ [2] _[/]_ [2)] _[ and]_ [ Vol(] _[X]_ _[−][l]_ [) =][ �] _̸_



_̸_


_j_ = _̸_ _l_ [(] _[U]_ _[j]_ _[ −]_ _[L]_ _[j]_ [)] _[.]_



_̸_


_̸_


Similar expressions can be derived for other kernel choices, e.g., the Mat´ern kernel, but


may be more involved. With this closed-form objective, one can easily optimize the


one-dimensional problem in (15) (e.g., via grid search) to obtain the BOMM estimator ˆ _x_ _[∗]_ _n,l_ [.]


The above procedure, however, requires the estimation of parameters **Θ** from data.


To do this, we employ the same empirical Bayes approach as Lin and Joseph [2020].


This approach first assigns the following non-informative priors on model parameters


17


**Algorithm 1** GP-based BOMM+

**Input** : Sample size _n_ (from run budget), threshold _T_, significance level _ρ_

1: Construct a maximin Latin hypercube design _{_ **x** _i_ _}_ _[n]_ _i_ =1 [, and evaluate] _[ f]_ [ on such points.]
2: Fit the transformed approximate additive GP in Lin and Joseph [2020] and obtain
parameter estimates **Θ** [ˆ] .
3: Using **Θ** [ˆ] _−η_, compute the plug-in estimate of the posterior probability _ξ_ = P ( _η >_
_T_ _|_ **Θ** [ˆ] _−η_ _,_ data) via (16).
4: **if** _ξ ≤_ 1 _−_ _ρ_ **then**
5: **for** _l_ = 1 _, · · ·, d_ **do**
6: _•_ Optimize the BOMM estimator ˆ _x_ _[∗]_ _n,l_ [via (13).]


7: **else**

8: **for** _l_ = 1 _, · · ·, d_ **do**
9: _•_ Specify the tail probability _α_ _[∗]_ following Appendix F.
10: _•_ Optimize the tail BOMM estimator ˆ _x_ _[∗]_ _n,l_ [= ˆ] _[x]_ _[∗]_ _n,α_ _[∗]_ _,l_ [via (17).]

**Output** : ˆ **x** _[∗]_ _n_ [= (ˆ] _[x]_ _[∗]_ _n,_ 1 _[,][ · · ·][,]_ [ ˆ] _[x]_ _[∗]_ _n,d_ [)]


[ _λ, µ, τ_ [2] _, δ,_ **w** _,_ _**θ**_ _A_ _,_ _**θ**_ _Z_ ] _∝_ 1, where _τ_ [2] = _σ_ [2] (1 _−_ _η_ ) and _δ_ = _η/_ (1 _−_ _η_ ) reparametrize ( _σ, η_ ). It


then finds the fitted parameters **Θ** [ˆ] that maximize the corresponding marginal likelihood


given observed data **f** _n_ . Details on this procedure can be found in Section 3 of Lin and Joseph


[2020]. With this in hand, the GP-based BOMM estimator (13) can then be computed


using the plug-in estimate **Θ** = **Θ** [ˆ] .


Algorithm 1 summarizes each step of the GP-based BOMM optimization procedure,


with a diagnostic procedure described later. First, the black-box simulator _f_ is evaluated


at designed input points **x** 1 _, · · ·,_ **x** _n_ . In later experiments, we find that maximin Latin


hypercube designs [Morris and Mitchell, 1995] work quite well: they not only provide


desirable space-filling performance, but also offer good projective properties onto each input,


which is important for accurate estimation of the additive structure in (3) . Next, one fits


the transformed approximate additive GP in Lin and Joseph [2020]; our implementation


makes use of the authors’ R package `TAG` [Lin and Joseph, 2021]. With the fitted model,


one then constructs the BOMM estimator via the optimization formulation (13) . In our


implementation, this optimization is performed via one-dimensional grid searches.


18


#### **4.2 GP-based BOMM+**

Recall that a key motivation for BOMM is its use of marginal mean functions that can


be estimated efficiently when _h_ = _ϕ_ _[−]_ [1] _◦_ _f_ is near-additive, i.e., it has few interaction effects.


When considerable interactions are present, a modification of BOMM using marginal _tail_


means can be used for robust performance. We present next a diagnostic approach for


detecting such non-additivity, followed by a marginal tail means modification for estimating


**x** _[∗]_ . We refer to this approach with diagnostic modification as BOMM+ hereafter.


Recall that the parameter _η_ dictates the level of non-additivity in the model (10) : the


larger _η_ is, the greater its non-additivity. Thus, a reasonable diagnostic for non-additivity


might be the posterior probability that _η_ is large, i.e., P ( _η > T_ _|_ data ) for a desired threshold


_T >_ 0. Suppose for now that all model parameters in **Θ** except _η_ (denoted **Θ** _−η_ ) are known.


Then the posterior distribution of _η_ takes the form:


[ _η|_ **Θ** _−η_ _,_ data] _∝_ _s_ _[−][n]_ _η_ _[δ]_ (1 _−_ _η_ )det _{_ (1 _−_ _η_ ) **R** _n,A_ + _η_ **R** _n,Z_ _}_ _[−]_ 2 [1] _,_ (16)


where _s_ [2] = _n_ _[−]_ [1] ( _ϕ_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** [)] _[⊤]_ _[{]_ [(1] _[ −]_ _[η]_ [)] **[R]** _[n,A]_ [ +] _[ η]_ **[R]** _[n,Z]_ _[}]_ _[−]_ [1] [ (] _[ϕ]_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** [). With this, the]


diagnostic probability P ( _η > T_ _|_ **Θ** _−η_ _,_ data ) can be easily computed via Monte Carlo methods.


In our later implementation, this is computed via the self-normalized importance sampling


approach in Chapter 9 of Owen [2016]. One can then infer whether considerable non

additivity is present by seeing whether this probability is above a certain cut-off 1 _−_ _ρ_ ; in


later experiments, we used a threshold of _T_ = 0 _._ 4 and a significance level of _ρ_ = 0 _._ 3, which


seemed to work well.


Of course, in practice the parameters **Θ** _−η_ are unknown. From a Bayesian perspective,


one would ideally sample from the full posterior distribution [ **Θ** _|_ data ], then marginalize over


**Θ** _−η_ to compute the diagnostic probability P ( _η > T_ _|_ data ). Such a fully Bayesian approach,


however, may be expensive given the many parameters in **Θ** . We adopt an alternate


strategy using the empirical Bayes estimates **Θ** [ˆ] _−η_ from the previous subsection, which can


19


be efficiently optimized via the R package `TAG` [Lin and Joseph, 2021]. In particular, we


employ the plug-in estimator P ( _η > T_ _|_ **Θ** [ˆ] _−η_ _,_ data ), where given **Θ** [ˆ] _−η_, one can compute this


probability from (16) via, e.g., self-normalized importance sampling [Owen, 2016].


With this non-additivity diagnostic in hand, the BOMM estimator from the previous


subsection should be used when P ( _η > T_ _|_ **Θ** [ˆ] _−η_ _,_ data ) _<_ 1 _−_ _ρ_, as this suggests there is


near-additivity in _h_ that can be exploited. When this is not the case, there is some evidence


for considerable non-additivity in _h_, in which case we adopt the following _tail_ marginal mean


estimator. The intuition is as follows. Even when _h_ is not globally near-additive, its degree


of additivity should increase as one hones in locally around its minimizer **x** _[∗]_ . Following Mak


and Wu [2019], we employ a marginal tail means approach to exploit such _local_ additivity for


optimization. In place of the posterior marginal mean _µ_ _n,l_ ( _x_ ) = E [ _m_ _l_ ( _x_ ) _|_ data ], we instead

employ the posterior marginal tail mean _µ_ [[] _n,l_ _[α]_ []] [(] _[x]_ [) =] [ E] [[] _[m]_ _[l]_ [(] _[x]_ [)] _[|]_ [data] _[, m]_ _[l]_ [(] _[x]_ [)] _[ ≤]_ _[Q]_ _l_ [[] _[α]_ []] [(] _[x]_ [)], where]

_Q_ [[] _l_ _[α]_ []] [(] _[x]_ [) is the 100] _[α]_ [%-percentile of the posterior distribution [] _[m]_ _[l]_ [(] _[x]_ [)] _[|]_ [data] []. Such a tail mean]


discards the top 100(1 _−_ _α_ )% of this posterior distribution before evaluating its expectation;


this removes the part of the posterior that is more sensitive to large objective values in the


data **f** _n_, allowing it to better exploit local additivity of _h_ near **x** _[∗]_ . Note that, with _α_ = 1,


this reduces to the original posterior marginal mean _µ_ _n,l_ ( _x_ ). A similar tail means approach


was employed in Mak and Wu [2019] for discrete black-box optimization.


Since the posterior distribution of _m_ _l_ ( _x_ ) is Gaussian (Proposition 2), its posterior tail

mean function further admits the closed form _µ_ [[] _n,l_ _[α]_ []] [(] _[x]_ _[l]_ [) =] _[ µ]_ _[n,l]_ [(] _[x]_ _[l]_ [)] _[ −]_ ~~�~~ _k_ _n,l_ ( _x_ _l_ _, x_ _l_ ) _φ_ ( _z_ _α_ ) _/α_,


where _z_ _α_ is the 100 _α_ %-percentile of the standard Gaussian and _φ_ is the standard Gaussian


density. The resulting tail BOMM estimator is then given as:


**x** ˆ _[∗]_ _n,α_ [:= (ˆ] _[x]_ _[∗]_ _n,α,_ 1 _[,][ · · ·][,]_ [ ˆ] _[x]_ _[∗]_ _n,α,d_ [)] _[,]_ _x_ ˆ _[∗]_ _n,α,l_ [= arg min] _µ_ [[] _n,l_ _[α]_ []] [(] _[x]_ _[l]_ [)] _[,]_ _l_ = 1 _, · · ·, d._ (17)
_x_ _l_


As before, one can employ the plug-in estimate **Θ** = **Θ** [ˆ] for evaluating (17) . We show


later in experiments that such a tail estimator can provide robust optimization under non

20


additivity in _h_ . Algorithm 1 summarizes the full BOMM+ procedure with this non-additivity


diagnostic. Appendix F provides guidance on how _α_ can be selected in implementation.

#### **4.3 Optimization consistency**


We now establish the optimization consistency of the GP-based BOMM and BOMM+.


The key difference between this analysis and that in Section 3.2 lies in the considered


function space for _f_ . The earlier analysis from Section 3.2 establishes an optimization


rate for BOMM when _f_ follows a GAM, i.e., the model (3) with _ζ_ ( **x** ) = 0. The following


analysis shows that the GP-based BOMM and BOMM+ are consistent, i.e., its optimality


ˆ
gap _f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [) goes to zero, under mild deviations of] _[ ζ]_ [(] **[x]** [) from zero, i.e., under mild]


deviations from additivity for _h_ in (3) . Optimization rates for the GP-based BOMM and


BOMM+ are more difficult to establish since there is little work on their corresponding


function space; we will explore this as future work.


As before, suppose _X_ = [�] _[d]_ _l_ =1 [[] _[L]_ _[l]_ _[, U]_ _[l]_ []. We presume the following form for] _[ f]_ [:]


**Assumption 5.** _The objective f lies on the function space F_ _λ_ _, defined as:_


_F_ _λ_ = _{f_ : _f_ = _ϕ_ _λ_ _◦_ _h, h ∈H_ TAAG _},_ _λ ∈_ R _._ (18)


Here, _H_ TAAG is the reproducing kernel Hilbert space (RKHS; Aronszajn, 1950) of the kernel


_k_ _A_ + _k_ _Z_ on domain _X_, corresponding to the GP for _A_ ( **x** ) + _Z_ ( **x** ) in (10) . This RKHS


takes the form _H_ TAAG = _{h_ : _h_ = _h_ _A_ + _h_ _Z_ _, h_ _A_ _∈H_ _k_ _A_ _, h_ _Z_ _∈H_ _k_ _Z_ _}_ equipped with the norm



, where _H_ _k_ _A_ and _H_ _k_ _Z_ correspond
�



_∥h∥_ _H_ TAAG = min
_h_ = _h_ _A_ + _h_ _Z_ _,h_ _A_ _∈H_ _kA_ _,h_ _Z_ _∈H_ _kZ_



� _∥h_ _A_ _∥_ _H_ _kA_ + _∥h_ _Z_ _∥_ _H_ _kZ_



to the RKHS for kernels _k_ _A_ and _k_ _Z_, respectively. Note that _H_ TAAG consists of functions


that are _non-additive_ for _h_ due to the presence of _h_ _Z_ .


We further make the following set of assumptions for theoretical analysis:


**Assumption 6.** _The kernels_ _k_ _A_ _and_ _k_ _Z_ _in the RKHS_ _H_ TAAG _take the squared-exponential_


_form_ (10) _and_ (14) _. The GP modeling framework_ (10) _for BOMM adopts the same kernels,_


_with no misspecification of kernel hyperparameters or λ._


21


**Assumption 7.** _The objective_ _f_ _satisfies_ _f_ ( **x** ) _∈_ [ _f_ _[∗]_ _, f_ [+] ] _for_ **x** _∈X_ _, where_ _f_ _[∗]_ = _f_ ( **x** _[∗]_ ) _>_ 0


_is the global minimum and f_ [+] _< ∞_ _is an upper bound._


**Assumption 8.** _The objective f admits a unique minimizer_ **x** _[∗]_ _∈X_ _._


**Assumption 9.** _The objective f satisfies the so-called “first-order dominating” condition:_



arg min
**x** _∈X_ _[h]_ [(] **[x]** [) = arg min] **x** _∈X_



_d_
�


_l_ =1



_h_ ( **x** ) _d_ **x** _−l_ _,_ _h_ = _ϕ_ _[−]_ _λ_ [1] _[◦]_ _[f.]_ (19)
�



Assumption 6 on kernel specification is typical for GP analysis (see, e.g., Ritter, 2000),


although recent work [Wang et al., 2020; Wynne et al., 2021] has explored the case of


potential kernel misspecification. The consistency results later also hold for Mat´ern kernels.


Assumption 7 is needed to ensure the Box-Cox transformation is valid; this is always possible


by adding an appropriately large constant on _f_, which does not affect its optimization.


Assumption 8 is a mild condition on the uniqueness of **x** _[∗]_ . Assumption 9 on the “first-order


dominating” condition (a term we coined) permits mild interactions in _h_ ( **x** ), as long as its


minimizer corresponds to that of its marginal mean functions; note that this holds naturally


when _h_ ( **x** ) is additive.


With this in hand, we now state the desired consistency result for the GP-based BOMM:


**Theorem 4.** _Under Assumptions 3 and 5 – 9, the BOMM estimator_ ˆ **x** _[∗]_ _n_ [(13)] _[ using the GP]_

_modeling framework_ (10) _satisfies f_ (ˆ **x** _[∗]_ _n_ [)] _→_ _P_ _f_ ( **x** _∗_ ) _._


Its proof is provided in Appendix D. This theorem shows that, even when _f_ deviates mildly


from generalized additivity (in that the first-order dominating condition (19) still holds),


the optimality gap for the GP-based BOMM converges to zero in probability as sample size


_n_ increases, as desired. Here, the function space _F_ _λ_ provides generalization on the GAM


space considered earlier in Theorem 1, which do not permit interaction effects in _h_ .


We can further prove a similar consistency result for the GP-based BOMM+:


22


**Corollary 1.** _Under Assumptions 3 and 5 – 9, the BOMM+ estimator_ ˆ **x** _[∗]_ _n_ _[from Algorithm]_

_1 satisfies f_ (ˆ **x** _[∗]_ _n_ [)] _→_ _P_ _f_ ( **x** _∗_ ) _._


Its proof is provided in Appendix E. In practice, as seen later in experiments, BOMM+


can have considerable improvements over existing methods even when _h_ ( **x** ) has moderate


interactions. However, showing this via an optimization rate (as in Theorem 1) is difficult


for the broader function space in Theorem 4, as we have found little work on such a space.

### **5 Numerical Experiments**


We now inspect the performance of the proposed BOMM+ approach compared to


existing one-shot black-box optimization methods. We first outline the experiment set-up,


then investigate the compared methods for a suite of test functions and a custom function


where the degree of interactions can be controlled. Finally, we investigate a batch-sequential


implementation of BOMM+ and compare it with an existing batch-sequential black-box


approach. Such a batch-sequential setting is not the primary focus of this work, but we


include this to demonstrate the potential of BOMM+ in broader settings.

#### **5.1 Experiment set-up**


We first give an overview of the compared methods in the following experiments:


  - _Pick-the-Winner_ (PW): This is the simple benchmark of selecting the evaluated design


point that yields the lowest observed objective, i.e., ˆ **x** _[∗]_ _n_ [=] argmin _f_ ( **x** ).
**x** _∈{_ **x** 1 _,···,_ **x** _n_ _}_


  _Surrogate-based-optimization via the squared-exponential GP_ (SBO-SqExp): This


is a standard SBO benchmark, using a GP surrogate with an anisotropic squared

exponential kernel (commonly used in computer experiments; see Gramacy, 2020).


All model parameters are estimated via maximum likelihood using the R package


23


`DiceKriging` [Roustant et al., 2012]. Its estimator for **x** _[∗]_ takes the form ˆ **x** _[∗]_ _n_ [=]


ˆ
argmin _f_ _n_ ( **x** ), where ˆ _f_ _n_ ( _·_ ) is the posterior mean of the GP given data **f** _n_ .
**x** _∈X_


  _Surrogate-based-optimization via the deep GP_ (SBO-DGP): SBO-DGP uses the above


SBO approach, except the surrogate _f_ [ˆ] _n_ ( _·_ ) uses the deep GP from Sauer et al. [2023],


fitted with the R package `deepgp` [Booth, 2024] and its default settings.


  _Surrogate-based-optimization via TAAG_ (SBO-TAAG): SBO-TAAG uses the above


SBO approach, except the surrogate _f_ [ˆ] _n_ ( _·_ ) uses the TAAG model (10), fitted with


the R package `TAG` [Lin and Joseph, 2021] and its default settings. Another SBO


benchmark, SBO-TAG, uses the transformed additive GP surrogate (model (10) with


_η_ = 0) fitted with the same package. While these are not common benchmarks, we


include them to contrast our approach, which uses the marginal means estimator from


the TAAG surrogate, with the direct optimization of such a surrogate.


  - _BOMM+_ : This is the proposed approach in Algorithm 1 with threshold _T_ = 0 _._ 4 and


significance level _ρ_ = 0 _._ 3.


All methods use the same points, sampled from a maximin Latin hypercube design [Morris


and Mitchell, 1995] from the R package `lhs` [Carnell, 2024]. The sample size _n_ is set as


10 _d_, following Loeppky et al. [2009]. To quantify simulation variability, each experiment


is replicated 20 times. The considered methods are compared on their optimality gap


ˆ
_f_ ( **x** _[∗]_ _n_ [)] _[−][f]_ [(] **[x]** _[∗]_ [), i.e., the objective gap between its predicted minimizer and the true minimizer.]

#### **5.2 A simulation bake-off**


With this set-up, we investigate a simulation “bake-off” of the compared methods in a


suite of test functions in the computer experiments literature. In addition to the six-hump


and wing weight functions from Section 2.3, we consider two more test functions from


24


**Figure 2:** _Log-optimality gaps of the compared methods for the six-hump camel, wing weight,_
_OTL circuit and piston functions. Boxplots show experiment variability over 20 replications._


Surjanovic and Bingham [2013]: the OTL circuit function in _d_ = 6 dimensions, and the


piston function in _d_ = 7 dimensions; their specific forms are provided in Appendix G.


Figure 2 shows the boxplots of the log-optimality gaps


for each of the four test functions. There are several useful


observations. First, the same limitations of existing meth

ods noted in Section 2.3 arise here. In selecting ˆ **x** _[∗]_ _n_ [amongst]


evaluated points, PW yields mediocre performance par

ticularly as dimension _d_ increases. SBO approaches with



the standard squared-exponential GP (SBO-SqExp) and


deep GP (SBO-DGP) yield improvements over PW in


some cases; in other cases, they may perform considerably



**Figure 3:** _For BOMM+, the pos-_
_terior mode of_ [ _η|_ **Θ** [ˆ] _−η_ _,_ data ] _over_
_20 replications for the compared_
_functions._



worse. One reason is again its reliance on a good global surrogate fit on _X_ ; when this


is poor, such methods may perform worse than PW. Our BOMM+ approach performs


25


**Figure 4:** _Log-optimality gaps for the weak (_ _λ_ = 0 _._ 05 _), moderate (_ _λ_ = 0 _._ 3 _) and strong (_ _λ_ = 0 _._ 5 _)_
_interaction cases of the test function_ (20) _. Boxplots show experiment variability over 20 replications._


quite well; it yields considerably smaller optimality gaps compared to other methods for


all functions. Figure 3 shows boxplots of the estimated ˆ _η_ (taken as the posterior mode of


[ _η|_ **Θ** [ˆ] _−η_ _,_ data ]) for its underlying TAAG model. We see that the OTL, piston and wing weight


functions have near-zero ˆ _η_, suggesting (i) the presence of latent near-additive structure after


transformation; the six-hump camel has considerably larger ˆ _η_, suggesting (ii) the presence


of latent interaction effects in _h_ . For (i), BOMM+ employs the BOMM estimator (13)


to exploit such near-additive structure via marginal mean functions. For (ii), BOMM+


employs the tail BOMM estimator (17), which exploits local near-additivity via marginal


tail means. In doing so, BOMM+ enjoys improved optimization performance over existing


methods given limited runs in moderate-to-high dimensional domains.


The contrast between BOMM+ and the SBO approaches SBO-TAAG and SBO-TAG


deserves further discussion. The latter approaches directly optimize various forms of the


fitted TAAG model (10), whereas BOMM+ makes use of the marginal mean functions from


this fitted model. We see that, by _modeling_ for latent near-additive structure, SBO-TAAG


and SBO-TAG offer some improvements over existing benchmarks. However, by further


leveraging such latent near-additivity via a marginal means _estimator_ of **x** _[∗]_, BOMM+ can


further exploit this structure to yield considerably reduced optimization gaps. Given the


challenges of limited samples in high-dimensional domains, this highlights the importance


of fully exploiting marginal structure via BOMM+ for effective black-box optimization.


26


Next, we investigate the effectiveness of the diagnostic in Section 4.2 via the following


_d_ = 9-dimensional custom test function, which is based on the exponential test function in


Dette and Pepelyshev [2010]. For brevity, _ϵ_ and _{m_ _l_ _}_ [9] _l_ =1 [are specified in Appendix G.]



_d/_ 3
� _{_ ( _x_ 3 _l−_ 2 _−_ _m_ 3 _l−_ 2 ) _−_ ( _x_ 3 _l−_ 1 _−_ _m_ 3 _l−_ 1 ) _−_ ( _x_ 3 _l_ _−_ _m_ 3 _l_ ) _}_ [2] _,_


_l_ =1



_f_ ( **x** ) = 10



_d/_ 3 _−_ 1
�


_l_ =0



3

_e_ _[−]_ [2] _[/x]_ 3 [(] _[m]_ _l_ + [+1)] _m_ _[/]_ [2] + _ϵ_ + _λ_

�


_m_ =1



( _x_ 1 _, x_ 3 _, x_ 5 ) _∈_ [0 _,_ 5] [3] _,_ ( _x_ 2 _, x_ 8 ) _∈_ [1 _,_ 6] [2] _, x_ 4 _∈_ [1 _._ 5 _,_ 6 _._ 5] _,_ ( _x_ 6 _, x_ 7 _, x_ 9 ) _∈_ [2 _,_ 7] [3] _._ (20)


Here, the first term in (20) is additive, and its second term controls the magnitude of


interaction effects; the larger _λ >_ 0 is, the greater such interactions. We inspect three


functions with different interaction levels: _λ_ = 0 _._ 05 (weak), _λ_ = 0 _._ 3 (moderate) and _λ_ = 0 _._ 5


(strong). The same methods are compared under the same settings, with 20 replications.


Figure 4 shows the boxplots of the log-optimality

Percentage

gaps for each function, and Table 1 shows the per


centage of replications for which considerable non

additivity is detected on _h_ via the diagnostic in Sec

tion 4.2. For the weak interaction case, the diagnos

tic correctly identifies the lack of considerable non

additivity in all replications; BOMM+ then leverages


the marginal means estimator (13) to exploit such



Weak ( _λ_ = 0 _._ 05) 0%


Moderate ( _λ_ = 0 _._ 3) 95%


Strong ( _λ_ = 0 _._ 5) 100%


**Table 1:** _Percentage of replications for_
_which the BOMM+ diagnostic detects_
_considerable non-additivity on_ _h_ _for the_
_test function_ (20) _._



structure, yielding improved optimization over benchmarks. For the moderate and strong


cases, the diagnostic identifies considerable non-additivity in nearly all replications; BOMM+


then uses the marginal tail means estimator to exploit local additivity, yielding comparable


or better performance to the best benchmarks. Here, SBO-DGP also performs well in the


moderate and strong cases, with comparable optimality gaps to BOMM+. However, as


noted before, such a method may suffer from a lack of robustness: when its surrogate fits


poorly over _X_, its optimization can be worse than PW (see Figure 2).


27


#### **5.3 Batch-sequential BOMM+**

Suppose _f_ is evaluated at a set of ini

tial design points **x** 1 _, · · ·,_ **x** _n_ . We wish to


use this to adaptively select the next batch


of points **x** _n_ +1 _, · · ·,_ **x** _n_ + _b_ for minimizing _f_,


where _b >_ 1 is the batch size. Consider


the following simple approach. First, select


one of the _b_ points as the inferred solution


ˆ
**x** _[∗]_ _n_ [from BOMM+ using current evaluations]


of _f_ as data. Next, select the remaining


_b_ _−_ 1 points from a random Latin hypercube


design (LHD; McKay et al., 2000). The ob

jective _f_ is then evaluated on this batch of


design points, the TAAG model is re-fit, and



**Figure 5:** _Log-optimality gaps of the compared_
_batch-sequential methods for the piston function_
_as a function of batch iteration. Boxplots show_
_experimental variability over 20 replications._



the above batch procedure is repeated for _m ≥_ 1 iterations (or until the run budget is


exhausted). This can be intuited by the well-known exploration-exploitation trade-off


[Kearns and Singh, 2002]: the _b −_ 1 LHD points target the _exploration_ of _f_ to identify


latent near-additive structure, and the evaluation at the BOMM+ estimate ˆ **x** _[∗]_ _n_ [targets the]


_exploitation_ of this learned structure for optimization via marginal means.


As a proof-of-concept, we test this batch-sequential approach (which we call Batch

BOMM+) on the earlier _d_ = 7 piston function. Here, _n_ ini = 35 maximin LHD points are


used initially, then batches of _b_ = 5 runs are taken until a total budget of _n_ = 70 evaluations


is exhausted. We compare with two standard benchmarks. The first is a simple batch

sequential space-filling design approach using the maximum projection design in Joseph


et al. [2015], as implemented in the R package `MaxPro` [Ba and Joseph, 2018]. This can be


viewed as a “pure exploration” strategy. The second is the batch expected improvement


28


**Figure 6:** _[Left] The neutrino detector schematic for the LEGEND project, along with the consid-_
_ered neutron moderator geometry with_ _d_ = 4 _inputs. [Right] Comparison of_ [77(] _[m]_ [)] Ge _production_
_rates (smaller-the-better) for the selected moderator designs from each method. Boxplots show_
_experiment variability over 10 replications._


approach (Batch-EI; Chevalier and Ginsbourger, 2013), which is widely used for batch

sequential Bayesian optimization. Here, Batch-EI uses a GP surrogate with anisotropic


squared-exponential kernel, which is re-fit at each batch iteration. This simulation is


replicated 20 times. Figure 5 shows the log-optimality gaps for the compared methods


at each batch iteration. We see that Batch-BOMM+ yields consistent improvements


over the two benchmarks as batch iteration increases. This shows that a simple adaptive


optimization approach that leverages learned latent near-additive structure can be promising


in a batch-sequential setting; we will investigate this as future work.

### **6 Application: Neutrino detector optimization**


The search for neutrinoless double-beta decay (0 _νββ_ ) is a frontier in modern physics


[Nuclear Science Advisory Committee, 2023]; if detected, this decay could provide an expla

nation for the matter-antimatter asymmetry [Canetti et al., 2012], where there is a greater


abundance of matter over antimatter in the Universe. The LEGEND (Large Enriched


Germanium Experiment for Neutrinoless Double-Beta Decay) project [LEGEND Collabo

ration, 2021] searches for 0 _νββ_ decay in a massive liquid argon cryostat in which 1000 kg


29


of [76] Ge-enriched germanium detectors are immersed (Figure 6 left). A key experimental


challenge is to minimize the cosmogenic neutron background generated by high-energy


cosmic muons [Pandola et al., 2007]. Such muons can enter the experiment and generate


secondary neutrons, which interact with [76] Ge to produce unwanted isotopes (e.g., [77(] _[m]_ [)] Ge)


[Meierhofer, 2010]. The decays of such isotopes could mimic 0 _νββ_ events and thus obscure


the desired physics signals [Wiesinger et al., 2018].


To mitigate this background, one strategy is to employ a neutron moderator that


slows down or absorbs the undesirable neutrons before they reach the inner, sensitive


germanium detectors [Neuberger et al., 2021; Schuetz et al., 2025]. Designing an effective


moderator is challenging: it must suppress the flux of neutrons while remaining compatible


with demanding engineering and material constraints. We investigate here a turbine-like


moderator geometry (Figure 6 middle), in which eight polyethylene panels are arranged


radially around the germanium detector array to enhance the panels’ directional shielding


performance. This geometry is parametrized by _d_ = 4 inputs with corresponding ranges:


the turbine radius _x_ 1 (180-230 cm), the panel thickness _x_ 2 (10-15 cm), the panel length


_x_ 3 (100-150 cm), and the panel tilt angle _x_ 4 (0-20 degrees). The goal is to optimize the


moderator design **x** within this geometry range for effective neutron shielding by minimizing


_f_ ( **x** ), the production rate of the unwanted isotope [77(] _[m]_ [)] Ge.


A key challenge for this optimization is the simulation cost of a single moderator design.


A high-fidelity simulation of this shielding process requires modeling individual primary


muons and their interactions in the rock overburden and the shielding, which can require


hundreds of CPU hours and is thus too expensive for method comparison. Instead, as a


proof-of-concept, we use a lower-fidelity simulator [Ramachers and Morgan, 2020; Neuberger,


2023] that injects secondary neutrons directly as primaries within the liquid argon cryostat,


which focuses computational resources on the critical neutron transport within the active


detector region. Each run of this lower-fidelity simulator requires 1 CPU hour, which


facilitates method comparison. Here, the same methods as Section 5 are compared, with


30


all methods using the same _n_ = 50 design points from a maximin Latin hypercube design.


This experiment is replicated 10 times, and performance is gauged on the production rate


of [77(] _[m]_ [)] Ge (smaller-the-better) for the selected moderator designs.


Figure 6 (right) shows the boxplots of [77(] _[m]_ [)] Ge production rates for the selected moderator


designs from each method. As before, we see that PW yields mediocre performance, which is


expected since the evaluated points are likely far from optimal. The SBO benchmarks give


mixed results: some offer slightly lower [77(] _[m]_ [)] Ge rates to PW, whereas others yield slightly


higher rates. BOMM+ again improves upon existing benchmarks, which highlights the


importance of exploiting latent near-additive structure via marginal means for enhancing


black-box optimization given limited experimental runs. It should be noted that, for neutrino


detector design, optimization metrics at the upper tail percentiles are also of interest, as one


wants to ensure good shielding performance with high confidence. From Figure 6 (right),


BOMM+ and SBO-TAG provide the best performance at the 90% percentile (top whisker


of boxplot). Our approach, however, has greater potential for identifying detector designs


with improved shielding over SBO-TAG, as indicated by other percentiles in the boxplots.

### **7 Conclusion**


This paper introduces a new Black-box Optimization via Marginal Means (BOMM)


method for effective one-shot optimization of an expensive black-box function _f_ . Existing


methods, e.g., pick-the-winner and surrogate-based optimization approaches, may yield


mediocre performance with poor robustness, particularly as input dimensionality increases.


To address this, BOMM leverages a new estimator of a global optimizer using marginal mean


functions, which can be effectively estimated in high dimensions with limited runs. We prove


that, when _f_ follows a generalized additive model and under mild conditions, the optimality


gap from BOMM converges to zero and at a rate with considerably less dependence on


dimensionality than existing methods. We then present a practical framework for implement

31


ing BOMM using the transformed approximate additive GP in Lin and Joseph [2020], and


prove its consistency for black-box optimization. Numerical experiments and an application


to neutrino detector design demonstrate the improved black-box optimization performance


of BOMM over existing methods with limited runs in moderate-to-high dimensions.


Given these promising results, there are several directions for further investigation. First,


we will explore broader function spaces (e.g., extensions of the additive multi-index GP in


Li et al. [2025]) on which marginal structure can similarly be exploited for optimization.


Next, given the promising results in Section 5.3, we will develop an adaptive implementation


of BOMM that sequentially exploits marginal structure, and investigate its theoretical


properties. Finally, we will investigate a multi-fidelity extension of BOMM to fully tackle


the neutrino detector design application using high-fidelity simulators.


32


### **A Proof of Theorem 1**

As before, we suppose _X_ = [�] _[d]_ _l_ =1 [[] _[L]_ _[l]_ _[, U]_ _[l]_ []. To prove Theorem 1 of the main paper, we]


first require the following lemmas.


**Lemma 1** (Theorem 2.2 of [Horowitz and Mammen, 2007]) **.** _Under Assumptions 1 – 4 of_


_the main paper, we have:_


� 2
_ϕ ◦_ _h_ 1 + _· · ·_ + � _h_ _d_ _−_ _ϕ ◦_ ( _h_ 1 + _· · ·_ + _h_ _d_ )
���� � � ��� _L_ [2]



=
� _X_



� � 2
_ϕ ◦_ _h_ 1 ( _x_ 1 ) + _· · ·_ + � _h_ _d_ ( _x_ _d_ ) _−_ _ϕ ◦_ ( _h_ 1 ( _x_ 1 ) + _· · ·_ + _h_ _d_ ( _x_ _d_ )) _d_ **x** = _O_ _p_ � _n_ _[−]_ [2] _[k/]_ [(2] _[k]_ [+1)] [�] _._
� � � �



We can generalize this lemma to establish the following uniform convergence result:


**Lemma 2.** _Under Assumptions 1 – 4 of the main paper, we have:_


�
_ϕ ◦_ _h_ 1 + _· · ·_ + � _h_ _d_ _−_ _ϕ ◦_ ( _h_ 1 + _· · ·_ + _h_ _d_ ) � _n_ _[−][k/]_ [(4] _[k]_ [+2)] [�] _._
���� � � ��� _L_ _[∞]_ [=] _[ O]_ _[p]_


_Proof (Lemma 2)._ Let us define:


_ψ_ := _ϕ ◦_ ( _h_ 1 + _· · ·_ + _h_ _d_ )


� �
_ψ_ := � _ϕ ◦_ _h_ 1 + _· · ·_ + � _h_ _d_ _._
� �


From Assumption 1, it follows that _ψ_ [�] _−_ _ψ ∈_ _W_ [1] _[,]_ [2] ( _X_ ). From the Sobolev Embedding


Theorem (Theorem 12.71 in [Hunter and Nachtergaele, 2001]), we get that:



2
_ψ −_ _ψ_
���� ���



2 2

_ψ −_ _ψ_
_L_ _[∞]_ _[≤]_ _[C]_ ���� ���



_W_ [1] _[,]_ [2] ( _X_ )



_L_ [2] [ +] �



_|α|_ =1


33









2
_α_ � _α_
_D_ _ψ −_ _D_ _ψ_
��� ��� _L_ [2]



= _C_







2
_ψ −_ _ψ_
���� ���



_|α|_ =1



2
= _O_ _ψ −_ _ψ_
��� _L_ [2]
�����



2
= _O_ _ψ −_ _ψ_
��� _L_
�����



+ _O_
�







 _|_ [�] _α|_ =1





 _,_



2
_α_ � _α_
_D_ _ψ −_ _D_ _ψ_
��� ��� _L_ [2]



2
_α_ � _α_
_D_ _ψ −_ _D_ _ψ_
��� ��� _L_



for some constant _C >_ 0. With _|α|_ = 1 and _|α_ ˜ _|_ = 2, from the Gagliardo-Nirenberg inequality


(Theorem 1 in [Nirenberg, 1966]), we have:



1

2



1

2



_α_ � _α_ _α_ ˜ � _α_ ˜
_D_ _ψ −_ _D_ _ψ_ _D_ _ψ −_ _D_ _ψ_
��� ��� _L_ [2] _[ ≤]_ _[C]_ [1] ��� ���



_L_ [2]



_ψ −_ _ψ_
���� ���



2

_ψ −_ _ψ_
_L_ [2] [ +] _[ C]_ [2] ���� ��� _L_ [2]



1

2



= _O_ _ψ −_ _ψ_
���
�����



2

_ψ −_ _ψ_
_L_ [2] [ +] ���� ��� _L_ [2]



�



= _O_ _p_ � _n_ _[−][k/]_ [(4] _[k]_ [+2)] [�] _,_


for some constants _C_ 1 _>_ 0 and _C_ 2 _>_ 0. The first equality follows from the fact that


_D_ _[α]_ [˜] [ �] _ψ −_ _D_ _[α]_ [˜] _ψ_ is continuous on a bounded domain _X_, and the second equality is a consequence


of Lemma 1. Therefore, we have:



2
_α_ � _α_
_D_ _ψ −_ _D_ _ψ_
��� ��� _L_ [2]



2
_α_ � _α_
_D_ _ψ −_ _D_ _ψ_
��� ���



2 2

_ψ −_ _ψ_
_L_ _[∞]_ [=] _[ O]_ ���
�����



+ _O_
�







 _|_ [�] _α|_ =1



_|α|_ =1













2
_ψ −_ _ψ_
���� ���



_L_ [2]



= _O_ _p_ � _n_ _[−]_ [2] _[k/]_ [(2] _[k]_ [+1)] [�] + _O_ _p_ � _n_ _[−][k/]_ [(2] _[k]_ [+1)] [�]


= _O_ _p_ � _n_ _[−][k/]_ [(2] _[k]_ [+1)] [�] _,_


which yields the statement.


With this, we can now prove Theorem 1 of the main paper.


_Proof (Theorem 1)._ Recall that _f_ ( **x** ) = _ϕ_ ( _h_ 1 ( _x_ 1 ) + _· · ·_ + _h_ _d_ ( _x_ _d_ )). Define:


� �
_f_ ( **x** ) := � _ϕ_ _h_ 1 ( _x_ 1 ) + _· · ·_ + � _h_ _d_ ( _x_ _d_ ) _,_
� �


where _ϕ_ [�], [�] _h_ 1 _, · · ·_ [�] _h_ _d_ are solutions to the constrained least squares problem in Equations


34


(7)–(8) of the main paper. Note that:


�
0 _≤_ _f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)]


�
= _f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [�][ (] **[x]** [�] _[∗]_ _n_ [) +][ �] _[f]_ [ (] **[x]** [�] _[∗]_ _n_ [)] _[ −]_ _[f]_ [�][ (] **[x]** _[∗]_ [) +][ �] _[f]_ [ (] **[x]** _[∗]_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)]


�
_≤_ _f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [�][ (] **[x]** [�] _[∗]_ _n_ [) +][ �] _[f]_ [ (] **[x]** _[∗]_ [)] _[ −]_ _[f]_ [(] **[x]** _[∗]_ [)] _[,]_


where the last inequality follows from the fact _f_ [�] ( **x** � _[∗]_ _n_ [)] _[ −]_ _[f]_ [�][ (] **[x]** _[∗]_ [)] _[ ≤]_ [0. From Lemma 2, we have:]


�
_f_ ( **x** _[∗]_ _n_ [)] _[ −]_ _[f]_ [�][ (] **[x]** [�] _[∗]_ _n_ [) =] _[ O]_ _[p]_ � _n_ _[−][k/]_ [(4] _[k]_ [+2)] [�] _,_


�
_f_ ( **x** _[∗]_ ) _−_ _f_ ( **x** _[∗]_ ) = _O_ _p_ � _n_ _[−][k/]_ [(4] _[k]_ [+2)] [�] _,_


which proves the statement.

### **B Proof of Proposition 2**


_Proof._ Since the linear functional of the Gaussian process remains a Gaussian process


[Bogachev, 1998], it is enough to show the posterior mean and covariance functions for


_m_ _l_ ( _x_ _l_ ) = � _h_ ( **x** ) _d_ **x** _−l_ . Note that the posterior mean and covariance functions for _h_ ( **x** ) =


_ϕ_ _[−]_ _λ_ [1] _[◦]_ _[f]_ [(] **[x]** [), given in Equation (12) of the main paper, follow directly from the GP predictive]


_̸_ _̸_


_̸_



equations (Equation (2) of main paper); we denote these as _µ_ _n,h_ ( **x** ) = �


_̸_ _̸_


_̸_



Ω _[h]_ [(] **[x]** [;] _[ ω]_ [)] [P] [(] _[dω]_ [) and]


_̸_ _̸_


_̸_



_k_ _n,h_ ( **x** _,_ **x** _[′]_ ), where P denotes the posterior measure on _h_ given data. For the posterior mean


function of _m_ _l_, using Fubini’s theorem, we have:


_̸_ _̸_


_̸_



_̸_ � _̸_


_̸_



_h_ ( **x** ; _ω_ ) _d_ **x** _−l_ P( _dω_ )

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_



E P


_̸_ _̸_


_̸_



_h_ ( **x** ) _d_ **x** _−l_

�� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_



=
� Ω

_̸_ _̸_


_̸_



_̸_ _̸_


=
� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


35



_̸_ _̸_


_h_ ( **x** ; _ω_ )P( _dω_ ) _d_ **x** _−l_

� Ω

_̸_


= _µ_ _n,h_ ( **x** ) _d_ **x** _−l_ _._
� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


For its posterior covariance function, note that:


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_ �


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


Cov P


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


�� _̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_h_ ( **x** ) _d_ **x** _−l_ _,_
� _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] � _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_h_ ( **x** _[′]_ ) _d_ **x** _[′]_ _−l_

_̸_ � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_h_ ( **x** _[′]_ ; _ω_ ) _−_ _µ_ _n,h_ ( **x** _[′]_ ) _d_ **x** _[′]_ _−l_

_̸_ ��� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_

_̸_ _̸_ �


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


=
� Ω

_̸_ _̸_


=
� Ω

_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_h_ ( **x** ; _ω_ ) _−_ _µ_ _n,h_ ( **x** ) _d_ **x** _−l_

�� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_

� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


P( _dω_ )


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


( _h_ ( **x** _[′]_ ; _ω_ ) _−_ _µ_ _n,h_ ( **x** _[′]_ ))( _h_ ( **x** ; _ω_ ) _−_ _µ_ _n,h_ ( **x** )) _d_ **x** _−l_ _d_ **x** _[′]_ _−l_ [P][(] _[dω]_ [)]

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


=
� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


=
� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_k_ _n,h_ ( **x** _,_ **x** _[′]_ ) _d_ **x** _−l_ _d_ **x** _[′]_ _−l_ _[,]_

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


( _h_ ( **x** _[′]_ ; _ω_ ) _−_ _µ_ _n,h_ ( **x** _[′]_ ))( _h_ ( **x** ; _ω_ ) _−_ _µ_ _n,h_ ( **x** ))P( _dω_ ) _d_ **x** _−l_ _d_ **x** _[′]_ _−l_

� Ω

_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


which proves the statement.

### **C Proof of Proposition 3**


Notice that:


_µ_ _n,ϕ_ _−λ_ 1 _[◦][f]_ [(] **[x]** [)] _[d]_ **[x]** _[−][l]_

� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


= _µ_ + ((1 _−_ _η_ ) **r** _n,A_ ( **x** ) + _η_ **r** _n,Z_ ( **x** )) _[⊤]_ ((1 _−_ _η_ ) **R** _n,A_ + _η_ **R** _n,Z_ ) _[−]_ [1] [ �] _ϕ_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** � _d_ **x** _−l_
� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


�( _U_ _j_ _−_ _L_ _j_ ) +

_j_ = _̸_ _l_ � _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


= _µ_ �


_̸_ _̸_



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_n_
� _q_ _i_ [(1 _−_ _η_ ) **r** _n,A_ ( **x** ) + _η_ **r** _n,Z_ ( **x** )] _i_ _d_ **x** _−l_ _,_


_̸_ _̸_ _i_ =1



_̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_ _̸_


where _q_ _i_ is the _i_ [th] coordinate of ((1 _−_ _η_ ) **R** _n,A_ + _η_ **R** _n,Z_ ) _[−]_ [1] [ �] _ϕ_ _[−]_ _λ_ [1] [(] **[f]** _[n]_ [)] _[ −]_ _[µ]_ **[1]** �, and:


[(1 _−_ _η_ ) **r** _n,A_ ( **x** ) + _η_ **r** _n,Z_ ( **x** )] _i_ = (1 _−_ _η_ ) _r_ _A_ ( **x** _i_ _−_ **x** ) + _ηr_ _Z_ ( **x** _i_ _−_ **x** ) _._


36


Also note that:


_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_n_


_q_ _i_

�


_̸_ _i_ =1 _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_r_ _A_ ( **x** _i_ _−_ **x** ) _d_ **x** _−l_

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_n_
� _q_ _i_ [(1 _−_ _η_ ) **r** _n,A_ ( **x** ) + _η_ **r** _n,Z_ ( **x** )] _i_ _d_ **x** _−l_ = (1 _−_ _η_ )


_̸_ _i_ =1 _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_r_ _Z_ ( **x** _i_ _−_ **x** ) _d_ **x** _−l_ _._

� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


+ _η_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_n_


_q_ _i_

�


_i_ =1 _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


In fact, we can derive explicit formulae to compute these integrals by exploiting the structure


of _r_ _A_ and _r_ _Z_ . We first focus on the expression that involves _r_ _A_ . Using the additive structure


of _r_ _A_ with [�] _[n]_ _k_ =1 _[w]_ _[k]_ [ = 1] _[, w]_ _[k]_ _[ ≥]_ [0 and] **[ x]** _[i]_ [ = (] _[x]_ _[i,]_ [1] _[,][ · · ·][, x]_ _[i,d]_ [)] _[⊤]_ [, we have:]


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


exp( _−_ ( _x_ _k_ _−_ _x_ _i,k_ ) [2] _/θ_ _A,k_ [2] [)] _[d]_ **[x]** _[−][l]_ _[,]_

_̸_ � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_r_ _A_ ( **x** _−_ **x** _i_ ) _d_ **x** _−l_ =

� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_d_


_w_ _k_

�


_̸_ _k_ =1 _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


and it can be shown that:


exp( _−_ ( _x_ _k_ _−_ _x_ _i,k_ ) [2] _/θ_ _A,k_ [2] [)] _[d]_ **[x]** _[−][l]_

� � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


exp( _−_ ( _x_ _j_ _−_ _x_ _i,l_ ) [2] _/θ_ _A,l_ [2] [)] _[ ·]_ [ �] _̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


=


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_




 _̸_


 _̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


exp( _−_ ( _x_ _j_ _−_ _x_ _i,l_ ) [2] _/θ_ _A,l_ [2] [)] _[ ·]_ [ �] _j_ = _̸_ _l_ [(] _[U]_ _[j]_ _[ −]_ _[L]_ _[j]_ [)] for _k_ = _l,_


� _L_ [exp] � _−_ ( _x_ _k_ _−_ _x_ _i,k_ ) [2] _/θ_ _A,k_ [2] � _dx_ _k_ _·_ [�] = _̸_ _l,k_ [(] _[U]_ _[j]_ _[ −]_ _[L]_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


[ _L_ _k_ _,U_ _k_ ] [exp] � _−_ ( _x_ _k_ _−_ _x_ _i,k_ ) [2] _/θ_ _A,k_ [2] � _dx_ _k_ _·_ [�] _̸_


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_j_ = _̸_ _l,k_ [(] _[U]_ _[j]_ _[ −]_ _[L]_ _[j]_ [)] for _k ̸_ = _l._


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


In particular, we have:


exp( _−_ ( _x_ _k_ _−_ _x_ _i,k_ ) [2] _/θ_ _A,k_ [2] [)] _[dx]_ _[k]_ [=] _[ √][πθ]_ _[A,k]_ [(Φ] _[i,k]_ [(] _[U]_ _[k]_ [)] _[ −]_ [Φ] _[i,k]_ [(] _[L]_ _[k]_ [))] _[,]_

� [ _L_ _k_ _,U_ _k_ ]


where Φ _i,k_ is the cumulative distribution corresponding to _N_ � _x_ _i,k_ _, θ_ _A,k_ [2] _[/]_ [2] � . Therefore, we


have:


_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


�

_̸_ _̸_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


( _U_ _j_ _−_ _L_ _j_ )


_̸_ _j_ = _̸_ _l_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_r_ _A_ ( **x** _−_ **x** _i_ ) _d_ **x** _−l_ = _w_ _l_ exp( _−_ ( _x_ _l_ _−_ _x_ _i,l_ ) [2] _/θ_ _A,l_ [2] [)] �
� _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _j_ = _̸_ _l_



_̸_ _̸_


_̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_


37


+ _[√]_ _π_
�

_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



� ( _U_ _j_ _−_ _L_ _j_ ) _,_
_̸_ _j_ = _̸_ _l,k_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



� _w_ _k_ _θ_ _A,k_ (Φ _i,k_ ( _U_ _k_ ) _−_ Φ _i,k_ ( _L_ _k_ )) �

_k_ = _̸_ _l_ _j_ = _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


which leads to:


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_r_ _A_ ( **x** _−_ **x** _i_ ) _d_ **x** _−l_ = (1 _−η_ ) _w_ _l_ �
� _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _j_ = _̸_ _l_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


( _U_ _j_ _−L_ _j_ ) _·_


_̸_ _j_ = _̸_ _l_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


� _q_ _i_ exp( _−_ ( _x_ _l_ _−x_ _i,l_ ) [2] _/θ_ _A,l_ [2] [)+] _[C]_ _[l]_ _[,]_


_̸_ _̸_ _i_ =1


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_n_
�


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


(1 _−η_ )


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_n_


_q_ _i_

�


_i_ =1 _̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


�

_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


for some constant _C_ _l_ _>_ 0. For the second term, note that:


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_r_ _Z_ ( **x** _−_ **x** _i_ ) _d_ **x** _−l_ =
� _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] � _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


�

_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []]


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_d_
� exp � _−_ ( _x_ _j_ _−_ _x_ _i,j_ ) [2] _/θ_ _Z,j_ [2] � _d_ **x** _−l_

_̸_ _̸_ _j_ =1


_̸_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


= exp( _−_ ( _x_ _l_ _−_ _x_ _i,l_ ) [2] _/θ_ _Z,l_ [2] [)] _[ ·]_ �

_j_ = _̸_ _l_


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


exp( _−_ ( _x_ _j_ _−_ _x_ _i,j_ ) [2] _/θ_ _Z,j_ [2] [)] _[dx]_ _[j]_

_̸_ � [ _L_ _j_ _,U_ _j_ ]


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_d−_ 1
= exp( _−_ ( _x_ _l_ _−_ _x_ _i,l_ ) [2] _/θ_ _Z,l_ [2] [)] _[ ·][ π]_ 2


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


[�] _θ_ _Z,j_ ˜Φ _i,j_ ( _U_ _j_ ) _−_ ˜Φ _i,j_ ( _L_ _j_ ) _,_

� �
_j_ = _̸_ _l_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_−_

2 [�]


_̸_


_̸_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


where Φ [˜] _i,j_ is the cumulative distribution corresponding to _N_ ( _x_ _i,j_ _, θ_ _Z,j_ [2] _[/]_ [2). Combining these]

together, it follows that arg min _x_ _l_ _∈_ [ _L_ _l_ _,U_ _l_ ] � � _j_ = _̸_ _l_ [[] _[L]_ _[j]_ _[,U]_ _[j]_ []] _[ µ]_ _[n,ϕ]_ _λ_ _[−]_ [1] _[◦][f]_ [(] **[x]** [)] _[d]_ **[x]** _[−][l]_ [ is equivalent to:]


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


arg min
_x_ _l_ _∈_ [ _L_ _l_ _,U_ _l_ ]


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


(1 _−_ _η_ ) _w_ _l_ Vol( _X_ _−l_ )

�


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_n_
�


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_d−_ 1

� _q_ _i_ exp ( _−_ ( _x_ _l_ _−_ _x_ _i,l_ ) [2] _/θ_ _A,l_ [2] [) +] _[ π]_ 2


_i_ =1


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


2 _η_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_n_
� _p_ _i,l_ _q_ _i_ exp( _−_ � _x_ _l_ _−_ _x_ _i,l_ ) [2] _/θ_ _Z,l_ [2] �

_i_ =1 �


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_,_


_̸_ _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


where Vol ( _X_ _−l_ ) = [�] _̸_ _̸_


statement.



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_j_ = _̸_ _l_ [(] _[U]_ _[j]_ _[ −]_ _[L]_ _[j]_ [) and] _[ p]_ _[i,l]_ [ =] [ �] _̸_



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _j_ = _̸_ _l_ _[θ]_ _[Z,j]_ �˜Φ _i,j_ ( _U_ _j_ ) _−_ ˜Φ _i,j_ ( _L_ _j_ )� . This proves the



_̸_ _̸_


_̸_ _̸_


_̸_ _̸_


_̸_


_̸_


_̸_


_̸_ _̸_

### **D Proof of Theorem 4**


_Proof._ Denote _h_ ( **x** ) = _ϕ_ _[−]_ _λ_ [1] _◦_ _f_ ( **x** ), and its posterior mean and variance as _µ_ _n,h_ ( **x** ) and


_k_ _n,h_ ( **x** _,_ **x** ) (see Equation (12) of the main paper). To ease presentation, denote the minimizer


as **x** _[∗]_ = ( _x_ _[∗]_ 1 _[,][ · · ·][, x]_ _[∗]_ _d_ [) and the BOMM estimator to be] [ �] **[x]** [BOMM] [ :] [= (] _[x]_ [�] [1] _[,][ · · ·][,]_ [ �] _[x]_ _[d]_ [). We prove the]


desired convergence result in the following two steps.


38


**Step 1:** We first show that the marginal mean of _h_ can be well-approximated by that of


_µ_ _n,h_ ( **x** ). More precisely, we establish, for all _j_ = 1 _, · · ·, d_ :



= _O_ (exp ( _−C/d_ _n_ )) _,_ (21)
����



sup
_x_ _j_ _∈_ [ _L_ _j_ _,U_ _j_ ]



����



_h_ ( **x** ) _d_ **x** _−j_ _−_ _µ_ _n,h_ ( **x** ) _d_ **x** _−j_
� �



where _d_ _n_ := sup **x** _∈X_ inf **x** _′_ _∈{_ **x** 1 _,···,_ **x** _n_ _}_ _∥_ **x** _−_ **x** _[′]_ _∥_ is the so-called fill-distance in the kriging and


kernel interpolation literature [Wendland, 2004]. To show this, recall that _h ∈H_ TAAG .


Then, by Corollary 3.11 in [Kanagawa et al., 2018], we first have:


_|h_ ( **x** ) _−_ _µ_ _n,h_ ( **x** ) _| ≤∥h∥_ _H_ TAAG _V_ _n,h_ ( **x** ) _,_ where _V_ _n,h_ ( **x** ) := _k_ _n,h_ ( **x** _,_ **x** ) (22)
~~�~~


for all **x** _∈X_ . Furthermore, since the kernel _k_ TAAG is infinitely differentiable, by Theorem


11.22 in [Wendland, 2004], we know that, for large enough _n_ :



sup
**x** _∈X_



_V_ _n,h_ ( **x** ) ≲ exp ( _−C/d_ _n_ ) _,_ (23)

~~�~~



where the constant _C_ is independent of _n_ . Combining (22) and (23), we have, for large


enough _n_ :


sup _|h_ ( **x** ) _−_ _µ_ _n,h_ ( **x** ) _|_ ≲ _∥h∥_ _H_ TAAG exp ( _−C/d_ _n_ ) _._ (24)
**x** _∈X_


Back to (21), for any _j ∈{_ 1 _, · · ·, d}_, we have, for large enough _n_ :



_≤_ sup _|h −_ _µ_ _n,h_ _| d_ **x** _−j_ ≲ _∥h∥_ _H_ TAAG exp ( _−C/d_ _n_ ) _._
���� � **x** _∈X_



sup
_x_ _j_ _∈_ [ _L_ _j_ _,U_ _j_ ]



����



_h_ ( **x** ) _d_ **x** _−j_ _−_ _µ_ _n,h_ ( **x** ) _d_ **x** _−j_
� �



�
**Step 2:** We next establish convergence of � **x** [BOMM] : = ( _x_ 1 _, · · ·,_ � _x_ _d_ ) to **x** _[∗]_ = ( _x_ _[∗]_ 1 _[,][ · · ·][, x]_ _[∗]_ _d_ [) and]


�
_h_ ( **x** [BOMM] ) to _h_ ( **x** _[∗]_ ). Let us define the following notation:


�
_m_ _j_ ( _x_ _j_ ) := _h_ ( **x** ) _d_ **x** _−j_ _,_ _m_ _j_ ( _x_ _j_ ) := _µ_ _n,h_ ( **x** ) _d_ **x** _−j_ _._
� �


39


Using the first-order dominating condition in Assumption 9, we know that _x_ _[∗]_ _j_ [minimizes]


_m_ _j_ ( _x_ _j_ ). From Step 1 (see (21) ), we have, for some constant _C >_ 0 with large enough _n >_ 0:



= _O_ (exp ( _−C/d_ _n_ )) _._ (25)
����



� � �
_|m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _j_ ) _| ≤_ sup
_x_ _j_ _∈_ [ _L_ _j_ _,U_ _j_ ]


Similarly, by the same logic:


_∗_
��� _m_ _j_ ( _x_ _j_ [)] _[ −]_ _[m]_ _[j]_ [(] _[x]_ _[∗]_ _j_ [)] �� _≤_ sup
_x_ _j_ _∈_ [ _L_ _j_ _,U_ _j_ ]



����



_h_ ( **x** ) _d_ **x** _−j_ _−_ _µ_ _n,h_ ( **x** ) _d_ **x** _−j_
� �



����



_µ_ _n,h_ ( **x** ) _d_ **x** _−j_ _−_ _h_ ( **x** ) _d_ **x** _−j_ = _O_ (exp ( _−C/d_ _n_ )) _._ (26)
� � ����



Hence, for all _j ∈{_ 1 _, · · ·, d}_ with large enough _n >_ 0, we have:


�
0 _≤_ _m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _[∗]_ _j_ [)]


� � � � �
= _m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _j_ ) + � _m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _[∗]_ _j_ [) +][ �] _[m]_ _[j]_ [(] _[x]_ _[∗]_ _j_ [)] _[ −]_ _[m]_ _[j]_ [(] _[x]_ _[∗]_ _j_ [)]


� �
= _O_ (exp ( _−C/d_ _n_ )) + � _m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _[∗]_ _j_ [) +] _[ O]_ [ (exp (] _[−][C/d]_ _[n]_ [))] _[,]_


where the first inequality follows from the definition of _x_ _[∗]_ _j_ [, and the last equality comes from]


(25) and (26). Since � _m_ _j_ ( _x_ � _j_ ) _−_ _m_ � _j_ ( _x_ _[∗]_ _j_ [)] _[ ≤]_ [0 by the definition of][ �] _[x]_ _[j]_ [, we deduce that:]


�� _m_ _j_ ( _x_ � _j_ ) _−_ _m_ _j_ ( _x_ _∗j_ [)] �� = _O_ (exp ( _−C/d_ _n_ )) _._


Under Assumption 3, we know that the fill-distance _d_ _n_ converges to zero in probability

as _n_ increases [Oates et al., 2019; Helin et al., 2022], yielding _m_ _j_ ( _x_ � _j_ ) _→_ _P_ _m_ _j_ ( _x_ _∗j_ [) for all]


_j ∈{_ 1 _, · · ·, d}_ . Furthermore, as _h ∈H_ TAAG, _m_ _j_ is a continuous function on a closed interval

[ _L_ _j_ _, U_ _j_ ]. Then, from the uniqueness of **x** _[∗]_ (Assumption 8), � **x** [BOMM] _→_ _P_ **x** _∗_ follows. To see


this, let _ϵ >_ 0 and consider a set _B_ _j_ := _{x_ _j_ : _|x_ _j_ _−_ _x_ _[∗]_ _j_ _[| ≥]_ _[ϵ][}]_ [. Due to the uniqueness of] **[ x]** _[∗]_ [, we]


know that inf _x_ _j_ _∈B_ _j_ _m_ _j_ ( _x_ _j_ ) _−_ _m_ _j_ ( _x_ _[∗]_ _j_ [)] _[ ≥]_ _[η]_ [, for some] _[ η >]_ [ 0. Therefore, for all] _[ j][ ∈{]_ [1] _[,][ · · ·][, d][}]_ [,]


40


we have:


ˆ _P_
P � _|x_ _j_ _−_ _x_ _[∗]_ _j_ _[| ≥]_ _[ϵ]_ � _≤_ P � _m_ _j_ (ˆ _x_ _j_ ) _−_ _m_ _j_ ( _x_ _[∗]_ _j_ [)] _[ ≥]_ _[η/]_ [2] � _→_ 0 as _n →∞._


Moreover, from the continuity of _h_ and _ϕ_ _λ_, we obtain _h_ ( **x** � [BOMM] ) _→_ _P_ _h_ ( **x** _∗_ ) as well as

_f_ ( **x** � [BOMM] ) = _ϕ_ _λ_ _◦_ _h_ ( **x** � [BOMM] ) _→_ _P_ _f_ ( **x** _∗_ ) = _ϕ_ _λ_ _◦_ _h_ ( **x** _∗_ ), which proves the claim.

### **E Proof of Corollary 1**


_Proof._ To avoid confusion, let us denote the tail BOMM estimator (from Algorithm 1 of


the main paper) as � **x** [TBOMM] and the BOMM estimator as � **x** [BOMM] . Observe that:


0 _≤_ _h_ �� **x** TBOMM � _−_ _h_ ( **x** _[∗]_ )


= _h_ �� **x** TBOMM � _−_ _µ_ _n,h_ �� **x** TBOMM � + _µ_ _n,h_ �� **x** TBOMM � _−_ _µ_ _n,h_ �� **x** BOMM �


+ _µ_ _n,h_ �� **x** BOMM � _−_ _h_ �� **x** BOMM � + _h_ �� **x** BOMM � _−_ _h_ ( **x** _[∗]_ )


_≤_ _h_ �� **x** TBOMM � _−_ _µ_ _n,h_ �� **x** TBOMM � + _µ_ _n,h_ �� **x** BOMM � _−_ _h_ �� **x** BOMM � + _h_ �� **x** BOMM � _−_ _h_ ( **x** _[∗]_ ) _,_


TBOMM BOMM
where we used the identity _µ_ _n,h_ �� **x** � _≤_ _µ_ _n,h_ �� **x** � in the last inequality, which


holds using the specification rule for _α_ in Appendix F. From (24), for large enough _n_, we


know that:


�� _h_ �� **x** TBOMM � _−_ _µ_ _n,h_ �� **x** TBOMM ��� = _O_ (exp ( _−C/d_ _n_ ))

�� _µ_ _n,h_ �� **x** BOMM � _−_ _h_ �� **x** BOMM ��� = _O_ (exp ( _−C/d_ _n_ )) _._


This gives us:


0 _≤_ _h_ �� **x** TBOMM � _−_ _h_ ( **x** _[∗]_ ) = _O_ (exp ( _−C/d_ _n_ )) + _h_ �� **x** BOMM � _−_ _h_ ( **x** _[∗]_ ) _._ (27)


41


From Theorem 4, we observe that the right-most term of (27) converges to zero in probability


_P_
as _n →∞_ . Under Assumption 3, we know the fill-distance _d_ _n_ _→_ 0 in probability as _n →∞_ .


TBOMM _P_ _∗_
And therefore, we have _h_ �� **x** � _→_ _h_ ( **x** ). From the continuity of _ϕ_ _λ_, we can further show

the convergence of _f_ �� **x** TBOMM � = _ϕ_ _λ_ _◦_ _h_ ( **x** � [TBOMM] ) _→_ _P_ _f_ ( **x** _∗_ ) = _ϕ_ _λ_ _◦_ _h_ ( **x** _∗_ ), as desired.

### F Selection of tail probability α in BOMM+


In our experiments, we employ the following strategy for selecting the tail probability _α_


in the tail BOMM estimator (see Equation (17) of the main paper). The idea is to choose


_α_ such that the predicted response of _h_ (and thus _f_ ) at the tail BOMM estimator ˆ **x** _[∗]_ _n,α_ [is]


minimized. In other words, this uses the fitted model to calibrate a good choice of _α_ that


effectively leverages local additivity for minimization; a similar idea was used in Mak and


Wu, 2019 for discrete optimization. Formally, _α_ is selected as:


_α_ _[∗]_ = argmin _µ_ _n,h_ (ˆ **x** _[∗]_ _n,α_ [)] _[,]_ (28)
_α∈_ (0 _,_ 1]


where _µ_ _n,h_ is the posterior mean of _h_ (see Equation (12) of the main paper). This one

dimensional optimization is performed via grid search in our numerical experiments.

### **G Test function specification**


We provide below the detailed specification of test functions in numerical experiments:


  - The six-hump camel function [Molga and Smutnicki, 2005] in _d_ = 6 dimensions:



_x_ [2] 2 _k−_ 1 [+] _[ x]_ [2] _[k][−]_ [1] _[x]_ [2] _[k]_ [+ (] _[−]_ [4 + 4] _[x]_ [2] 2 _k_ [)] _[x]_ 2 [2] _k_
�



+ 5 _,_
�



_f_ ( **x** ) =



3
�


_k_ =1



��4 _−_ 2 _._ 1 _x_ [2] 2 _k−_ 1 [+] _[ x]_ 2 [4] 3 _k−_ 1



_x_ 1 _, x_ 3 _, x_ 5 _∈_ [ _−_ 2 _,_ 2] _,_ _x_ 2 _, x_ 4 _, x_ 6 _∈_ [ _−_ 1 _,_ 1] _._


42


- The wing weight function [Moon, 2010] in _d_ = 10 dimensions:



_−_ 0 _._ 3
( _N_ _z_ _W_ _dg_ ) [0] _[.]_ [49] + _S_ _w_ _W_ _p_ _,_
�



0 _._ 6
_q_ [0] _[.]_ [006] _λ_ [0] _[.]_ [04] 100 _t_ _c_
� � cos(Λ)



_f_ ( **x** ) = 0 _._ 036 _S_ _w_ [0] _[.]_ [758] _W_ _fw_ [0] _[.]_ [0035]



_A_
� cos [2] (Λ)



**x** = ( _S_ _w_ _, W_ _fw_ _, A,_ Λ _, q, λ, t_ _c_ _, N_ _z_ _, W_ _dg_ _, W_ _p_ )


_S_ _w_ _∈_ [150 _,_ 200] _, W_ _fw_ _∈_ [220 _,_ 300] _, A ∈_ [6 _,_ 10] _,_ Λ _∈_ [ _−_ 10 _,_ 10] _, q ∈_ [16 _,_ 45] _,_


_λ ∈_ [0 _._ 5 _,_ 1] _, t_ _c_ _∈_ [0 _._ 08 _,_ 0 _._ 18] _, N_ _z_ _∈_ [2 _._ 5 _,_ 6] _, W_ _dg_ _∈_ [1700 _,_ 2500] _, W_ _p_ _∈_ [0 _._ 025 _,_ 0 _._ 08] _._


- The OTL circuit function [Moon et al., 2012] in _d_ = 6 dimensions:


_[β]_ [(] _[R]_ _[c]_ [2] [ + 9][)] 11 _._ 35 _R_ _f_ 0 _._ 74 _R_ _f_ _β_ ( _R_ _c_ 2 + 9)
_f_ ( **x** ) = [(] _[V]_ _[b]_ [1] [ + 0] _[.]_ [74][)] + + _,_

_β_ ( _R_ _c_ 2 + 9) + _R_ _f_ _β_ ( _R_ _c_ 2 + 9) + _R_ _f_ ~~�~~ _β_ ( _R_ _c_ 2 + 9) + _R_ _f_ ~~�~~ _R_ _c_ 1

12 _R_ _b_ 2
**x** = ( _R_ _b_ 1 _, R_ _b_ 2 _, R_ _f_ _, R_ _c_ 1 _, R_ _c_ 2 _, β_ ) _,_ _V_ _b_ 1 = _,_
_R_ _b_ 1 + _R_ _b_ 2


_R_ _b_ 1 _∈_ [50 _,_ 150] _, R_ _b_ 2 _∈_ [25 _,_ 75] _, R_ _f_ _∈_ [0 _._ 5 _,_ 3] _,_


_R_ _c_ 1 _∈_ [1 _._ 2 _,_ 2 _._ 5] _, R_ _c_ 2 _∈_ [0 _._ 25 _,_ 1 _._ 2] _, β ∈_ [50 _,_ 300] _._


- The piston simulation function [Moon, 2010] in _d_ = 7 dimensions:



_f_ ( **x** ) = 2 _π_

~~�~~



_M_
_k_ + _S_ [2] _[P]_ [0] _[ V]_ [0] _T_ _a_ _,_ **x** = ( _M, S, V_ 0 _, k, P_ 0 _, T_ _a_ _, T_ 0 ) _,_

_T_ 0 _V_ [2]



_V_ = _[S]_

2 _k_ � ~~[�]~~




[0] _[ V]_ [0]

_T_ _a_ _−_ _A_ _,_ _A_ = _P_ 0 _S_ + 19 _._ 62 _M −_ _[k V]_ [0]
_T_ 0 � _S_



_A_ [2] + 4 _k_ _[P]_ [0] _[ V]_ [0]



_S_ _[,]_



_M ∈_ [30 _,_ 60] _, S ∈_ [0 _._ 005 _,_ 0 _._ 020] _, V_ 0 _∈_ [0 _._ 002 _,_ 0 _._ 010] _, k ∈_ [1000 _,_ 5000] _,_


_P_ 0 _∈_ [90 000 _,_ 110 000] _, T_ _a_ _∈_ [290 _,_ 296] _, T_ 0 _∈_ [340 _,_ 360] _._


- Custom test function in _d_ = 9 dimensions (Equation (20) of the main paper): _ϵ_ = 0 _._ 01,


_m_ 1 = _m_ 3 = _m_ 5 = 2 _._ 5, _m_ 2 = _m_ 8 = 3 _._ 5, _m_ 4 = 4, _m_ 6 = _m_ 7 = _m_ 9 = 4 _._ 5.


43


### **References**

Aronszajn, N. (1950). Theory of reproducing kernels. _Transactions of the American Mathematical_
_Society_, 68(3):337–404.
Ba, S. and Joseph, V. R. (2018). _MaxPro: Maximum Projection Designs_ . R package version 4.1-2.
Bellman, R. (1966). Dynamic programming. _Science_, 153(3731):34–37.
Bogachev, V. I. (1998). _Gaussian Measures_ . Number 62. American Mathematical Soc.
Booth, A. S. (2024). _deepgp: Bayesian Deep Gaussian Processes using MCMC_ . R package version
1.1.3.
Box, G. E. and Cox, D. R. (1964). An analysis of transformations. _Journal of the Royal Statistical_
_Society Series B: Statistical Methodology_, 26(2):211–243.
Bull, A. D. (2011). Convergence rates of efficient global optimization algorithms. _Journal of_
_Machine Learning Research_, 12(10).
Canetti, L., Drewes, M., and Shaposhnikov, M. (2012). Matter and Antimatter in the Universe.
_New Journal of Physics_, 14(9):095012.
Carnell, R. (2024). _lhs: Latin Hypercube Samples_ . R package version 1.2.0.
Chen, Z., Mak, S., and Wu, C. F. J. (2024). A hierarchical expected improvement method for
Bayesian optimization. _Journal of the American Statistical Association_, 119(546):1619–1632.
Chevalier, C. and Ginsbourger, D. (2013). Fast computation of the multi-points expected improvement with applications in batch selection. In _International Conference on Learning and_
_Intelligent Optimization_, pages 59–69. Springer.
Deng, X., Kang, L., and Lin, C. D. (2025). Design of experiments for emulations: A selective
review from a modeling perspective. _arXiv preprint arXiv:2505.09596_ .
Dette, H. and Pepelyshev, A. (2010). Generalized Latin hypercube design for computer experiments.
_Technometrics_, 52(4):421–429.
Ding, L., Mak, S., and Wu, C. F. J. (2019). BdryGP: a new Gaussian process model for
incorporating boundary information. _arXiv preprint arXiv:1908.08868_ .
Ding, L., Mak, S., and Wu, C. F. J. (2025). The BdryMat´ern GP: Reliable incorporation of
boundary information on irregular domains for Gaussian process modeling. _arXiv preprint_
_arXiv:2507.09178_ .
Dolinski, M. J., Poon, A. W. P., and Rodejohann, W. (2019). Neutrinoless Double-Beta Decay:
Status and Prospects. _Ann. Rev. Nucl. Part. Sci._, 69:219–251.
Ehlers, R., Chen, Y., Mulligan, J., Ji, Y., Kumar, A., Mak, S., Jacobs, P., Majumder, A., Angerami,
A., Arora, R., et al. (2025). Bayesian inference analysis of jet quenching using inclusive jet and
hadron suppression measurements. _Physical Review C_, 111(5):054913.
Frazier, P. I., Powell, W. B., and Dayanik, S. (2008). A knowledge-gradient policy for sequential
information collection. _SIAM Journal on Control and Optimization_, 47(5):2410–2439.
Golchi, S., Bingham, D. R., Chipman, H., and Campbell, D. A. (2015). Monotone emulation of
computer experiments. _SIAM/ASA Journal on Uncertainty Quantification_, 3(1):370–392.
Gramacy, R. B. (2020). _Surrogates: Gaussian Process Modeling, Design, and Optimization for the_
_Applied Sciences_ . Chapman and Hall/CRC.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). _The Elements of Statistical Learning: Data_
_Mining, Inference, and Prediction_ . Springer.
Hastie, T. J. and Tibshirani, R. J. (1990). _Generalized Additive Models_ . CRC Press.
Helin, T., Stuart, A., Teckentrup, A., and Zygalakis, K. (2022). Introduction to Gaussian process
regression in Bayesian inverse problems, with new results on experimental design for weighted
error measures. _International Conference on Monte Carlo and Quasi-Monte Carlo Methods in_


44


_Scientific Computing. Cham: Springer International Publishing_ .
Horowitz, J. L. and Mammen, E. (2007). Rate-optimal estimation for a general class of nonparametric regression models with unknown link functions. _Annals of Statistics_, 35(6):2589–2619.
Hunter, J. K. and Nachtergaele, B. (2001). _Applied Analysis_ . World Scientific.
Ji, Y., Mak, S., Soeder, D., Paquet, J. F., and Bass, S. A. (2024a). A graphical multi-fidelity
Gaussian process model, with application to emulation of expensive computer simulations.
_Technometrics_, 66(2):267–281.
Ji, Y., Yuchi, H. S., Soeder, D., Paquet, J.-F., Bass, S. A., Joseph, V. R., Wu, C. J., and Mak, S.
(2024b). Conglomerate multi-fidelity Gaussian process modeling, with application to heavy-ion
collisions. _SIAM/ASA Journal on Uncertainty Quantification_, 12(2):473–502.
Jones, D. R., Schonlau, M., and Welch, W. J. (1998). Efficient global optimization of expensive
black-box functions. _Journal of Global Optimization_, 13:455–492.
Joseph, V. R., Gul, E., and Ba, S. (2015). Maximum projection designs for computer experiments.
_Biometrika_, 102(2):371–380.
Kanagawa, M., Hennig, P., Sejdinovic, D., and Sriperumbudur, B. K. (2018). Gaussian processes
and kernel methods: A review on connections and equivalences. _arXiv preprint arXiv:1807.02582_ .
Kaufman, C. G., Bingham, D., Habib, S., Heitmann, K., and Frieman, J. A. (2011). Efficient
emulators of computer experiments using compactly supported correlation functions, with an
application to cosmology. _Annals of Applied Statistics_, 5(4):2470–2492.
Kearns, M. and Singh, S. (2002). Near-optimal reinforcement learning in polynomial time. _Machine_
_Learning_, 49:209–232.
Kim, H., Liu, C., and Chen, Y. (2025). Bayesian optimization with inexact acquisition: Is random
grid search sufficient? In _The 41st Conference on Uncertainty in Artificial Intelligence_ .
Kim, H. and Sanz-Alonso, D. (2025). Enhancing Gaussian process surrogates for optimization
and posterior approximation via random exploration. _to appear in SIAM/ASA Journal on_
_Uncertainty Quantification_ .
Kim, H., Sanz-Alonso, D., and Yang, R. (2024). Optimization on manifolds via graph Gaussian
processes. _SIAM Journal on Mathematics of Data Science_, 6(1):1–25.
LEGEND Collaboration (2021). LEGEND-1000 preconceptual design report.
Li, K., Mak, S., Paquet, J.-F., and Bass, S. A. (2025). Additive multi-index Gaussian process
modeling, with application to multi-physics surrogate modeling of the quark-gluon plasma.
_Journal of the American Statistical Association_ . Forthcoming.
Lin, L.-H. and Joseph, V. R. (2020). Transformation and additivity in Gaussian processes.
_Technometrics_, 62(4):525–535.
Lin, L.-H. and Joseph, V. R. (2021). _TAG: Transformed Additive Gaussian Processes_ . R package
version 0.5.1.
Loeppky, J. L., Sacks, J., and Welch, W. J. (2009). Choosing the sample size of a computer
experiment: A practical guide. _Technometrics_, 51(4):366–376.
Mak, S., Sung, C.-L., Wang, X., Yeh, S.-T., Chang, Y.-H., Joseph, V. R., Yang, V., and Wu, C.
F. J. (2018). An efficient surrogate model for emulation and physics extraction of large eddy
simulations. _Journal of the American Statistical Association_, 113(524):1443–1456.
Mak, S. and Wu, C. F. J. (2019). Analysis-of-marginal-tail-means (atm): A robust method for
discrete black-box optimization. _Technometrics_, 61(4):545–559.
McKay, M. D., Beckman, R. J., and Conover, W. J. (2000). A comparison of three methods for
selecting values of input variables in the analysis of output from a computer code. _Technometrics_,
42(1):55–61.
Meierhofer, G. (2010). _Neutron capture on_ [76] Ge. Phd thesis, Eberhard Karls Universitaet Tuebingen.


45


Available at `[https://www.mpi-hd.mpg.de/gerda/public/2010/phd2010_georgMeierhofer.](https://www.mpi-hd.mpg.de/gerda/public/2010/phd2010_georgMeierhofer.pdf)`
`[pdf](https://www.mpi-hd.mpg.de/gerda/public/2010/phd2010_georgMeierhofer.pdf)` .
Miller, J. J. and Mak, S. (2025). Targeted variance reduction: Effective Bayesian optimization of
black-box simulators with noise parameters. _Technometrics_, (just-accepted):1–23.
Miller, J. J., Mak, S., Sun, B., Narayanan, S. R., Yang, S., Sun, Z., Kim, K. S., and Kweon,
C.-B. M. (2024). Expected diverse utility (EDU): Diverse Bayesian optimization of expensive
computer simulators. _arXiv preprint arXiv:2410.01196_ .
Molga, M. and Smutnicki, C. (2005). Test functions for optimization needs. _Test functions for_
_optimization needs_, 101:48.
Montagna, S. and Tokdar, S. T. (2016). Computer emulation with nonstationary Gaussian
processes. _SIAM/ASA Journal on Uncertainty Quantification_, 4(1):26–47.
Moon, H. (2010). _Design and analysis of computer experiments for screening input variables_ . PhD
thesis, The Ohio State University.
Moon, H., Dean, A. M., and Santner, T. J. (2012). Two-stage sensitivity-based group screening in
computer experiments. _Technometrics_, 54(4):376–387.
Morris, M. D. and Mitchell, T. J. (1995). Exploratory designs for computational experiments.
_Journal of Statistical Planning and Inference_, 43(3):381–402.
Neuberger, M. (2023). warwick-legend. GitHub repository: `[https://github.com/](https://github.com/MoritzNeuberger/warwick-legend)`
`[MoritzNeuberger/warwick-legend](https://github.com/MoritzNeuberger/warwick-legend)` .
Neuberger, M., Pertoldi, L., Sch¨onert, S., and Wiesinger, C. (2021). The cosmic muon-induced
background for the LEGEND-1000 alternative site at LNGS. _Journal of Physics: Conference_
_Series_, 2156(012216).
Nirenberg, L. (1966). An extended interpolation inequality. _Annali della Scuola Normale Superiore_
_di Pisa-Scienze Fisiche e Matematiche_, 20(4):733–737.
Nuclear Science Advisory Committee (2023). A new era of discovery: The 2023 long range plan
for nuclear science.
Oates, C. J., Cockayne, J., Briol, F.-X., and Girolami, M. (2019). Convergence rates for a class of
estimators based on stein’s method. _Bernoulli_, 25:1141–1159.
Overstall, A. M. and Woods, D. C. (2016). Multivariate emulation of computer simulators: model
selection and diagnostics with application to a humanitarian relief model. _Journal of the Royal_
_Statistical Society Series C: Applied Statistics_, 65(4):483–505.
Owen, A. B. (2016). Monte Carlo Theory, Methods and Examples. `[https://artowen.su.domains/](https://artowen.su.domains/mc/)`
`[mc/](https://artowen.su.domains/mc/)` .
Pandola, L., Bauer, M., Kr¨oninger, K., Liu, X., Tomei, C., Belogurov, S., Franco, D., Klimenko,
A., and Knapp, M. (2007). Monte Carlo evaluation of the muon-induced background in the
gerda double beta decay experiment. _Nucl. Instrum. Meth. A_, 570(1):149–158.
Paulson, J. A. and Tsay, C. (2025). Bayesian optimization as a flexible and efficient design
framework for sustainable process systems. _Current Opinion in Green and Sustainable Chemistry_,
51:100983.
Peng, C.-Y. and Wu, C. F. J. (2014). On the choice of nugget in kriging modeling for deterministic
computer experiments. _Journal of Computational and Graphical Statistics_, 23(1):151–168.
Ramachers, Y. and Morgan, B. (2020). warwick-legend. GitHub repository: `[https://github.](https://github.com/drbenmorgan/warwick-legend)`
`[com/drbenmorgan/warwick-legend](https://github.com/drbenmorgan/warwick-legend)` .
Rasmussen, C. E. and Williams, K. I. (2006). _Gaussian Processes for Machine Learning_ . MIT
Press.
Ritter, K. (2000). _Average-Case Analysis of Numerical Problems_ . Springer Science & Business
Media.


46


Roustant, O., Ginsbourger, D., and Deville, Y. (2012). DiceKriging, DiceOptim: Two R packages
for the analysis of computer experiments by kriging-based metamodeling and optimization.
_Journal of Statistical Software_, 51(1):1–55.
Rudin, C., Chen, C., Chen, Z., Huang, H., Semenova, L., and Zhong, C. (2022). Interpretable
machine learning: Fundamental principles and 10 grand challenges. _Statistic Surveys_, 16:1–85.
Sauer, A., Gramacy, R. B., and Higdon, D. (2023). Active learning for deep Gaussian process
surrogates. _Technometrics_, 65(1):4–18.
Schuetz, A.-K., Poon, A. W. P., and Li, A. (2025). RESuM: A rare event surrogate model for physics
detector design. In _Proceedings of the International Conference on Learning Representations_
_(ICLR)_ .
Stein, M. L. (2012). _Interpolation of Spatial Data: Some Theory for Kriging_ . Springer Science &
Business Media.
Stone, C. J. (1986). The dimensionality reduction principle for generalized additive models. _The_
_Annals of Statistics_, 14:590–606.
Surjanovic, S. and Bingham, D. (2013). Virtual Library of Simulation Experiments: Test Functions
and Datasets.
Taguchi, G. (1986). _Introduction to Quality Engineering: Designing Quality into Products and_
_Processes_ . American Supplier Institute.
Thomaser, A., Kononova, A. V., Vogt, M.-E., and B¨ack, T. (2022). One-shot optimization for
vehicle dynamics control systems: towards benchmarking and exploratory landscape analysis.
In _Proceedings of the Genetic and Evolutionary Computation Conference Companion_, pages
2036–2045.
Tikhomirov, V. (1991). On the representation of continuous functions of several variables as
superpositions of continuous functions of one variable and addition. In _Selected Works of AN_
_Kolmogorov_, pages 383–387. Springer.
Wang, W., Tuo, R., and Wu, C. F. J. (2020). On prediction properties of kriging: Uniform error
bounds and robustness. _Journal of the American Statistical Association_, 115(530):920–930.
Wendland, H. (2004). _Scattered Data Approximation_, volume 17. Cambridge University Press.
Wiesinger, C., Pandola, L., and Sch¨onert, S. (2018). Virtual depth by active background suppression:
revisiting the cosmic muon induced background of gerda phase II. _Eur. Phys. J. C_, 78(7).
Wu, C., Mao, S., and Ma, F. (1990). SEL: A search method based on orthogonal arrays. _Statistical_
_Design and Analysis of Industrial Experiments_, pages 279–310.
Wu, C. F. J. and Hamada, M. S. (2009). _Experiments: Planning, Analysis, and Optimization_ .
John Wiley & Sons.
Wu, Z.-M. and Schaback, R. (1993). Local error estimates for radial basis function interpolation
of scattered data. _IMA Journal of Numerical Analysis_, 13(1):13–27.
Wynne, G., Briol, F.-X., and Girolami, M. (2021). Convergence guarantees for gaussian process
means with misspecified likelihoods and smoothness. _Journal of Machine Learning Research_,
22(123):1–40.
Yeo, I.-K. and Johnson, R. A. (2000). A new family of power transformations to improve normality
or symmetry. _Biometrika_, 87(4):954–959.


47


