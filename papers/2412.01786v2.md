Published as a conference paper at ICLR 2025

## G RADIENT -F REE G ENERATION FOR H ARD -C ONSTRAINED S YSTEMS


**Chaoran Cheng** [1] **, Boran Han** _[∗]_ [2] **, Danielle C. Maddix** [2] **, Abdul Fatir Ansari** [2] **,**
**Andrew Stuart** [3] **, Michael W. Mahoney** [4] **, Yuyang Wang** [2]

1 University of Illinois Urbana-Champaign
2 Amazon Web Services
3 Stores Foundational AI, Amazon
4 Amazon SCOT

chaoran7@illinois.edu
_{_ boranhan,dmmaddix,ansarnd,andrxstu,zmahmich,yuyawang _}_ @amazon.com


A BSTRACT


Generative models that satisfy hard constraints are critical in many scientific and
engineering applications, where physical laws or system requirements must be
strictly respected. Many existing constrained generative models, especially those
developed for computer vision, rely heavily on gradient information, which is
often sparse or computationally expensive in some fields, e.g., partial differential equations (PDEs). In this work, we introduce a novel framework for adapting pre-trained, unconstrained flow-matching models to satisfy constraints exactly in a zero-shot manner without requiring expensive gradient computations
or fine-tuning. Our framework, _ECI sampling_, alternates between extrapolation
(E), correction (C), and interpolation (I) stages during each iterative sampling step
of flow matching sampling to ensure accurate integration of constraint information while preserving the validity of the generation. We demonstrate the effectiveness of our approach across various PDE systems, showing that ECI-guided
generation strictly adheres to physical constraints and accurately captures complex distribution shifts induced by these constraints. Empirical results demonstrate that our framework consistently outperforms baseline approaches in various zero-shot constrained generation tasks and also achieves competitive results
in the regression tasks without additional fine-tuning. Our code is available at
[https://github.com/amazon-science/ECI-sampling.](https://github.com/amazon-science/ECI-sampling)


1 I NTRODUCTION


Diffusion and flow matching models have achieved remarkable success in generative tasks of image
generation (Ho et al., 2020; Esser et al., 2024a), language modeling (Lou et al., 2023; Gat et al.,
2024), time series prediction (Lin et al., 2024; Kollovieh et al., 2024), and functional data modeling
(Lim et al., 2023; Kerrigan et al., 2023). Constrained generation built upon these generative models for solving various inverse problems has also been explored in the image domain (Kawar et al.,
2022; Ben-Hamu et al., 2024). These approaches predominantly rely on gradient information with
respect to some cost function as a soft constraint. While such soft-constrained methods have been
successful in the image domain, applications in other domains often require the generation to adhere
to certain constraints strictly. Such hard-constrained generation, i.e., generative modeling where
the natural hard constraints are _requirements_ and not just _suggestions_, is crucial for tasks in many
scientific domains. For example, in scientific computing, numerical simulations often require generated solutions to adhere to specific physical constraints (energy or mass conservation (Hansen et al.,
2023; Mouli et al., 2024)) and satisfy boundary condition constraints on the values or derivatives of
the solutions (Saad et al., 2023). An intuitive example of the generation of PDE solutions is demonstrated in Figure 1, where the solution set narrows and shifts when the constraint is imposed. While
existing approaches for constrained generation in image inverse problems can always be adapted


_∗_ Work done during internship at AWS.


1


Published as a conference paper at ICLR 2025


(Esser et al., 2024b; Ansari et al., 2024), hard-constrained generation for complex systems like PDE
solutions presents the following challenges:


Figure 1: Hard-constrained generation of PDE solutions with the boundary condition (BC) or initial
condition (IC) prescribed _a posteriori_ . _ω, k_ are PDE parameters that determine the IC and BC.


**Scarcity of constraint information.** In contrast to computer vision (CV) tasks, e.g., image inpainting, which typically assumes a considerable amount of context information, common constraints
like BC and IC in Figure 1 have measure zero with respect to the spatiotemporal domain. For example, a standard setting with a spatiotemporal resolution of 100 _×_ 100 leads to only 1% of known pixel
values as context. This is significantly less than the context provided in a typical CV application.


**Exact constraint satisfaction.** The exact conservation of mass, energy, or momentum is often essential in the simulation of PDEs (LeVeque, 1990) for ensuring physically feasible and consistent
solutions. This is in contrast with inverse problems in CV, e.g., super-resolution and de-blurring,
where constraints are often implicitly defined as modeling assumptions and evaluation metrics, e.g.,
peak signal-to-noise ratio (PSNR) do not directly depend on the exact satisfaction of these constraints. Existing CV approaches fail to guarantee exact satisfaction of constraints.


**Issues with gradient-based methods.** Existing zero-shot frameworks for constrained generation
predominantly rely on gradient guidance from a differentiable cost function on the final step of generation. This information can be prohibitively expensive, especially in large 3D PDE spatiotemporal
systems. Previous work has also indicated the drawbacks of these soft-constrained gradient-based
approaches. Well-known limitations include gradient imbalances in the loss terms (Wang et al.,
2020; 2021) and ill-conditioning (Krishnapriyan et al., 2021), which can lead to failure modes in
scientific machine learning (SciML) tasks.


To address these challenges for hard-constrained generation, we propose a general framework that
adopts a gradient-free and zero-shot approach for guiding unconstrained pre-trained flow matching
models. For the diversity of practical hard constraints for PDE systems, our proposed framework
provides a unified and efficient approach without the need for expensive fine-tuning or gradient
backpropagation. We term our framework _ECI sampling_ since it interleaves _extrapolation (E)_, _cor-_
_rection (C)_, and _interpolation (I)_ stages at each iterative sampling step. ECI sampling effectively and
accurately captures the distribution shift imposed by the constraints and maintains the consistency
of the generative prior. We summarize our main contributions as follows:


- **Unified gradient-free generation framework.** We introduce ECI sampling, a unified gradientfree sampling framework for guiding an unconstrained pre-trained flow matching model. By interleaving extrapolation, correction, and interpolation stages at each iterative sampling step, ECI
offers fine-grained iterative control over the flow sampling that can accurately capture the distribution shift imposed by the constraints and maintain the consistency of the system.


2


Published as a conference paper at ICLR 2025


- **Exact and efficient satisfaction of hard constraints.** ECI sampling addresses the unique challenges imposed by hard constraints. The memory- and time-efficient gradient-free approach guarantees the exact satisfaction of constraints and mitigates gradient issues known with existing
gradient-based methods in CV domains.


- **Zero-shot performance on generative and regression tasks.** Comprehensive generative metrics of distributional properties manifest the superior generative performance of our ECI sampling
compared with various existing zero-shot guidance methods on various PDE systems. We also
show that ECI sampling can be applied to zero-shot regression tasks, still achieving competitive performance with state-of-the-art Neural Operators (NOs) (Li et al., 2020a) that are directly
trained on the regression tasks.


2 R ELATED W ORK


**Diffusion and Flow Matching Models.** We first review existing generative models for functional
data that serve as the generative prior for the spatiotemporal solutions in our approach. Diffusion
models (Song & Ermon, 2020; Ho et al., 2020; Song et al., 2020) rely on a variational bound for the
log-likelihood and learn a reverse diffusion process to transform prior noise into meaningful samples. Lim et al. (2023) propose the diffusion denoising operator (DDO) to extend diffusion models
to function spaces. Flow matching models (Lipman et al., 2022; Liu et al., 2022) are a family of
continuous normalizing flows that learn a time-dependent vector field that defines the data dynamics
via a flow ordinary differential equations (ODEs). Kerrigan et al. (2023) proposes functional flow
matching (FFM) as an extension of existing flow matching models for image generation. Compared
to DDO, FFM has a more concise mathematical formulation and better empirical generation results.
We also note the close connection between diffusion and flow models in Lipman et al. (2022); Albergo et al. (2023). Therefore, while we focus on guiding flow-based models in this work, we also
compare our approach with a wide range of diffusion-based methods.


**Constrained Generation.** Flow-based generative modeling in function spaces has not been explored until recently, and constrained generation for PDE systems remains largely unexplored. The
neural process (NP) (Garnelo et al., 2018; Kim et al., 2019; Sitzmann et al., 2020) is one traditional
constrained generation approach that learns to map a context set of observed input-output pairs to
a distribution over regression functions. Hansen et al. (2023) introduce a generic model-agnostic
approach to control the variance of the generation and enjoy the exact satisfaction of constraints.
Lippe et al. (2024) use gradient guidance from the constraint to guide each sampling step of the
pre-trained diffusion model in a zero-shot fashion, similar to diffusion posterior sampling in Chung
et al. (2022). Huang et al. (2024) further incorporate the gradients from physics-informed losses
(Shu et al., 2023). Other approaches focus on controlling NOs in regression tasks (N´egiar et al.,
2023; Saad et al., 2023; Li et al., 2024; Mouli et al., 2024). As the sampling for flow or diffusion models is an iterative procedure, these methods cannot be directly adopted to provide iterative
control. They can be, however, incorporated in our correction stage (see Section 3.2).


**Inverse Problems.** Zero-shot generation for pre-trained diffusion and flow models has been explored in the image domain for solving various inverse problems, including image inpainting, deblurring, and superresolution (Bai et al., 2020). Existing approaches predominantly rely on gradient
guidance. Liu et al. (2023b); Ben-Hamu et al. (2024); Wang et al. (2024) propose to modify the
prior noise or vector field by differentiating through the ODE solver, thus being extremely timeand memory-consuming. Pan et al. (2023a;b) use the adjoint sensitivity method to mitigate the high
memory assumption. Other works propose gradient-free control, usually with strong prior assumptions on the constraints to derive control at each sampling step. Lugmayr et al. (2022) propose to
mix forward diffusion steps from the context with the model’s prediction, and Kawar et al. (2022)
use a similar variational approach to solve linear inverse problems.


Although these existing methods have achieved decent generation results for inverse problems in
image generation, the approximate enforcement of physical laws (soft-constrained) (N´egiar et al.,
2023) failed to address the challenges imposed by hard constraints (Wang et al., 2020; 2021; Krishnapriyan et al., 2021). Our proposed zero-shot guidance approach focuses on the challenges of exact
constraint satisfaction, using fine-grain iterative control at each sampling step for more consistent
generations. The gradient-free approach is also more efficient and provides a unified framework for
various linear and non-linear constraints. We summarize the major differences between the existing
methods and our proposed method in Table 1.


3


Published as a conference paper at ICLR 2025


Table 1: Comparison between existing constrained generation methods and our ECI sampling.


Zero-shot Gradient-free Exact constraint Iterative control


Conditional FFM [18] ✗ ✓ ✗ ✓
ANP [19] ✓ ✓ ✗ ✗
ProbConserv [13] ✓ ✓ ✓ ✗
DiffusionPDE [16] ✓ ✗ ✗ ✓
D-Flow [4] ✓ ✗ ✗ ✓
ECI (ours) ✓ ✓ ✓ ✓


3 M ETHOD


3.1 P RELIMINARY


**Problem Definition.** Consider a PDE system _F_ _ϕ_ _u_ ( _x_ ) = 0 _, x ∈X ⊆_ R _[D]_ with PDE parameters
_ϕ ∈_ Φ. For a fixed PDE family _F_, let _U_ _F_ = _{u_ ( _x_ ) : _∃ϕ ∈_ Φ _, F_ _ϕ_ _u_ ( _x_ ) = 0 _, x ∈X}_ denote the
set of all the plausible PDE solutions by varying the PDE parameters. Consider some constraint
operator _Gu_ ( _x_ ) = 0 _, x ∈X_ _G_ _⊆X_ defined on a subset of the PDE domain, and let _U_ _G_ = _{u_ ( _x_ ) :
_Gu_ ( _x_ ) = 0 _, x ∈X_ _G_ _}_ denote the solution set for the constraint. We are interested in finding a subset
of solutions _U_ _F|G_ := _U_ _F_ _∩U_ _G_ _⊆U_ _F_ in which both the original PDE and the constraint are satisfied.
We assume that the prior solution set _U_ _F_ can be captured by a generative model, e.g., an FFM pretrained on a solution set. We aim to guide the pre-trained model to satisfy the constraint operator
in a zero-shot fashion towards the narrowed solution set _U_ _F|G_ . A more mathematically rigorous
definition using measure theoretic terms is provided in Appendix A.1.


**Prior Generative Model.** FFM (Kerrigan et al., 2023) extends the flow matching framework (Lipman et al., 2022) to model measures over the Hilbert space of continuous functions. Given a set _U_ of
well-behaved (see Appendix C.2) functions _u_ : _X →_ R, FFM learns a time-dependent vector field
operator _v_ _t_ : _U ×_ [0 _,_ 1] _→U_ that defines a time-dependent diffeomorphism _ψ_ _t_ : _U ×_ [0 _,_ 1] _→U_
called the _flow_ via the _flow ODE_ :

_∂_ _t_ _ψ_ _t_ ( _u_ ) = _v_ _t_ ( _ψ_ _t_ ( _u_ )) _,_ _ψ_ 0 ( _u_ ) = _u_ 0 _._ (1)



The flow _ψ_ _t_ induces a pushforward measure ˆ _µ_ _t_ := ( _ψ_ _t_ ) _∗_ _µ_ 0, where _µ_ 0 is the prior **Algorithm 1** Sampling from FFM (Euler Method)
noise measure from which _u_ 0 can be sam- 1: **Input:** Learned vector field _v_ _θ_, Euler steps _N_ .
pled. FFM showed that a tractable flow 2: Sample noise function _u_ 0 _∼_ _µ_ 0 ( _u_ ).
matching objective can be derived when 3: **for** _t ←_ 0 _,_ 1 _/N,_ 2 _/N, . . .,_ ( _N −_ 1) _/N_ **do**
the flow is conditioned on the target func- 4: _u_ _t_ +1 _/N_ _←_ _u_ _t_ + _v_ _θ_ ( _u_ _t_ _, t_ ) _/N_
tion _u_ 1 sampled from the target measure

5: **return** _u_ 1

_µ_ 1 . As we are interested in guiding pretrained generative models, we will assume
that FFM can well approximate the target measure ˆ _µ_ 1 _≈_ _µ_ 1 over _U_ _F_ . We want to guide the pretrained FFM towards the conditional measure _µ_ _F|G_ over _U_ _F|G_ . The sampling procedure of the FFM,
similar to all diffusion and flow models, is an iterative process in which the initial noise function is
iteratively refined into target functions via the learned vector field with the dynamics described in
Equation 1. Algorithm 1 describes sampling from the FFM method using the Euler method. In practice, the vector field can be parameterized by some discretization-invariant NOs (Lu et al., 2019; Li
et al., 2020b;a) such that the whole generative framework is discretization-invariant. This indicates
that FFM can be naturally adapted for zero-shot superresolution (Kerrigan et al., 2023).



**Algorithm 1** Sampling from FFM (Euler Method)



1: **Input:** Learned vector field _v_ _θ_, Euler steps _N_ .
2: Sample noise function _u_ 0 _∼_ _µ_ 0 ( _u_ ).
3: **for** _t ←_ 0 _,_ 1 _/N,_ 2 _/N, . . .,_ ( _N −_ 1) _/N_ **do**
4: _u_ _t_ +1 _/N_ _←_ _u_ _t_ + _v_ _θ_ ( _u_ _t_ _, t_ ) _/N_



5: **return** _u_ 1



3.2 H ARD C ONSTRAINT I NDUCED G UIDANCE


Although various regression NOs have been proposed to satisfy specific constraints in their prediction (Liu et al., 2023a; N´egiar et al., 2023; Saad et al., 2023; Liu et al., 2024; Mouli et al., 2024), the
difficulty in applying these existing methods to flow matching models lies in the iterative nature of
the flow sampling in Algorithm 1. For flow matching sampling, intermediate generations are noised
data instead of final predictions, making the correction algorithm inapplicable as the constraint can
only be applied to the last generation step.


4


Published as a conference paper at ICLR 2025


Following previous zero-shot guidance frameworks (Lugmayr et al., 2022; Kawar et al., 2022), at
each iterative sampling timestep _t_, we model the constrained generation process with the conditional
generation probability _p_ (ˆ _u_ _t_ _|G_ ) = _p_ (ˆ _u_ _t_ _|u_ _t_ _, G_ ) _p_ ( _u_ _t_ ), where the unconditional probability _p_ ( _u_ _t_ ) can
be modeled by a pre-trained generative model. We assume a correction algorithm _C_ ( _u_ 1 _, G_ ) is readily available for _final predictions_ that uses an orthogonal/oblique projection (Hansen et al., 2023)
to ensure hard constraint satisfaction (see Appendix A.3). To offer iterative control over the sampling process for hard constraints, we note that if the final prediction can be _extrapolated_ from
intermediate noised generations, the correction can be safely applied. Furthermore, if intermediate
noised generations can be _interpolated_ back from the corrected prediction, we can “propagate” the
constraint information back to each time step. Indeed, let _p_ _θ_ ( _u_ 1 _|u_ _t_ ) denote the probability of extrapolating the final prediction of _u_ 1, given the current noise data _u_ _t_ and the learnable parameter _θ_ ;
and let _q_ (ˆ _u_ _t_ _|u_ ˆ 1 ) denote the probability of interpolating the intermediate noised data ˆ _u_ _t_, given the
corrected data ˆ _u_ 1 = _C_ ( _u_ 1 _, G_ ); then the constraint-conditional probability can be decomposed in a
variational way by marginalizing over the auxiliary variable _u_ 1 :
_p_ (ˆ _u_ _t_ _|u_ _t_ _, G_ ) = E _u_ 1 _∼p_ _θ_ ( _u_ 1 _|u_ _t_ ) [ _q_ (ˆ _u_ _t_ _|C_ ( _u_ 1 _, G_ ))] _._ (2)


We call such a constraint-guided step in each flow
sampling step an **ECI Step** (for _extrapolation-_ **Algorithm 2** ECI Step
_correction-interpolation_ ). With the determinis- 1: **Input:** Learned vector field _v_ _θ_, constraint
tic flow ODE formulation, good properties can be _G_, current noise data _u_ _t_, timestep _t_ _[′]_ _≥_ _t_ .
naturally deduced with the flow-matching frame- 2: _u_ 1 _←_ _u_ _t_ + (1 _−_ _t_ ) _v_ _θ_ _▷_ Extrapolation
work. The _extrapolation probability p_ _θ_ ( _u_ 1 _|u_ _t_ ) 3: ˆ _u_ 1 _←_ _C_ ( _u_ 1 _, G_ ) _▷_ Correction


ˆ

can be learned by the pre-trained unconditional 4: _u_ _t_ _′_ _←_ (1 _−_ _t_ _[′]_ ) _u_ 0 + _t_ _[′]_ _u_ 1 _▷_ Interpolation
model by doing a deterministic one-step extrap- 5: **return** _u_ _t_ _′_
olation of the current predicted vector field as
_u_ 1 = _u_ _t_ + (1 _−_ _t_ ) _v_ _θ_ ( _u_ _t_ ). The _interpolation_
_probability q_ (ˆ _u_ _t_ _|u_ ˆ 1 ) is also well-defined in the FFM along the _OT-path_ of measures (Kerrigan
et al., 2023), where the ground truth data are linearly interpolated with random noises as ˆ _u_ _t_ =

ˆ
(1 _−_ _t_ ) _u_ 0 + _tu_ 1 _, u_ 0 _∼_ _µ_ 0 ( _u_ ). In this way, the expectation can be discarded, and the conditional
probability can be further simplified as
_p_ (ˆ _u_ _t_ _|u_ _t_ _, G_ ) = _q_ (ˆ _u_ _t_ _|C_ ( _u_ _t_ + (1 _−_ _t_ ) _v_ _θ_ ( _u_ _t_ ) _, G_ )) _._ (3)



**Algorithm 2** ECI Step



1: **Input:** Learned vector field _v_ _θ_, constraint
_G_, current noise data _u_ _t_, timestep _t_ _[′]_ _≥_ _t_ .
2: _u_ 1 _←_ _u_ _t_ + (1 _−_ _t_ ) _v_ _θ_ _▷_ Extrapolation
3: ˆ _u_ 1 _←_ _C_ ( _u_ 1 _, G_ ) _▷_ Correction

ˆ
4: _u_ _t_ _′_ _←_ (1 _−_ _t_ _[′]_ ) _u_ 0 + _t_ _[′]_ _u_ 1 _▷_ Interpolation
5: **return** _u_ _t_ _′_



We summarize the ECI step in Algorithm 2, in which we also allow **Algorithm 3** ECI Sampling (Euler Method)
for _t_ _[′]_ _≥_ _t_ to advance the solver. 1: **Input:** Learned vector field _v_ _θ_, Euler steps _N_, mixing
Furthermore, we note that our ECI iterations _M_, constraint _G_ .
steps can be applied recursively to 2: Sample noise function _u_ 0 _∼_ _µ_ 0 ( _u_ ).
the same timestep in a manner sim- 3: **for** _t ←_ 0 _,_ 1 _/N,_ 2 _/N, . . .,_ ( _N −_ 1) _/N_ **do**
ilar to previous work that applied
multiple rounds of guidance (Lug- 4: _u_ [(0)] _t_ _←_ _u_ _t_
mayr et al., 2022; Ben-Hamu et al., 5: **for** _m ←_ 0 _,_ 1 _, . . ., M −_ 1 **do**

6: **if** _m < M −_ 1 **then**

2024). With the constraint information “backpropagating” into the noise 7: _u_ [(] _t_ _[m]_ [+1)] _←_ ECIStep( _v_ _θ_ _, G, u_ [(] _t_ _[m]_ [)] _, t_ )
data via one ECI step, a recursive ap- 8: **else** _▷_ Advance the flow ODE solver
plication of such steps can promote 9: _u_ _t_ +1 _/N_ _←_ ECIStep( _v_ _θ_ _, G, u_ [(] _t_ _[m]_ [)] _, t_ + 1 _/N_ )
further information mixing between 10: **return** _u_ 1
the constrained and unconstrained regions, leading to a more consistent generation. At the last mixing step, we instead interpolate the
new time step _t_ _[′]_ _> t_ with sample _u_ _t_ _′_ to advance the ODE solver. The number of total mixing
steps _M_ is a controllable hyperparameter. Algorithm 3 describes the complete sampling process for
flow matching models using iterative ECI steps, for which we term **ECI sampling** . ECI guarantees
exact satisfaction of the constraint and facilitates information mixing between the constrained and
unconstrained regions to produce a more consistent generation. See Appendix A for details on the
sampling and proof of exact satisfaction.



**Algorithm 3** ECI Sampling (Euler Method)



1: **Input:** Learned vector field _v_ _θ_, Euler steps _N_, mixing
iterations _M_, constraint _G_ .
2: Sample noise function _u_ 0 _∼_ _µ_ 0 ( _u_ ).
3: **for** _t ←_ 0 _,_ 1 _/N,_ 2 _/N, . . .,_ ( _N −_ 1) _/N_ **do**

4: _u_ [(0)] _t_ _←_ _u_ _t_
5: **for** _m ←_ 0 _,_ 1 _, . . ., M −_ 1 **do**
6: **if** _m < M −_ 1 **then**
7: _u_ [(] _t_ _[m]_ [+1)] _←_ ECIStep( _v_ _θ_ _, G, u_ [(] _t_ _[m]_ [)] _, t_ )
8: **else** _▷_ Advance the flow ODE solver
9: _u_ _t_ +1 _/N_ _←_ ECIStep( _v_ _θ_ _, G, u_ [(] _t_ _[m]_ [)] _, t_ + 1 _/N_ )



10: **return** _u_ 1



3.3 C ONTROL OVER S TOCHASTICITY


ECI sampling provides a unified zero-shot and gradient-free guidance framework for various constraints. Nonetheless, the varieties in the constraint types may impose challenges. For example, if


5


Published as a conference paper at ICLR 2025


the constraint contains little information, the variance is expected to match the unconstrained case.
On the other hand, with enough constraints to result in a well-posed PDE with a unique solution, the
generation variance should be as small as possible (see Section 3.4).


Inspired by the work of the stochastic interpolant that unified flow matching and diffusion (Albergo
et al., 2023), we propose to control the generation stochasticity with different sampling strategies
of _u_ 0 during our ECI sampling. For flow-based models, stochasticity only appears in the sampling
process of _u_ 0 _∼_ _µ_ 0 ( _u_ ) for interpolating the new ˆ _u_ _t_ . In the previous context, we assume the initially
sampled _u_ 0 at _t_ = 0 is used throughout the sampling process. This makes the trajectory straighter
as the interpolation always starts with the same point. An alternative approach is to sample new
noises on each step, effectively marginalizing over the prior noise measure _µ_ 0 and resulting in a
more concentrated generation. We use a hyperparameter _R_ to denote the length of the interval
for re-sampling the noise function _u_ 0 for interpolation. Intuitively, a smaller _R_ leads to a more
concentrated marginalized generation, whereas a larger _R_ leads to larger generational diversity. We
quantitatively demonstrate the effect of this hyperparameter in Section 4.3 and also propose heuristic
rules for choosing it.


3.4 G ENERATIVE FFM AS A R EGRESSION M ODEL


We illustrate the potential of our ECI sampling framework for various regression tasks. Specifically,
the solution set _U_ _F|G_ defined in Section 3.1 will degenerate to a unique solution given enough
constraints _G_, leading to a well-posed PDE system. In this way, the generative task degenerates into
a deterministic regression learning task, with the target distribution now shifted to the Dirac measure
over the solution _δ_ _u_ sol . Although our constrained generative framework is not designed for these
regression tasks and has intrinsic stochasticity, in our empirical study, we show that our framework
is able to generate competitive predictions to standard regression models, e.g., the Fourier Neural
Operator (FNO) (Li et al., 2020a).


4 E MPIRICAL EVALUATION


In this section, we present an extensive evaluation of our ECI sampling and other existing zero-shot
guidance models for both generative and regression tasks. Covering a wide range of diverse PDE
systems, Table 2 summarizes the dataset specifications used in our evaluation. See Appendix B
for more details regarding the PDE systems and the data generation process. For most zero-shot
guidance methods, we first pre-train an unconditional flow matching model as the common prior
generative model (See Appendix C.3). We test our proposed method using more than one mixing
iteration in most cases. See Appendix D for the detailed experimental setup and baseline adaptations
for flow sampling, and Appendix E for additional visualizations and results.


Table 2: Dataset specifications (IC / BC / CL for initial condition / boundary condition / conservation
law). For different resolutions, we test models with the zero-shot superresolution setting.

Type Dataset Underspecification Spatial Resolution Temporal Resolution Constraint



Generative


Regression



Stokes Problem IC & BC 100 100 IC / BC
Heat Equation IC & diffusion 100 100 IC
Darcy Flow BC & field 101 × 101 N/A BC
NS Equation IC & forcing 64 × 64 50 IC


Heat Equation diffusion 100 / 200 100 / 200 CL
PME diffusion 100 / 200 100 / 200 CL
Stefan Problem shock range 100 / 200 100 / 200 CL
Stokes Problem IC & BC 100 100 IC & BC
NS Forward IC & forcing 64 × 64 50 first 15 frames



4.1 G ENERATIVE T ASK


In generative tasks, the solution set is non-degenerate with an infinite cardinality after imposing
the constraint. We expect the generation distribution to match the ground truth shifted distribution.
The prior FFM is pre-trained on a larger solution set, whereas during the zero-shot constrained


6


Published as a conference paper at ICLR 2025


generation, we impose a specific PDE constraint to narrow down the solution set. Additionally, we
use a conditionally-trained FFM (CondFFM) that takes known constraints as conditions during both
training and sampling as an upper bound for the zero-shot guidance models. We conduct extensive
experiments on various 2D and 3D PDE datasets with different types of constraints.


Figure 2: Generation mean and standard deviation errors for the Stokes problem with IC fixed. Note
that grey indicates _≈_ 0 error, red for positive error, and blue for negative error. Noticeable IC errors
(left column) can be observed for the gradient-based methods.


To compare two distributions over continuous functions, we follow Kerrigan et al. (2023) to calculate
the pointwise mean and standard deviation (std) of the ground truth solutions and the solutions
generated from various sampling methods. Mean squared errors (MSEs) of the mean ( **MMSE** ) and
of the std ( **SMSE** ) are then calculated to assess the resemblance in these statistics. To evaluate
the satisfaction of the constraint, the constraint error ( **CE** ) is defined as the MSE for the constraint
operator CE( _u_ ) = MSE( _G_ ( _u_ ) _,_ 0) on the constrained region _X_ _G_ . Inspired by the generative metric
of Fr´echet inception distance (FID) in image and PDE generation domains (Lim et al., 2023), we
calculate the Fr´echet distance between the hidden representations extracted by the pre-trained PDE
foundation model Poseidon (Herde et al., 2024). We call this metric Fr´echet Poseidon distance
( **FPD** ). A lower FPD indicates a closer match to the ground truth distribution regarding the highlevel representations. To alleviate the impact of randomness in sampling, we sample 512 solutions
for the 2D datasets and 100 solutions for the 3D datasets for all methods to calculate the metrics.


We carry out experiments on 1D **Stokes problem**, 1D **heat equation**, 2D **Darcy flow**, and 2D
**Navier-Stokes (NS) equation** . The detailed description of these PDE systems, as well as the set
of PDE parameters for pre-training and constrained generation, are provided in Appendix B. Note
that we also ensure at least two degrees of freedom (underspecification) in generative tasks, as
demonstrated in Table 2. This ensures that distribution over the constrained generations does not
collapse to a Dirac distribution as in a regression task (see Figure 1).


Figure 3: Generation mean and standard deviation errors of the trajectories for the NS equation with
IC fixed. 25 downsampled time frames are plotted for each method.


Table 3 summarizes the results for the generative tasks with the best zero-shot method performance
highlighted in bold. Figure 2 shows the visualization of pointwise errors for the generation statistics
for the Stokes problem with IC fixed together with the ground truth reference. Figure 3 shows the
visualization for the NS equation with 25 downsampled time frames. Our proposed ECI sampling
achieves state-of-the-art performance on most metrics and also enjoys zero constraint errors. Notice

7


Published as a conference paper at ICLR 2025


Table 3: Generative metrics on various constrained PDEs. The best results for zero-shot methods
(CondFFM is not zero-shot) are highlighted in bold. The unconstrained pre-trained FFM is provided.






|Dataset Metric ECI ANP ProbConserv DiffusionPDE D-Flow|CondFFM FFM|
|---|---|
|Stokes IC<br>MMSE / 10_−_2<br>**0.090**<br>13.750<br>2.014<br>0.601<br>1.015<br>SMSE / 10_−_2<br>**0.127**<br>9.183<br>1.528<br>0.330<br>0.219<br>CE / 10_−_2<br>**0**<br>3.711<br>**0**<br>0.721<br>1.021<br>FPD<br>**0.076**<br>26.685<br>13.075<br>0.202<br>0.614|0.121<br>2.068<br>0.016<br>1.572<br>0.017<br>9.792<br>0.028<br>13.342|
|Stokes BC<br>MMSE / 10_−_2<br>**0.005**<br>3.003<br>2.938<br>0.603<br>4.800<br>SMSE / 10_−_2<br>**0.003**<br>3.055<br>2.302<br>0.341<br>_>_100<br>CE / 10_−_2<br>**0**<br>29.873<br>**0**<br>0.007<br>53.551<br>FPD<br>**0.010**<br>5.339<br>2.780<br>1.162<br>27.680|0.032<br>3.692<br>0.047<br>3.320<br>0.012<br>_>_100<br>0.048<br>2.824|
|Heat Equation<br>MMSE / 10_−_2<br>1.605<br>5.127<br>3.744<br>**0.979**<br>1.174<br>SMSE / 10_−_2<br>**0.157**<br>0.740<br>4.453<br>1.291<br>9.928<br>CE / 10_−_2<br>**0**<br>0.183<br>**0**<br>11.646<br>94.551<br>FPD<br>1.365<br>2.432<br>2.639<br>**0.951**<br>3.831|0.008<br>3.954<br>0.004<br>4.737<br>0.002<br>49.296<br>0.006<br>2.799|
|Darcy Flow<br>MMSE / 10_−_2<br>2.314<br>14.647<br>56.608<br>10.442<br>**1.728**<br>SMSE / 10_−_2<br>**0.592**<br>1.269<br>65.967<br>1.097<br>6.343<br>CE / 10_−_2<br>**0**<br>3.152<br>**0**<br>0.656<br>14.553<br>FPD<br>**0.946**<br>5.805<br>_>_100<br>8.027<br>10.043|0.043<br>58.879<br>0.037<br>69.432<br>0.071<br>_>_104<br>0.021<br>_>_100|
|NS Equation<br>MMSE / 10_−_2<br>7.961<br>59.020<br>18.635<br>19.141<br>**2.846**<br>SMSE / 10_−_2<br>**5.846**<br>14.990<br>8.306<br>5.941<br>6.840<br>CE / 10_−_2<br>**0**<br>5.527<br>**0**<br>2.295<br>0.776<br>FPD<br>**1.131**<br>57.263<br>2.171<br>5.750<br>1.651|0.851<br>18.749<br>0.554<br>8.423<br>0.079<br>11.451<br>0.232<br>2.185|


ably, ECI sampling excels in capturing the higher-order distributional properties of the shifted conditional measure, e.g., the standard deviation and FPD, and even surpasses the conditional FFM in
several simple 2D cases. Specifically, we observe noticeable artifacts around the IC for the gradientbased methods, i.e., DiffusionPDE and D-Flow, as they do not guarantee the exact satisfaction of
the constraint. For ProbConserv (Hansen et al., 2023), although the constraint is satisfied exactly,
the naive non-iterative correction approach leaves a sudden change around the boundary that violates the physical feasibility. In addition, Table 4 shows that our gradient-free approach enjoys
sampling efficiency in both 2D and 3D cases compared with gradient-based methods. Noticeably,
ECI-1 achieved _×_ 440 acceleration and _×_ 310 memory saving compared to gradient-based D-Flow
on the Stokes problem. More visualizations for the other PDE systems and the different generation
statistics are provided in Appendix E.1.


Table 4: Generation setup (batch size vs Euler step), time per sample in seconds, and GPU memory
for 2D (Stokes IC) and 3D (NS equation) PDE systems. A fraction sample size is available for ANP.


Dataset Resource ECI-1 ECI-5 CondFFM ANP ProbConserv DiffusionPDE D-Flow


#sample/#Euler 128/200 128/200 128/200 32/NA 128/200 128/200 2/200
Stokes IC Time/sample/s 0.065 0.325 0.057 0.009 0.058 0.131 28.774
GPU Memory/GB 5.4 5.4 5.4 7.4 5.4 10.8 26.4


#sample/#Euler 25/100 25/100 25/100 0.2/NA 25/100 25/100 1/20
NS equation Time/sample/s 0.415 2.067 0.669 2.324 0.675 0.676 8.456
GPU Memory/GB 16.3 16.3 16.3 11.0 16.3 27.0 27.1


4.2 R EGRESSION T ASK


We additionally experiment with regression scenarios (see Section 3.4), where enough constraints
reduce the generative task into the standard regression setting in neural operator learning. As the
ground truth targets are available in this case, we calculate the MSE and other related evaluation
metrics and compare them with traditional NOs.


**Uncertainty Quantification.** In the uncertainty quantification task, random context points from
the true solution are sampled together with the conservation laws to pin down the unique solution.
Following Hansen et al. (2023), we experiment with our ECI sampling on the Generalized Porous
Medium Equation (GPME) family of equations using identical PDE parameter choices. We evaluate


8


Published as a conference paper at ICLR 2025


Table 5: Uncertainty quantification metrics on constrained PDEs from the Generalized Porous
Medium Equation (GPME) family of equations. All baseline results are taken from Hansen et al.
(2023). The best results are highlighted in bold.


Dataset Metric ECI ANP SoftC-ANP HardC-ANP ProbConserv


CE / 10 _[−]_ [3] **0** 4.68 3.47 **0** **0**
Heat Equation LL 1.90 2.72 2.40 **3.08** 2.74
MSE / 10 _[−]_ [4] **0.81** 1.71 2.24 1.37 1.55


CE / 10 _[−]_ [3] **0** 6.67 5.62 **0** **0**

PME LL 2.19 3.49 3.11 3.16 **3.56**
MSE / 10 _[−]_ [4] 0.19 0.94 1.11 0.43 **0.17**


CE / 10 _[−]_ [2] **0** 1.30 1.72 **0** **0**

Stefan Problem LL 3.30 3.53 **3.57** 2.33 3.56
MSE / 10 _[−]_ [3] **1.89** 5.38 6.81 5.18 **1.89**


Table 6: Neural operator learning metrics on two constrained PDEs. We abbreviate OOM for out
|of-memory. The results for the best generative models are highlighted in bold.|Col2|
|---|---|
|Dataset<br>Metric<br>ECI<br>ANP<br>ProbConserv<br>DiffusionPDE<br>D-Flow|FNO|
|Stokes Problem<br>MMSE / 10_−_3<br>**0.050**<br>16.470<br>103.136<br>5.839<br>89.514<br>SMSE / 10_−_2<br>**0.028**<br>0.033<br>7.509<br>0.473<br>136.936|0.033<br>0|
|NS Equation<br>MMSE / 10_−_2<br>**0.069**<br>OOM<br>46.673<br>19.015<br>1.310<br>SMSE / 10_−_2<br>**0.002**<br>OOM<br>28.789<br>13.914<br>4.068|0.380<br>0|



the generation results on three instances of the GPME, i.e., the heat equation, porous medium equation (PME), and Stefan problem, where the additional constraints are various physical conservation
laws. We also follow the paper to report the pointwise Gaussian log-likelihood (LL) based on the
sample mean and variance. The results in Table 5 show that our ECI sampling is able to achieve
comparable results with ProbConserv (Hansen et al., 2023), which is specifically tailored for these
uncertainty quantification tasks. We also notice that, though being a generative model with intrinsic
stochasticity from the prior noise distribution, our method is able to produce quite confident predictions with little variance. It surpasses the other baselines in terms of MSE but at the cost of worse
LL. See Appendix E.3 for visualizations and more discussions.


**Neural Operator Learning.** We further experiment with the zero-shot neural operator learning
task on the Stokes problem and Navier-Stokes equation. For the Stokes problem, both BC and
IC are prescribed to give a unique solution. For the NS equation, we follow Li et al. (2020a) to
consider the regression task from the first 15 frames to the other 35 frames. In addition, a standard
FNO (Li et al., 2020a) was trained as the regression baseline. Table 6 summarizes the results. The
large amount of context pixels in the NS equation makes it infeasible for the ANP model. Our
ECI sampling significantly outperforms gradient-based methods and reaches comparable or even
better MSEs than the NOs directly trained on these regression tasks. Similar to the uncertainty
quantification task, we observe that our ECI sampling is quite confident about its prediction with
little variance. We also provide a comparison of ECI sampling with different numbers of frames
fixed as the constraint in Appendix E.4 to demonstrate how the variance gradually reduces with
more constraint information prescribed.


4.3 A BLATION S TUDY


Similar to most existing controlled generation methods for diffusion or flow models, the number of
mixing iterations is a tunable hyperparameter for our ECI sampling to control the extent of information mixing. Intuitively, an insufficient number of mixing iterations may fail to mix the information
between the constrained and the unconstrained regions, leading to more artifacts. On the other hand,
an excessively large number of mixing iterations may discard the prior PDE structure learned in the
pre-trained generative model, leading to inconsistency with the PDE system. We further test with


9


Published as a conference paper at ICLR 2025


the length of noise re-sampling interval _R_, as discussed in Section 3.3, to control stochasticity for
different constrained generation tasks.


We test the combination of mixing iterations _M ∈{_ 1 _,_ 2 _,_ 5 _,_ 10 _,_ 20 _}_, with a re-sampling interval
_R ∈{_ 1 _,_ 2 _,_ 5 _,_ 10 _}_, and no re-sampling settings on the Stokes problem with the IC or BC prescribed.
Figure 4 illustrates the results of different ECI sampling settings together with other baselines. For
the IC task, _R_ = None performs the best, whereas for the BC task, _R_ = None performs worse, with
_R_ = 5 and _R_ = 2 achieving lower MSEs. In both cases, the MSEs do not decrease monotonically
with increasing mixing iterations. Therefore, the mixing iteration impacts the final performance in
a task-specific manner. We hypothesize that the difference in the shifted variance may cause this
difference in the performance after imposing a specific constraint. For the Stokes problem, the prior
ground truth variance is higher for the BC than for the IC. Prescribing BC values provides more
information and reduces the uncertainty, making it easier for the model to be guided. In contrast,
prescribing IC values provides less information, which results in a larger variance in the shifted
distribution and makes it harder to guide the model.


Inspired by the aforementioned observations, we propose the following heuristic rules for choosing
the appropriate number of mixing iterations _M_ and re-sample interval _R_ : 1) We usually limit _M_
between 1-10, as a larger number of iterations is both computation-intensive and leads to worse
performance; and 2) For easier tasks with lower variance, we choose a smaller _M_ and smaller _R_ ;
and for harder tasks with higher variance, we choose a larger _M_ and larger _R_ or no re-sampling.
Indeed, we use 1 mixing iteration for the relatively easy tasks on the heat equation and Darcy flow
and use 10 mixing iterations for the harder NS equation.


Figure 4: MMSE and SMSE of ECI sampling with mixing iterations _M_ in different colors and resampling intervals _R_ in different markers in the Stokes problem with IC (left) or BC (right) fixed.
“None” for no re-sampling (the initial noise is used).


5 C ONCLUSION


In this work, we present ECI sampling as a unified zero-shot and gradient-free framework for guiding pre-trained flow-matching models towards hard constraints. Instantiated on PDE systems, our
framework obtains superior sampling quality and efficient sampling time compared to existing zeroshot methods, and it also enjoys the additional benefit of a zero-shot regression model with comparable performance to the traditional regression models. We also highlight the potential of ECI
sampling on other domains outside of SciML, including supply chain and time series, which we
leave as future work. We further note a limitation of our framework: the number of mixing iterations and the re-sampling interval have a task-specific impact on the final performance. While we
have provided empirical evaluations of these hyperparameters and suggested heuristic guidelines for
selecting them, theoretical analyses would be beneficial.


10


Published as a conference paper at ICLR 2025


R EFERENCES


Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying
framework for flows and diffusions. _arXiv preprint arXiv:2303.08797_, 2023. 3, 6


Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen,
Oleksandr Shchur, Syama Syndar Rangapuram, Sebastian Pineda Arango, Shubham Kapoor,
Jasper Zschiegner, Danielle C. Maddix, Hao Wang, Michael W. Mahoney, Kari Torkkola, Andrew Gordon Wilson, Michael Bohlke-Schneider, and Yuyang Wang. Chronos: Learning the
language of time series. _arXiv preprint arXiv:2403.07815_, 2024. 2


Yanna Bai, Wei Chen, Jie Chen, and Weisi Guo. Deep learning methods for solving linear inverse
problems: Research directions and paradigms. _Signal Processing_, 177:107729, 2020. 3


Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow:
Differentiating through flows for controlled generation. _arXiv preprint arXiv:2402.14017_, 2024.
1, 3, 4, 5, 23


Ricky TQ Chen and Yaron Lipman. Riemannian flow matching on general geometries. _arXiv_
_preprint arXiv:2302.03660_, 2023. 19


Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. _Advances in neural information processing systems_, 31, 2018. 19


Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion
posterior sampling for general noisy inverse problems. _arXiv preprint arXiv:2209.14687_, 2022.
3, 23


J.R. Dormand and P.J. Prince. A family of embedded runge-kutta formulae. _Journal of Computa-_
_tional and Applied Mathematics_, 6(1):19–26, 1980. ISSN 0377-0427. 21


Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for
high-resolution image synthesis. In _Forty-first International Conference on Machine Learning_,
2024a. 1


Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M¨uller, Harry Saini, Yam
Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Goodwin, Yannik Marek, and Robin Rombach. Scaling rectified flow
[transformers for high-resolution image synthesis, 2024b. URL https://arxiv.org/abs/](https://arxiv.org/abs/2403.03206)
[2403.03206. 2](https://arxiv.org/abs/2403.03206)


Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. _arXiv preprint arXiv:1807.01622_, 2018. 3


Itai Gat, Tal Remez, Neta Shaul, Felix Kreuk, Ricky TQ Chen, Gabriel Synnaeve, Yossi Adi, and
Yaron Lipman. Discrete flow matching. _arXiv preprint arXiv:2407.15595_, 2024. 1


Derek Hansen, Danielle C Maddix, Shima Alizadeh, Gaurav Gupta, and Michael W Mahoney.
Learning physical models that can respect conservation laws. In _International Conference on_
_Machine Learning_, pp. 12469–12510. PMLR, 2023. 1, 3, 4, 5, 8, 9, 15, 17, 18, 19, 22, 27


Maximilian Herde, Bogdan Raoni´c, Tobias Rohner, Roger K¨appeli, Roberto Molinaro, Emmanuel
de B´ezenac, and Siddhartha Mishra. Poseidon: Efficient foundation models for pdes. _arXiv_
_preprint arXiv:2405.19101_, 2024. 7, 21


Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In _Advances_
_in Neural Information Processing Systems_, volume 33, pp. 6840–6851, 2020. 1, 3, 19


Jiahe Huang, Guandao Yang, Zichen Wang, and Jeong Joon Park. DiffusionPDE: Generative PDEsolving under partial observation. _arXiv preprint arXiv:2406.17763_, 2024. 3, 4, 23


Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming Song. Denoising diffusion restoration
models. volume 35, pp. 23593–23606, 2022. 1, 3, 5


11


Published as a conference paper at ICLR 2025


Gavin Kerrigan, Giosue Migliorini, and Padhraic Smyth. Functional flow matching. _arXiv preprint_
_arXiv:2305.17209_, 2023. 1, 3, 4, 5, 7, 14, 16, 19, 20, 22


Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. _arXiv preprint arXiv:1901.05761_, 2019.
3, 4, 22


Marcel Kollovieh, Abdul Fatir Ansari, Michael Bohlke-Schneider, Jasper Zschiegner, Hao Wang,
and Yuyang Bernie Wang. Predict, refine, synthesize: Self-guiding diffusion models for probabilistic time series forecasting. _Advances in Neural Information Processing Systems_, 36, 2024.
1


Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in physics-informed neural networks. In _Advances in Neural_
_Information Processing Systems_, volume 34, pp. 26548–26560, 2021. 2, 3


Randall J. LeVeque. _Numerical Methods for Conservation Laws_ . Lectures in mathematics ETH
Z¨urich. Birkh¨auser Verlag, 1990. 2


Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier neural operator for parametric partial differential
equations. _arXiv preprint arXiv:2010.08895_, 2020a. 3, 4, 6, 9, 17, 18, 20, 23


Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator: Graph kernel network for partial differential equations. _arXiv preprint arXiv:2003.03485_, 2020b. 4


Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar
Azizzadenesheli, and Anima Anandkumar. Physics-informed neural operator for learning partial
differential equations. _ACM/JMS Journal of Data Science_, 1(3):1–27, 2024. 3, 23


Jae Hyun Lim, Nikola B Kovachki, Ricardo Baptista, Christopher Beckham, Kamyar Azizzadenesheli, Jean Kossaifi, Vikram Voleti, Jiaming Song, Karsten Kreis, Jan Kautz, et al. Score-based
diffusion models in function space. _arXiv preprint arXiv:2302.07400_, 2023. 1, 3, 7, 20


Lequan Lin, Zhengkun Li, Ruikun Li, Xuliang Li, and Junbin Gao. Diffusion models for time-series
applications: a survey. _Frontiers of Information Technology & Electronic Engineering_, 25(1):
19–41, 2024. 1


Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching
for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022. 3, 4, 15, 19


Phillip Lippe, Bas Veeling, Paris Perdikaris, Richard Turner, and Johannes Brandstetter. Pde-refiner:
Achieving accurate long rollouts with neural pde solvers. In _Advances in Neural Information_
_Processing Systems_, volume 37, 2024. 3


Ning Liu, Yue Yu, Huaiqian You, and Neeraj Tatikola. INO: Invariant neural operators for learning
complex physical systems with momentum conservation. In _Proceedings of The 26th Interna-_
_tional Conference on Artificial Intelligence and Statistics_, volume 206, pp. 6822–6838. PMLR,
2023a. 4


Ning Liu, Yiming Fan, Xianyi Zeng, Milan Kl¨ower, Lu Zhang, and Yue Yu. Harnessing the power
of neural operators with automatically encoded conservation laws. In _International conference on_
_machine learning_ . PMLR, 2024. 4


Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and
transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022. 3


Xingchao Liu, Lemeng Wu, Shujian Zhang, Chengyue Gong, Wei Ping, and Qiang Liu. Flowgrad: Controlling the output of generative odes with gradients. In _Proceedings of the IEEE/CVF_
_Conference on Computer Vision and Pattern Recognition_, pp. 24335–24344, 2023b. 3


Aaron Lou, Chenlin Meng, and Stefano Ermon. Discrete diffusion language modeling by estimating
the ratios of the data distribution. _arXiv preprint arXiv:2310.16834_, 2023. 1


12


Published as a conference paper at ICLR 2025


Lu Lu, Pengzhan Jin, and George Em Karniadakis. Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. _arXiv_
_preprint arXiv:1910.03193_, 2019. 4


Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.
Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the_
_IEEE/CVF conference on computer vision and pattern recognition_, pp. 11461–11471, 2022. 3, 5


S. Chandra Mouli, Danielle C. Maddix, Shima Alizadeh, Gaurav Gupta, Andrew Stuart, Michael W.
Mahoney, and Yuyang Wang. Using uncertainty quantification to characterize and improve outof-domain learning for pdes. In _International Conference on Machine Learning_ . PMLR, 2024. 1,
3, 4


Geoffrey N´egiar, Michael W. Mahoney, and Aditi S. Krishnapriyan. Learning differentiable solvers
for systems with hard constraints. In _International Conference on Learning Representations_,
2023. 3, 4


Jiachun Pan, Jun Hao Liew, Vincent YF Tan, Jiashi Feng, and Hanshu Yan. Adjointdpm: Adjoint
sensitivity method for gradient backpropagation of diffusion probabilistic models. _arXiv preprint_
_arXiv:2307.10711_, 2023a. 3


Jiachun Pan, Hanshu Yan, Jun Hao Liew, Jiashi Feng, and Vincent YF Tan. Towards accurate guided
diffusion sampling through symplectic adjoint method. _arXiv preprint arXiv:2312.12030_, 2023b.
3


M. Raissi, P. Perdikaris, and G.E. Karniadakis. Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential
equations. _Journal of Computational Physics_, 378:686–707, 2019. ISSN 0021-9991. 23


Nadim Saad, Gaurav Gupta, Shima Alizadeh, and Danielle C. Maddix. Guiding continuous operator
learning through physics-based boundary constraints. In _International Conference on Learning_
_Representations_, 2023. 1, 3, 4, 17


Dule Shu, Zijie Li, and Amir Barati Farimani. A physics-informed diffusion model for high-fidelity
flow field reconstruction. _Journal of Computational Physics_, 478:111972, 2023. 3


Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. In _Advances in Neural Information_
_Processing Systems_, volume 33, pp. 7462–7473, 2020. 3


Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv_
_preprint arXiv:2010.02502_, 2020. 3


Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.
In _Advances in Neural Information Processing Systems_, volume 32, 2019. 19


Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In
_Advances in Neural Information Processing Systems_, volume 33, pp. 12438–12448, 2020. 3


Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on_
_computer vision and pattern recognition_, pp. 2818–2826, 2016. 21


Ashish Vaswani. Attention is all you need. _arXiv preprint arXiv:1706.03762_, 2017. 20


Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, and Ju Sun. Dmplug: A plugin method for solving inverse problems with diffusion models. _arXiv preprint arXiv:2405.16749_,
2024. 3


Sifan Wang, Yujun Teng, and Paris Perdikaris. Understanding and mitigating gradient pathologies
in physics-informed neural networks. _arXiv preprint arXiv:2001.04536_, 2020. 2, 3


Sifan Wang, Hanwen Wang, and Paris Perdikaris. Learning the solution operator of parametric partial differential equations with physics-informed deeponets. _Science advances_, 7(40):eabi8605,
2021. 2, 3


13


Published as a conference paper at ICLR 2025

### **Supplementary Material**


A D ETAILS OF C ONSTRAINED S AMPLING


In this section, we further discuss additional details of our proposed ECI sampling framework. Our
proposed ECI sampling interleaves extrapolation, correction, and interpolation stages at each sampling step for flow-based generative models to enforce the hard constraint. We will elaborate further
on the implementation details below.


Figure 5: Intermediate results (trajectory) of the extrapolation, correction, and interpolation stages
for the Stokes problem with IC fixed. The trajectories demonstrate how the initial random noise
gradually transforms into the controlled generation with decreasing artifacts around the boundary.


A.1 F ORMAL D EFINITION OF C ONSTRAINED G ENERATION


First, we provide a more rigorous measure-theoretic definition of constrained generation for functional data as a generative task. Consider a well-posed PDE system _F_ _ϕ_ _u_ ( _x_ ) = 0 _, x ∈X_ with
PDE parameters _ϕ ∈_ Φ, then the solution map _T_ : _ϕ �→_ _u_ ( _x_ ) is a well-defined mapping. For a
fixed distribution _p_ Φ over the parameter _ϕ_, we can naturally define the measure over the solution
set as the push-forward of the solution map: _µ_ _F_ = _T_ _∗_ _p_ Φ . Now consider some constraint operator
_Gu_ ( _x_ ) = 0 _, x ∈X_ _G_ _⊆X_ defined on a subset of the PDE domain. Denote the solution set for _G_ as
_U_ _G_ = _{u_ ( _x_ ) : _Gu_ ( _x_ ) = 0 _, x ∈X_ _G_ _}_ and the characteristic function over the solution set as _χ_ _G_ such
that

1 _,_ _u ∈U_ _G_
_χ_ _G_ ( _u_ ) = _._ (4)
�0 _,_ _u ̸∈U_ _G_


With the non-degenerating assumption that � _U_ _[χ]_ _[G]_ _[dµ]_ _[F]_ _[ >]_ [ 0][ (this is essentially saying that we should]

have at least one solution), the conditional measure can be obtained as



_µ_ _F|G_ ( _A_ ) := _µ_ _F_ ( _A|G_ ) = _χ_ _G_ _dµ_ _F_
� _A_



_χ_ _G_ _dµ_ _F_ _,_ _∀A ⊆U,_ (5)

�� _U_



where _U_ is the Hilbert space of all “well-behaving” functions (see Kerrigan et al. (2023) for regularity conditions). We assume _µ_ _F_ can be approximated well by generative priors like FFM, and we
want to guide it towards the conditional measure _µ_ _F|G_ . In practice, the ground truth solution map
_T_ can be obtained by either exact analytical solutions or by using numerical PDE solvers. With a
pre-defined distribution over PDE parameters (often uniform over some interval, see Table 7 and
Appendix B for detailed specification), we can easily sample the PDE parameters and apply the
solution map to obtain the sampled functions for pre-training.


For constrained sampling, instead of directly calculating the conditional measure _µ_ _F|G_, we can
sample the PDE parameters from the pull-back probability _p_ Φ _|G_ = _T_ _[∗]_ _µ_ _F|G_ . In practice, we directly
assume such a conditional probability distribution _p_ Φ _|G_ over the PDE parameters is known. Indeed,
for all the experiments in this paper, we simply choose a subset of PDE parameters and apply the
solution map to obtain functions as the ground truth constrained solutions.


14


Published as a conference paper at ICLR 2025


A.2 E XTRAPOLATION OF S OLUTIONS


The continuous-time and deterministic formulation of the flow-matching sampling makes it possible
to apply different step sizes to advance the solver for the flow ODE. Specifically, at timestep _t_, we
can make a one-step prediction with a step size of 1 _−_ _t_ that directly advances the ODE solver to
_t_ = 1 as _u_ 1 = _u_ _t_ + (1 _−_ _t_ ) _v_ . Theoretically, if the optimal transport paths are learned exactly,
the flow should be straight (Lipman et al., 2022), and any arbitrary discretization should lead to the
same target. Indeed, in the left plot in Figure 5, the extrapolation gives reasonable prediction even
at a timestep close to 0. The constraint, on the other hand, is not necessarily satisfied in this stage.


A.3 S OLUTION C ORRECTION


Our ECI sampling scheme is a unified framework for different constraint operators as long as the
corresponding correction algorithm at _t_ = 1 is readily available. Specifically, we consider constraint
operators _G_ of two following categories: 1) _value constraints_ that specify exact values for a subset of
the PDE domain: _u_ ( _x_ ) = _g_ ( _x_ ) _, x ∈X_ _G_ _⊆X_, and 2) _region constraints_ that specify an exact value
for the integration over some subset of the domain: � _X_ _G_ _⊆X_ _[u]_ [(] _[x]_ [)] _[dx]_ [ =] _[ a][ ∈]_ [R][. These two classes of]

constraints encompass a wide range of practical constraints for PDE systems.
_Example_ 1 _._ For _X_ = Ω _×_ [0 _, T_ ] where Ω _∈_ R _[D][−]_ [1], the follow examples are value constraints:


    - _Initial condition_ (IC): _u_ ( _x,_ 0) = _g_ ( _x_ ) _, x ∈_ Ω.


    - _Boundary condition_ (BC): _u_ ( _x, t_ ) = _f_ ( _t_ ) _, x ∈_ _∂_ Ω _, t ∈_ [0 _, T_ ].


_Example_ 2 _._ For _X_ = [0 _,_ 1] _×_ [0 _, T_ ], the following examples are region constraints:


1

    - _Mass conservation_ : � 0 _[u]_ [(] _[x, t]_ [)] _[dx]_ [ = 0] _[, t][ ∈]_ [[0] _[, T]_ []][.]


    - _Periodic boundary condition_ (PBC): _u_ (0 _, t_ ) = _u_ (1 _, t_ ) _, t ∈_ [0 _, T_ ].


Furthermore, any linear conservation law can be understood as a region constraint, specifying a
conserved quantity over a region in the PDE domain. As these constraints are linear constraints,
we follow Hansen et al. (2023) to apply an oblique projection as a correction. The corresponding
correction algorithms are outlined in Algorithm 4 and 5.


**Algorithm 4** Value Constraint Correction

1: **Input:** function _u_ 1, constraint function _g_, region _X_ _G_ .
2: ˆ _u_ 1 _←_ 1 [ _x ∈X_ _G_ ] _⊙_ _g_ + 1 [ _x ̸∈X_ _G_ ] _⊙_ _u_ 1
3: **return** ˆ _u_ 1


**Algorithm 5** Region Constraint Correction



1: **Input:** function _u_ 1, conservation value _a_, region _X_ _G_ .
2: _g ←_ _u_ 1 + ( _a −_ � _X_ _[u]_ [1] [(] _[x]_ [)] _[dx]_ [)] _[/]_ � _X_ _[dx]_



_X_ _G_ _[u]_ [1] [(] _[x]_ [)] _[dx]_ [)] _[/]_ �



_X_ _G_ _[dx]_



3: ˆ _u_ 1 _←_ 1 [ _x ∈X_ _G_ ] _⊙_ _g_ + 1 [ _x ̸∈X_ _G_ ] _⊙_ _u_ 1
4: **return** ˆ _u_ 1


It is easy to verify that the corrected function ˆ _u_ 1 in these correction algorithms indeed satisfies the
corresponding constraint exactly. We can also have multiple (even infinitely many) non-overlapping
1
constraints. For example, consider the generalized mass conservation laws � 0 _[u]_ [(] _[x, t]_ [)] _[dx]_ [ =] _[ f]_ [(] _[t]_ [)] _[ ≡]_
0 _, t ∈_ [0 _, T_ ] for some given function _f_ ( _t_ ), we have a different conservation law for each PDE
timestep _t_ . After discretization, we have a finite set of non-overlapping constraints that can be
corrected according to Algorithm 5. The correction stage is demonstrated in the middle plot in
Figure 5. Note how directly applying the correction algorithm creates noticeable artifacts around
the boundary and how such artifacts gradually disappear when advancing the ODE solver.


15


Published as a conference paper at ICLR 2025


A.4 I NTERPOLATION OF S OLUTIONS


The iterative nature of flow sampling requires interpolation back to arbitrary timestep _t_ during each
sampling step. This can be understood as the forward noising process along the conditional path of
measures and can be efficiently calculated as the linear interpolation _u_ _t_ = (1 _−_ _t_ ) _u_ 1 + _tu_ 0 according
to the FFM formulation (Kerrigan et al., 2023). In the right plot in Figure 5, we demonstrate how the
initial Gaussian process noise is transformed into the constrained generation. We have also discussed
the impact of the re-sampling interval length to control stochasticity in the generation in Section 3.3.


A.5 P ROOF OF E XACT S ATISFACTION OF C ONSTRAINTS


**Proposition 1.** _Suppose the corrected algorithm C_ ( _u_ 1 _, G_ ) _in Equation 2 satisfies the constraint_
_G exactly, then for any number of mixing steps M ≥_ 1 _, the ECI sampling scheme described in_
_Algorithm 3 exactly recovers the constraint in the final generation at t_ = 1 _._


_Proof._ As the advancing step for the ODE solver (the last mixing step) will always perform, it
suffices to consider _M_ = 1. Consider the last Euler step at _t_ = 1 _−_ 1 _/N_, the linear interpolation

ˆ
procedure _q_ (ˆ _u_ _t_ _′_ _|u_ 1 ) = 0 _· u_ 0 + 1 _·_ ˆ _u_ 1 = ˆ _u_ 1 will be deterministic as the noise will not contribute to
the final interpolant at timestep _t_ _[′]_ = 1. Therefore, the interpolation will deterministically produce

ˆ
_q_ (ˆ _u_ _t_ _[′]_ _|u_ 1 ) = ˆ _u_ 1 = _C_ ( _u_ _t_ + (1 _−_ _t_ ) _v_ _θ_ ( _u_ _t_ ) _, G_ ) which satisfies the constraint exactly.


B D ATASET D ESCRIPTION AND G ENERATION


This section provides a more detailed description of the datasets used in this work and their generation procedure. In addition to the statistics in Table 2, we further provide additional information
in Table 7 for pre-training the FFM as our generative prior. For dataset types, _synthetic_ indicates
that the exact solutions are calculated on the fly based on the randomly sampled PDE parameters for
both the training and test datasets. We manually assign the training set with 5k solutions and the test
set with 1k solutions. On the other hand, _simulated_ indicates that the solutions are pre-generated
using numerical PDE solvers and are different for the training and test datasets.


Table 7: More dataset specifications for pre-training the prior FFM.


Dataset PDE Parameter Split Spatial Domain Time Domain Type


_k ∼_ _U_ [2 _,_ 20]
Stokes Problem _ω ∼_ _U_ [2 _,_ 8] 5k / 1k [0 _,_ 1] [0 _,_ 1] Synthetic

_α ∼_ _U_ [1 _,_ 5]
Heat Equation _ϕ ∼_ _U_ [0 _, π_ ] 5k / 1k [0 _,_ 2 _π_ ] [0 _,_ 1] Synthetic

_k_ (see below)
Darcy Flow _C ∼_ _U_ [ _−_ 2 _,_ 2] 10k / 1k [0 _,_ 1] [2] NA Simulated

NS Equation _w_ 0 _, f_ (see below) 10k / 1k [0 _,_ 1] [2] [0 _,_ 49] Simulated


PME _m ∼_ _U_ [1 _,_ 6] 5k / 1k [0 _,_ 1] [0 _,_ 1] Synthetic


Stefan Problem _u_ _[∗]_ _∼_ _U_ [0 _._ 55 _,_ 0 _._ 7] 5k / 1k [0 _,_ 1] [0 _,_ 0 _._ 1] Synthetic


B.1 S TOKES P ROBLEM


The 1D Stokes problem is given by the heat equation:


_u_ _t_ = _νu_ _xx_ _,_ _x ∈_ [0 _,_ 1] _, t ∈_ [0 _,_ 1] _,_

_u_ ( _x,_ 0) = _Ae_ _[−][kx]_ cos( _kx_ ) _,_ _x ∈_ [0 _,_ 1] _,_ (6)
_u_ (0 _, t_ ) = _A_ cos( _ωt_ ) _,_ _t ∈_ [0 _,_ 1] _,_


with viscosity _ν ≥_ 0, oscillation frequency _ω_, amplitude _A >_ 0, and _k_ = � _ω/_ (2 _ν_ ). The analytical

solution is given by _u_ exact ( _x, t_ ) = _Ae_ _[−][kx]_ cos( _kx −_ _ωt_ ). Note that _k_ and _ω_ independently and


16


Published as a conference paper at ICLR 2025


uniquely define the IC and BC, respectively, and fixing both values reduces the PDE to a well-posed
system with a unique solution. We follow Saad et al. (2023) to construct the dataset by fixing _A_ = 2
and sampling _ω ∼_ _U_ [2 _,_ 8] _, k ∼_ _U_ [2 _,_ 20] for pre-training. During constrained sampling, we test two
different settings of prescribing IC with _k_ = 5 or BC with _ω_ = 6, respectively. In the regression
task, we provide both the IC and BC values. Figure 1 shows the ground truth solutions for the
unconstrained and constrained Stokes problem.


B.2 H EAT E QUATION


The 1D heat (diffusion) equation with periodic boundary conditions is given as


_u_ _t_ = _αu_ _xx_ _,_ _x ∈_ [0 _,_ 2 _π_ ] _, t ∈_ [0 _,_ 1] _,_
_u_ ( _x,_ 0) = sin( _x_ + _φ_ ) _,_ _x ∈_ [0 _,_ 2 _π_ ] _,_ (7)
_u_ (0 _, t_ ) = _u_ (2 _π, t_ ) _,_ _t ∈_ [0 _,_ 1] _,_


where _α_ denotes the diffusion coefficient and _φ_ denotes the phase of the sinusoidal IC. The exact
solution is given as _u_ exact ( _x, t_ ) = _e_ _[−][αt]_ sin( _x_ + _φ_ ). We sample _α ∼_ _U_ [1 _,_ 5] and _φ ∼_ _U_ [0 _, π_ ] for
pre-training. During constrained sampling, we fix the phase _ϕ_ = _π/_ 4. Figure 6 shows the ground
truth solution family.


In the uncertainty quantification task, we follow Hansen et al. (2023) to fix _ϕ_ = 0 and vary only the
diffusion coefficient in _U_ [1 _,_ 5] during pre-training. During constrained generation, we use the same
setting to fix _α_ = 1 _, t_ = 0 _._ 5 for calculating the MSE and LL. The global conservation law for the
heat equation in Equation 7 is written as

2 _π_

_u_ ( _x, t_ ) _dx_ = 0 _,_ _t ∈_ [0 _,_ 1] _._ (8)

� 0



Figure 6: Ground truth solutions
for the heat equation with _α ∼_
_U_ [1 _,_ 5] _, φ ∼_ _U_ [0 _, π_ ].


B.3 D ARCY F LOW



Figure 7: Ground truth solutions for the Darcy flow with _C ∼_
_U_ [ _−_ 2 _,_ 2]. Solution values are unnormalized on the left and normalized per-solution on the right.



The 2D Darcy flow is a time-independent, second-order, elliptic PDE with the following form:


_−∇·_ ( _k_ ( _x_ ) _∇u_ ( _x_ )) = _f_ ( _x_ ) _,_ _x ∈_ _D_ = [0 _,_ 1] [2] _,_
(9)
_u_ ( _x_ ) = _C,_ _x ∈_ _∂D,_


where _k_ denotes the permeability field and _f_ denotes the forcing function. We follow Li et al.
(2020a) to fix _f_ ( _x_ ) _≡_ 1 and sample _k ∼_ _ψ_ _∗_ _N_ (0 _,_ ( _−_ ∆+ 9 _I_ ) _[−]_ [2] ) with zero Neumann boundary
conditions on the Laplacian. The mapping _ψ_ : R _→_ R takes the value 12 for positive numbers and
3 for the negative numbers, and the pushforward is defined pointwise. Unlike previous work, the
boundary condition is sampled from _C ∼_ _U_ [ _−_ 2 _,_ 2]. It can be verified that, for the same _k_, if _u_ 0 is
the solution for prescribing the boundary condition _u_ ( _x_ ) = 0 _, x ∈_ _∂D_, then _u_ 0 + _C_ is the solution
when prescribing the boundary condition _u_ ( _x_ ) = _C, x ∈_ _∂D_ . Therefore, we simulate 1000 samples
with different _k_ and the zero boundary condition and sample _C_ on the fly for both the training and
testing datasets. During constrained generation, we fix _C_ = 1. Figure 7 shows the ground truth
solution family.


17


Published as a conference paper at ICLR 2025


B.4 N AVIER -S TOKES E QUATION


The 2D Navier-Stokes (NS) equation for a viscous, incompressible fluid in the vorticity form with
periodic boundary conditions is given as


_∂_ _t_ _w_ ( _x, t_ ) + _u_ ( _x, t_ ) _· ∇w_ ( _x, t_ ) = _ν_ ∆ _w_ ( _x, t_ ) + _f_ ( _x_ ) _,_ _x ∈_ [0 _,_ 1] [2] _, t ∈_ [0 _, T_ ] _,_



_∇· u_ ( _x, t_ ) = 0 _,_ _x ∈_ [0 _,_ 1] [2] _, t ∈_ [0 _, T_ ] _,_

_w_ ( _x,_ 0) = _w_ 0 ( _x_ ) _,_ _x ∈_ [0 _,_ 1] [2] _,_



(10)



where _u_ denotes the velocity field, _w_ = _∇× u_ denotes the vorticity, and _w_ 0 denotes the initial
vorticity. We follow Li et al. (2020a) to sample _w_ 0 _∼N_ (0 _,_ 7 [3] _[/]_ [2] ( _−_ ∆+ 49 _I_ ) _[−]_ [5] _[/]_ [2] ) with periodic
boundary conditions. The forcing term is defined as _f_ ( _x_ ) = 0 _._ 1 _√_ 2 sin(2 _π_ ( _x_ 1 + _x_ 2 ) + _ϕ_ ) where

_ϕ ∼_ _U_ [0 _, π/_ 2], and the viscosity is fixed to be _ν_ = 10 _[−]_ [3] . We sample 100 initial vorticities and 100
forces with linearly spaced _ϕ_ in the interval [0 _, π/_ 2]. We mesh-grid the vorticities and forces to build
10000 solutions for the training set. For the testing set, we sample another 10 initial vorticities and
mesh-grid them with the same 100 forces, resulting in another 1000 solutions. We use _T_ = 49 and
sample 50 snapshots of the numerical solutions. We use the same generative NS equation datasets for
training for the regression task that maps the first 15 frames to the other 35 frames. The regression
evaluation is performed on the first sample in the test set.


B.5 P OROUS M EDIUM E QUATION


The nonlinear Porous Medium Equation (PME) with zero initial and time-varying Dirichlet left
boundary conditions is given as



_u_ _t_ = _∇·_ ( _u_ _[m]_ _∇u_ ) _,_ _x ∈_ [0 _,_ 1] _, t ∈_ [0 _,_ 1] _,_
_u_ ( _x,_ 0) = 0 _,_ _x ∈_ [0 _,_ 1] _,_

_u_ (0 _, t_ ) = ( _mt_ ) [1] _[/m]_ _,_ _t ∈_ [0 _,_ 1] _,_
_u_ (1 _, t_ ) = 0 _,_ _t ∈_ [0 _,_ 1] _,_



(11)



where _m ≥_ 1. The exact solution is given as _u_ exact ( _x, t_ ) = ( _m_ ReLU( _t −_ _x_ )) [1] _[/m]_ . We follow
Hansen et al. (2023) to sample _m ∼_ _U_ [1 _,_ 5] for pre-training and fix _m_ = 1 _, t_ = 0 _._ 5 for sampling.
The conservation law for the PME in Equation 11 is written as

1

_u_ ( _x, t_ ) _dx_ = [(] _[mt]_ [)] [1+1] _[/m]_ _,_ _t ∈_ [0 _,_ 1] _._ (12)

� 0 _m_ + 1


Note that this conservation law implicitly contains information about _m_ .


B.6 S TEFAN P ROBLEM


The Stefan problem is a challenging and nonlinear case of the Generalized Porous Medium Equation
(GPME). With fixed Dirichlet boundary conditions, it is given as:



1

_u_ ( _x, t_ ) _dx_ = [(] _[mt]_ [)] [1+1] _[/m]_
0 _m_ + 1



_,_ _t ∈_ [0 _,_ 1] _._ (12)
_m_ + 1



_u_ _t_ = _∇·_ ( _k_ ( _u_ ) _∇u_ ) _,_ _x ∈_ [0 _,_ 1] _, t ∈_ [0 _, T_ ] _,_
_u_ ( _x,_ 0) = 0 _,_ _x ∈_ [0 _,_ 1] _,_
_u_ (0 _, t_ ) = 1 _,_ _t ∈_ [0 _, T_ ] _,_
_u_ (1 _, t_ ) = 0 _,_ _t ∈_ [0 _, T_ ] _,_


where _k_ ( _u_ ) denotes the nonlinear step function with respect to a fixed shock value _u_ _[∗]_ :



(13)



_k_ ( _u_ ) = 1 _,_ _u ≥_ _u_ _[∗]_ (14)
�0 _,_ _u < u_ _[∗]_ _._



The exact solution is given as


_√t_ ))
_u_ exact ( _x, t_ ) = 1 [ _u ≥_ _u_ _[∗]_ ] 1 _−_ (1 _−_ _u_ _[∗]_ ) [erf][(] _[x][/]_ [(][2]
� erf( _α_ )


18



_,_ (15)
�


Published as a conference paper at ICLR 2025



2 _z_
where erf( _z_ ) = ~~_√π_~~ � 0 [exp(] _[−][t]_ [2] [)] _[dt]_ [ is the error function and] _[ α]_ [ is uniquely determined by] _[ u]_ _[∗]_ [as the]
solution for the nonlinear equation (1 _−_ _u_ _[∗]_ ) _/_ _[√]_ ~~_π_~~ = _u_ _[∗]_ erf( _α_ ) _α_ exp( _α_ [2] ). We follow Hansen et al.
(2023) to sample the shock value _u_ _[∗]_ _∼_ _U_ [0 _._ 55 _,_ 0 _._ 7] and use _T_ = 0 _._ 1. During sampling, we fix
_u_ _[∗]_ = 0 _._ 6 _, t_ = 0 _._ 05. The conservation law for the Stefan problem in Equation 13 is

1 _t_

_u_ ( _x, t_ ) _dx_ = [2][(][1] _[ −]_ _[u]_ _[∗]_ [)] _t ∈_ [0 _,_ 1] _._ (16)

� 0 erf( _α_ ) ~~�~~ _π_ _[,]_



_u_ ( _x, t_ ) _dx_ = [2][(][1] _[ −]_ _[u]_ _[∗]_ [)]
0 erf( _α_ )



~~�~~



erf( _α_ )



_t_
_t ∈_ [0 _,_ 1] _._ (16)
_π_ _[,]_



C F UNCTIONAL F LOW M ATCHING AS THE G ENERATIVE P RIOR


This section provides additional mathematical backgrounds on flow matching, functional flow
matching, and our pre-training settings for FFM as the generative prior.


C.1 F LOW M ATCHING


We first provide more details regarding the flow matching framework on common Euclidean data
(e.g., pixel values of images) before proceeding to functional flow matching for functional data
(e.g., PDE solutions). Flow matching (Lipman et al., 2022) is a generative framework built upon
continuous normalizing flows (Chen et al., 2018). This flow-based model can be viewed as the
continuous generalization of the score matching (diffusion) model that allows for a more flexible
design of the denoising process following the optimal transport formulation (Lipman et al., 2022).
Flow matching tries to learn the time-dependent _vector field v_ _t_ : R _[d]_ _×_ [0 _,_ 1] _→_ R _[d]_ that defines a
continuous time-dependent diffeomorphism called the _flow ψ_ _t_ : R _[d]_ _×_ [0 _,_ 1] _→_ R _[d]_ via the following
_flow ODE_ :
_∂_
_x_ 0 _∼_ _p_ 0 ( _x_ ) _,_ (17)
_∂t_ _[ψ]_ _[t]_ [(] _[x]_ [) =] _[ v]_ _[t]_ [(] _[ψ]_ _[t]_ [(] _[x]_ [))] _[,]_

where _p_ 0 is the initial noise distribution. The flow induces a probability path with the push-forward
_p_ _t_ = ( _ψ_ _t_ ) _∗_ _p_ 0 for generative modeling. The vanilla flow matching loss can be written as


_L_ FM = E _t∼U_ [0 _,_ 1] _,x_ 1 _∼p_ 1 ( _x_ ) [ _∥v_ _θ_ ( _x_ _t_ _, t_ ) _−_ _u_ _t_ ( _x_ _t_ ) _∥_ [2] ] _,_ (18)


where _x_ _t_ := _ψ_ _t_ ( _x_ ) is the noise data, _p_ 1 is the target data distribution, and _u_ _t_ ( _x_ _t_ ) is the ground truth
data vector field at _x_ _t_ and timestep _t_ . The vanilla flow matching objective is generally intractable, as
we do not know the ground truth vector fields. Lipman et al. (2022) demonstrates a key observation
that, when considering the _conditional_ probability path _ψ_ _t_ ( _x|x_ 1 ) conditioned on the target data, the
_conditional_ vector field _u_ _t_ ( _x_ _t_ _|x_ 1 ) can be calculated analytically while sharing the same gradient
with the vanilla flow matching objective. The conditional flow matching objective can be written as


_L_ CFM = E _t∼U_ [0 _,_ 1] _,x_ 1 _∼p_ 1 ( _x_ ) [ _∥v_ _θ_ ( _x_ _t_ _, t_ ) _−_ _u_ _t_ ( _x_ _t_ _|x_ 1 ) _∥_ [2] ] _._ (19)


Following Chen & Lipman (2023), when further conditioned on the noise _x_ 0 _∼_ _p_ 0 ( _x_ ), the CFM
loss can be reparameterized into a simple form:


_L_ CFM = E _t∼U_ [0 _,_ 1] _,x_ 0 _∼p_ 0 ( _x_ ) _,x_ 1 _∼p_ 1 ( _x_ ) [ _∥v_ _θ_ ( _x_ _t_ _, t_ ) _−_ ( _x_ 1 _−_ _x_ 0 ) _∥_ [2] ] _,_ _x_ _t_ = (1 _−_ _t_ ) _x_ 0 + _tx_ 1 _._ (20)


The linear interpolation above corresponds to the _optimal-transport probability path_ (OT-path) in
Lipman et al. (2022), which enjoys additional theoretical benefits of straighter vector fields over
other options, including the variance-preserving path (Ho et al., 2020) or the variance-exploding
path (Song & Ermon, 2019). During sampling, the flow ODE in Equation 17 is solved with the
learned vector field to obtain the final generation during sampling. The deterministic flow ODE
makes it potentially easier to guide than the stochastic Langevin dynamics or stochastic differential
equations (SDEs) in the diffusion formulation.


C.2 F UNCTIONAL F LOW M ATCHING


Functional flow matching (FFM) (Kerrigan et al., 2023) further extends the conditional flow matching framework to modeling functional data — intrinsically continuous data like PDE solutions. As
mentioned in the original work, the major challenge of extending the CFM framework lies in the
fact that probability densities are ill-defined over the infinite-dimensional Hilbert space of continuous functions. FFM proposed to generalize the idea of flow matching to define _path of measures_


19


Published as a conference paper at ICLR 2025


using measure-theoretic formulations. Specifically, consider a real separable Hilbert space _U_ of
functions _u_ : _X →_ R equipped with the Borel _σ_ -algebra _B_ ( _U_ ). FFM learns a time-dependent vector
field operator _v_ _t_ : _U ×_ [0 _,_ 1] _→U_ that defines a time-dependent diffeomorphism _ψ_ _t_ : _U ×_ [0 _,_ 1] _→U_
called the _flow_ via the differential equation
_∂_ _t_ _ψ_ _t_ ( _u_ ) = _v_ _t_ ( _ψ_ _t_ ( _u_ )) _,_ _u_ 0 _∼_ _µ_ 0 ( _u_ ) _,_ (21)

where _µ_ 0 is some fixed noise measure from which random continuous functions can be sampled.
Similar to CFM, the flow _ψ_ _t_ induces a push-forward measure ˆ _µ_ _t_ := ( _ψ_ _t_ ) _∗_ _µ_ 0 for generative modeling. FFM demonstrates that the conditional formulation in CFM to deduce a tractable flow matching
objective can also be adapted for the path of measures under some regularity conditions. In this way,
also relying on the optimal transport path of measures, the FFM objective shares a similar format as
the CFM loss in Equation 20:
_L_ FFM = E _t∼U_ [0 _,_ 1] _,u_ 0 _∼µ_ 0 ( _u_ ) _,u_ 1 _∼µ_ 1 ( _u_ ) [ _∥v_ _θ_ ( _u_ _t_ _, t_ ) _−_ ( _u_ 1 _−_ _u_ 0 ) _∥_ [2] ] _,_ _u_ _t_ = (1 _−_ _t_ ) _u_ 0 + _tu_ 1 _,_ (22)

where _u_ _t_ := _ψ_ _t_ ( _u|u_ 1 ) is the interpolation along the _conditional_ path of measures and _µ_ 1 is the
target data measure. The norm is the standard _L_ [2] -norm for square-integrable functions. Similarly,
sampling for FFM can be thought of as solving the flow ODE with the learned vector field. We
demonstrate the Euler method for FFM sampling in Algorithm 1.


We also noted that Lim et al. (2023) proposed the diffusion denoising operator (DDO) as an extension of diffusion models to function spaces. DDO relies on the non-trivial extension of Gaussian
measures on function spaces. Compared to DDO, FFM has a more concise mathematical formulation and better empirical generation results. Therefore, we use FFM as our generative prior.


C.3 P RE -T RAINING FOR FFM


We follow Kerrigan et al. (2023) to use Fourier Neural Operator (FNO) (Li et al., 2020a) as the vector
field operator parameterization with additional relative coordinates and sinusoidal time embeddings
(Vaswani, 2017) concatenated to the noised function as inputs. Following FFM, for all 2D data (1D
PDE with a time dimensional or Darcy flow), the prior noises are sampled from the 2D Gaussian
process with a Mat´ern kernel with a kernel length of 0.001 and kernel variance of 1. For 3D data (2D
NS equation), sampling from the Mat´ern kernel is prohibitively expensive. Though Kerrigan et al.
(2023) has indicated that the white noise does not meet the regularity requirement from mathematical
considerations, we empirically found that such white noise prior performed well enough. Therefore,
for 3D data, we always sample from the standard white noise.


For 2D data, we use a four-layer FNO with a frequency cutoff of 32 × 32, a time embedding channel
of 32, a hidden channel of 64, and a projection dimension of 256, which gives a total of 17.9M
trainable parameters. All FFMs are trained on a single NVIDIA A100 GPU with a batch size of
256, an initial learning rate of 3 _×_ 10 _[−]_ [4], and 20k iterations (approximately 1000 epochs for a 5k
training dataset).


For 3D data, we use a two-layer FNO with a frequency cutoff of 16 × 16 × 16, a time embedding
channel of 16, a hidden channel of 32, and a projection dimension of 256, which gives a total of
9.46M trainable parameters for efficiency concerns. This model is trained on 4 NVIDIA A100
GPUs with a batch size of 24 (per GPU) for approximately a total number of 2M iterations (or 5000
epochs) with an initial learning rate of 3 _×_ 10 _[−]_ [4] .


It is worth noting that our proposed ECI sampling framework, as a unified zero-shot approach for
guiding pre-trained flow-matching models, does not require conditional training like the conditional
FFM baseline. Therefore, we only need to train a separate model for each different PDE system, but
not for each different constraint. For example, the two generative tasks and the regression task on
the Stokes equation are based on the same pre-trained FFM on the unconstrained Stokes problem
dataset.


To evaluate the pre-training quality, we provide MMSE and SMSE between the generation by the
pre-trained unconstrained FFM model and the test dataset in Table 8 as the evaluation metrics. It can
be seen that most FFMs can achieve a decent approximation of the prior unconstrained distribution
over the solution set with small MMSEs and SMSEs. For the 2D Darcy flow, the boundary condition
has a more significant impact on the final solution as all pixel values should be shifted by the same
value, leading to larger errors in the distributional properties. The generation, on the other hand,
does seem reasonable.


20


Published as a conference paper at ICLR 2025


Table 8: Mean and standard deviation MSEs between the generation by the pre-trained unconstrained FFM model and the test dataset.


Dataset Stokes Problem Heat Equation Darcy Flow NS Equation


MMSE / 10 _[−]_ [2] 0.045 0.130 5.421 0.992
SMSE / 10 _[−]_ [2] 0.039 0.198 4.121 0.371


We also provide the generative metrics of the unconstrained pre-trained FFM without any guidance
as a sanity check in Table 3. It can be demonstrated that there is indeed a shift in the distribution
from the generative prior learned by the unconstrained FFM, as many errors are significant. Our
proposed ECI sampling can successfully capture such a shift in the distribution with significantly
lower MSEs and a closer resemblance to the ground truth constrained distribution.


D E XPERIMENTAL S ETUP


In this section, we provide further details regarding the experimental setups and evaluation metrics.
We also give a brief introduction to the baselines used in our experiments.


D.1 S AMPLING S ETUP


For the baseline models, we use the adaptive Dopri5 ODE solver (Dormand & Prince, 1980) for
CondFFM and ProbConserv. For ECI and DiffusionPDE, we used 200 Euler steps for 2D datasets
and 100 steps for 3D datasets. For D-Flow, due to the limitation of GPU memory, we use 100 Euler
steps for 2D datasets and 20 Euler steps for 3D datasets. The hyperparameters for sampling are
summarized in Table 4.


Generative models have intrinsic stochasticity, as the sampling procedure has randomness in the
prior noise distribution. To provide a more robust evaluation of the generated results, we always
generate 512 samples for every 2D dataset and baseline for computing all evaluation metrics and
the sampling time in Table 4. 100 samples are generated for all the NS equation tasks (3D dataset).
In this way, we aim to minimize the impact of randomness in the generation and provide a better
estimation of the statistics and metrics.


D.2 E VALUATION M ETRICS


As we have discussed in Section 4, for generative tasks, we calculate distributional properties like
the mean squared errors of mean and standard deviation ( **MMSE** and **SMSE** ) to measure the errors
from the ground truth statistics. We also use the constraint error ( **CE** ) to measure the violation of
constraints.


The Fr´echet Poseidon distance ( **FPD** ) is inspired by the metric of Fr´echet Inception distance (FID)
widely used in the image generation domain to evaluate the generative quality using the pre-trained
InceptionV3 model (Szegedy et al., 2016). By comparing the Fr´echet distance between the hidden
activations of the data captured by a pre-trained discriminative model, such a score provides a measurement of the similarity between data distributions. A smaller Fr´echet distance to the ground truth
data distribution indicates a closer resemblance to the ground truth data and, thus, a better generation
quality. Inspired by FID, we propose to use the pre-trained PDE foundation model _Poseidon_ (Herde
et al., 2024) to generate the hidden activations. We used the base version of Poseidon with 157.8M
model parameters. Poseidon takes the initial frame, _x_ -velocity field, _y_ -velocity field, pressure field,
and timestep as inputs and outputs predictions of the frame after the timestep. We always zeroed
out the _x_ -velocity field, _y_ -velocity field, and pressure field in the inputs. For 2D data, the generated
solutions are directly fed into Poseidon as the initial frames, and the timestep is always fixed to 0.
For 3D data, each time frame of the solution trajectories is fed separately into Poseidon. The Fr´echet
distances are calculated separately for different time frames and are averaged to obtain the final FPD
score. As we want to leverage the pre-trained Poseidon to extract high-level representations of the
generated solutions, we extract the last hidden activations of the encoder for Fr´echet distance calcu

21


Published as a conference paper at ICLR 2025


lation. The hidden activation size for the Poseidon base model is 784 × 4 × 4, which we mean-pool
into a 784-dimensional vector for FPD calculation.


For regression tasks, we can directly calculate the **MSE** with respect to the ground truth and the
**CE** for constraint violation. Following Hansen et al. (2023), the log-likelihood ( **LL** ) is calculated
pointwise as



LL = log _p_ � _u_ 1 _|N_ (ˆ _µ,_ ˆ _σ_ [2] _I_ )� = _−_ _[u]_ [1] _[ −]_ _[µ]_ [ˆ]

[2]



(23)
2 [log(2] _[π]_ [)] _[,]_




[1] _[ −]_ _[µ]_ [ˆ]

_−_ log ˆ _σ −_ [1]
2ˆ _σ_ [2]



where _u_ 1 is the ground truth data and ˆ _µ,_ ˆ _σ_ are the generation mean and standard deviation, respectively.


D.3 C ONSTRAINT E NFORCEMENT


For value constraints, we can easily enforce the constraint as described in Algorithm 4. Specifically, the initial condition is defined as the first column values after discretization, and the boundary
condition is defined as the first row values. For 2D Darcy flow, the boundary condition is defined
as the outer-most pixels in all four directions, thus leading to an additional resolution along each
dimension. For conservation laws, the integral is approximated with the Riemann sum across the
constrained region with uniform discretization along each dimension. The constraint error is also
calculated using the same Riemann sum.


In the uncertainty quantification task, we follow Hansen et al. (2023) to experiment with the zeroshot superresolution setting. While the pre-trained FFM is trained on a spatiotemporal resolution of
100 × 100, during constrained generation, a resolution of 200 × 200 is instead enforced. Following
Hansen et al. (2023), 100 random context points (0.25%) of the ground truth solution are provided
as the value constraint, with additional conservation laws given in Appendix B.


D.4 B ASELINE M ODELS


**CondFFM.** The conditional FFM model (Kerrigan et al., 2023) assumes the family of constraints
is known _a priori_ . During training, the constraints are fed to the FFM model as additional information. In this way, CondFFM can only handle value-based constraints but not conservation laws. In
practice, we copy the input function and set the pixels in the constrained region to the corresponding
values and the pixels in the unconstrained region to zero. This condition is then concatenated to the
input channel-wise as additional information. Similarly, this condition is fed into the model during
the constrained sampling.


We use an almost identical FNO-based encoder for CondFFM except for one additional channel for
the condition. For a fair comparison, the training hyperparameters are the same as the corresponding
generative FFM. As CondFFM has additional information on the constraint in both training and
sampling, we naturally expect it to exhibit better performance than other zero-shot (training-free)
baselines. As a drawback for all models that require training adjustment, different CondFFM models
need to be trained separately for different constraints even on the same PDE system (e.g., IC and BC
for the Stokes problem), making it less flexible to different constraints.


**ANP.** The Attentive Neural Process (ANP) (Kim et al., 2019) models the conditional distribution
of a function _u_ at a specific set of target points _{x_ _i_ _}_ _i∈T_ given another set of context points _{x_ _i_ _}_ _i∈C_ .
The ANP uses the attention mechanism and variational approach to maximize the variational lower
bound for the data likelihood. As it is a special case of neural processes, the ANP can take both
target and context points of arbitrary sizes.


In practice, we use 100 random context points and 1000 random target points during training. The
prior model is trained on the same amount of data as the corresponding FFM models. During
sampling, the context points are fixed to be the values in the constraints, and all discretized spatiotemporal coordinates are used as the target points in the generation. ANP model is also zero-shot
and can be applied to different value constraints.


**ProbConserv.** ProbConserv (Hansen et al., 2023) is a general black-box posterior sampling
method specifically designed for the exact satisfaction of various PDE constraints. ProbConserv
adopts the gradient-free approach that directly projects the generation to the corresponding solution


22


Published as a conference paper at ICLR 2025


space using the least squares method. When applied to value constraints, ProbConserv (the version
that ensures the exact satisfaction of constraints) directly modifies the corresponding pixels to the
constraint values, often leaving noticeable artifacts between the constrained and the unconstrained
regions. For a fair comparison, the ProbConserv model in our experiments is built upon the same
pre-trained FFM model. As ProbConserv directly operates on the final generation, it does not offer
fine-grained control over the intermediate steps of the iterative sampling process of flow-matching
models.


**DiffusionPDE.** DiffusionPDE (Huang et al., 2024) uses gradient guidance from the constraint
loss and the PINN loss (Raissi et al., 2019; Li et al., 2024) to modify the vector field at each step to
minimize these losses while maintaining the physical structure of the PDE system. This gradientbased approach is also representative of many models for inverse problems, specifically, the diffusion
posterior sampling approach (Chung et al., 2022). As our PDE system may have variations in the
PDE parameters, the PINN loss is not applicable. Thus, we only use the constraint loss to guide the
model at each time step. In practice, we follow the original paper to test several guidance strengths
from 10 [2] to 10 [3] and choose the value with the lowest MSE.


**D-Flow.** D-Flow (Ben-Hamu et al., 2024) guides the final generation via optimizing the initial
noise _u_ 0 rather than modifying the vector field. Assuming the constraint loss _L_ (ˆ _u_ 1 ) is differentiable, D-Flow backpropagates the gradient of the loss through the ODE solver to the initial noise
as _∇_ _u_ 0 _L_ (ˆ _u_ 1 ), hoping such a direct optimization can reduce the final constraint loss. Such a direct
gradient-based approach is extremely memory-demanding and time-consuming, as the simplest Euler solver requires about 100 steps to achieve decent generation results. We follow the original work
to use the LBFGS optimizer with a learning rate of 1 and 20 maximum iterations.


**FNO.** The Fourier Neural Operator (FNO) (Li et al., 2020a) is used as the additional baseline
in regression tasks of neural operator learning. We use an almost identical architecture as the encoder for FFM, except for the missing time embedding part. Similar to CondFFM, the pixels in
the constrained region are set to the constraint values, while other pixels are set to zero. For a fair
comparison, the FNO model tries to predict all the pixel values for the solution domain, including
the constrained ones already given as the input. As the regression models are easier to overfit, we
apply early-stopping techniques to choose the best model checkpoint for evaluation. The FNO was
trained for 10k iterations for the Stokes problem and 500k iterations for the NS equation when the
valid loss plateaued.


E A DDITIONAL R ESULTS AND V ISUALIZATIONS


In this section, we provide additional experimental results and ablation studies to further demonstrate
the effectiveness of our proposed ECI sampling scheme. Additional visualizations of the generations
and evaluation metrics are also provided in this section.


E.1 G ENERATIVE T ASKS


E.1.1 A DDITIONAL V ISUALIZATION


Figure 8: Generation mean and standard deviation for the Stokes problem with IC fixed.


23


Published as a conference paper at ICLR 2025


Figure 9: Generation mean and standard deviation for the Stokes problem with BC fixed.


Figure 10: Generation mean and standard deviation errors for the Stokes problem with BC fixed.


In addition to the error plots in Figure 2 and 3, we further provide visualizations of the generation
statistics in Figure 8 (Stokes problem with IC fixed), 9 (Stokes problem with BC fixed), 11 (heat
equation), 13 (Darcy flow), and 15 (2D NS equation).


Errors for generation statistics for different systems are also provided in Figure 10 (Stokes problem
with BC fixed), 12 (heat equation), and 14 (Darcy flow). It can be demonstrated more clearly that
gradient-based methods like DiffusionPDE and D-Flow had more artifacts around the boundary and
did not satisfy the constraint exactly. For ProbConserv, though its gradient-free approach indeed
ensured the exact satisfaction of the constraint, the non-iterative control of the prior flow model left
noticeable artifacts between the constrained and the unconstrained regions. Similar trends can also
be observed in other PDE systems.


Figure 11: Generation mean and standard deviation for the heat equation with IC fixed.


We also noticed that D-Flow tended to lead high-variance generations. This is probably because a
large number of Euler steps leads to an exceedingly complex dynamic that is difficult to optimize.
Also, note that directly taking the gradient with respect to the initial noise may break the initial
structure sampled from the Mat´ern kernel and may lead to initial noises never seen by the model
during training. These two reasons may also account for the better performance of the D-Flow for
the NS equation, in which we used fewer Euler steps and the standard white noises as the prior noise
functions.


24


Published as a conference paper at ICLR 2025


Figure 12: Generation mean and standard deviation errors for the heat equation with IC fixed.


Figure 13: Generation mean and standard deviation for the Darcy flow with BC fixed. Note that the
scaling range is different and large variance values are cropped for a better comparison.


E.2 S TOKES P ROBLEM WITH D IFFERENT C ONSTRAINTS


We provide additional generative evaluation on the Stokes problem with various ICs/BCs in Table 9
in addition to the results in Table 3. Recall that during unconstrained pre-training, we varied the
PDE parameter as _ω ∼_ _U_ [2 _,_ 8] _, k ∼_ _U_ [2 _,_ 20], and in Table 3 we fixed _k_ = 5 or _ω_ = 6. Combined
with the results Table 9, it can be clearly demonstrated that our proposed ECI sampling can achieve
consistent performance improvement over the baselines in a zero-shot manner with the flexibility to
all different IC settings _k_ = 5 _,_ 10 _,_ 15 and BC settings _ω_ = 4 _,_ 6 _,_ 8.


E.2.1 G ENERATIVE S UPERRESOLUTION


In Table 10, we provide the evaluation results under the zero-shot superresolution setting, where the
generation resolution is 200 _×_ 200, four times larger than the training set. It is interesting to see that
the conditional FFM is the most sensitive method in such a zero-shot setting, probably due to the
hard-coded connection of the masked input as the condition.


Figure 14: Generation mean and standard deviation errors for the Darcy flow with BC fixed. Note
that large error values are cropped for a better comparison.


25


Published as a conference paper at ICLR 2025


Figure 15: Generation mean and standard deviation for the 2D NS equation with IC fixed.


Table 9: Generative metrics on the Stokes problem with different constraints. The best results for






|zero-shot methods (CondFFM is not zero-shot) are highlighted in bold.|Col2|
|---|---|
|Dataset<br>Metric<br>ECI<br>ANP<br>ProbConserv<br>DiffusionPDE<br>D-Flow|CondFFM|
|Stokes IC<br>_k_ = 10<br>MMSE / 10_−_2<br>**0.113**<br>4.616<br>1.792<br>0.512<br>0.774<br>SMSE / 10_−_2<br>**0.124**<br>12.400<br>0.356<br>0.301<br>59.377<br>CE / 10_−_2<br>**0**<br>13.032<br>**0**<br>0.068<br>3.004<br>FPD<br>**0.156**<br>6.694<br>3.619<br>0.340<br>11.013|0.307<br>0.060<br>0.011<br>0.454|
|Stokes IC<br>_k_ = 15<br>MMSE / 10_−_2<br>0.190<br>11.148<br>0.797<br>0.216<br>**0.088**<br>SMSE / 10_−_2<br>**0.183**<br>3.354<br>2.751<br>0.470<br>12.063<br>CE / 10_−_2<br>**0**<br>18.278<br>**0**<br>0.194<br>13.149<br>FPD<br>**0.105**<br>6.509<br>6.425<br>0.832<br>4.745|0.125<br>0.010<br>0.010<br>0.020|
|Stokes BC<br>_ω_ = 4<br>MMSE / 10_−_2<br>**0.676**<br>9.273<br>3.034<br>1.488<br>1.728<br>SMSE / 10_−_2<br>**0.378**<br>2.777<br>2.462<br>2.278<br>6.343<br>CE / 10_−_2<br>**0**<br>115.335<br>**0**<br>39.99<br>14.553<br>FPD<br>**1.671**<br>5.309<br>1.686<br>3.954<br>10.043|0.031<br>0.081<br>0.012<br>0.071|
|Stokes BC<br>_ω_ = 8<br>MMSE / 10_−_2<br>**0.042**<br>4.115<br>9.008<br>5.794<br>3.801<br>SMSE / 10_−_2<br>**0.026**<br>2.462<br>2.710<br>3.288<br>22.376<br>CE / 10_−_2<br>**0**<br>73.916<br>**0**<br>160.397<br>51.056<br>FPD<br>**0.109**<br>6.663<br>0.843<br>9.356<br>13.202|0.028<br>0.010<br>0.017<br>0.111|


E.2.2 A DDITIONAL M ETRICS FOR NS E QUATION


We provide the per-frame FPD scores for the NS equation generation results in Figure 16. We noted
that ECI sampling tended to have more accurate generations for frames close to the initial condition,
with the FPD scores increasing with the time frames. In gradient-based models of DiffusionPDE
and D-Flow, we also noticed a peak in the FPD scores at an early stage, which should have been


Table 10: Generative metrics on the Stokes problem with the zero-shot superresolution setting to
















|200 × 200. The best results are highlighted in bold.|Col2|
|---|---|
|Dataset<br>Metric<br>ECI<br>ANP<br>ProbConserv<br>DiffusionPDE<br>D-Flow|CondFFM|
|Stokes IC<br>MMSE / 10_−_2<br>**0.274**<br>14.120<br>4.338<br>5.109<br>OOM<br>SMSE / 10_−_2<br>**0.515**<br>8.720<br>2.933<br>4.167<br>CE / 10_−_2<br>**0**<br>3.563<br>**0**<br>5.392<br>FPD<br>**2.647**<br>26.965<br>15.977<br>19.654|4.242<br>4.152<br>12.508<br>24.798|
|Stokes BC<br>MMSE / 10_−_2<br>**0.863**<br>2.819<br>5.223<br>1.610<br>OOM<br>SMSE / 10_−_2<br>**0.418**<br>2.870<br>1.491<br>1.768<br>CE / 10_−_2<br>**0**<br>29.168<br>**0**<br>0.068<br>FPD<br>**1.903**<br>5.282<br>2.021<br>3.830|4.669<br>2.706<br>21.567<br>9.504|


26


Published as a conference paper at ICLR 2025


easier for the model. As expected, CondFFM outperformed all zero-shot guidance models by a large
margin in such a complex task for almost all frames.


Figure 16: Per frame FPD score for the NS equation with IC fixed.


E.3 R EGRESSION T ASKS


E.3.1 U NCERTAINTY Q UANTIFICATION


For the uncertainty quantification tasks, the specific snapshot at the test time as described in Appendix B (following the same settings in (Hansen et al., 2023)) is plotted in Figure 17, with the generation mean and 3 times of the generation standard deviation as the confidence interval. Our results
share a similar trend described in (Hansen et al., 2023), in which they also observed more uncertainty
with the increasing difficulty from the heat equation, PME, to the Stefan problem. Specifically, the
solution in the Stefan problem has a shock in the function value, which leads to higher uncertainty
around the shock position. For other linear parts of the solution, our ECI sampling generation is able
to achieve quite good predictions with low variance and low uncertainty.


Figure 17: Uncertainty quantification visualization results for three instances of the Generalized
Porous Medium Equation (GPME). The 3 _σ_ confidence intervals are plotted.


E.3.2 N EURAL O PERATOR L EARNING


For NO learning tasks, we provide additional visualizations of the generation statistics and the reference FNO predictions in Figure 18 (Stokes problem) and 19 (NS equation). It can be demonstrated
that, though not directly trained on such regression tasks, our ECI sampling naturally produces
generation results with low variance in such scenarios. In comparison, other models often have a
variance scale similar to those in the generative tasks, making them less suitable for such zero-shot
regression tasks.


27


Published as a conference paper at ICLR 2025


Figure 18: Regression mean and standard deviation for the Stokes problem with both IC and BC
fixed. Note that the scaling range for the standard deviation is different from that in the generative
task.


Figure 19: Regression mean and standard deviation for the NS equation with the first 15 frames as
the constraint. Note that the scaling range for the standard deviation is different from that in the
generative task.


E.4 M ORE A BLATION S TUDIES


E.4.1 S TOKES P ROBLEM FPD


To provide a more thorough investigation into the impact of mixing iterations _M_ and re-sampling
interval _R_ on the final performance, we further calculate the FPD scores in Figure 20 for the two
settings for the Stokes problem in addition to Figure 4. The FPD scores are more consistent than
the MMSE and SMSE. For the Stokes IC problem with higher variance, generations with no resampling work the best, with the FPD scores peaking at 10 mixing iterations. For the Stokes BC
problem with lower variance, the best performance combination of hyperparameters has a diagonal
pattern with more mixing iterations pairing best with a smaller re-sampling interval. Such results
are also consistent with our heuristic suggestions on choosing the best _M_ and _R_ for different tasks
with different variances.


E.4.2 P RIOR G ENERATIVE M ODEL


We further provide the ablation studies on the impact of the quality of the prior generative model.
Intuitively, a stronger generative prior that better captures the underlying information of the PDE
system can be easier to guide. The results are demonstrated in Table 11, with checkpoints from
5k, 10k, and 20k training iterations. A clear trend of better guided-generation performance can be
observed in both IC and BC constraints. In this way, the quality of FFM indeed plays an important
role in our ECI sampling framework.


28


Published as a conference paper at ICLR 2025


Figure 20: FPD score of ECI sampling with different mixing iterations and re-sampling intervals in
the Stokes problem with IC (left) or BC (right) fixed.


Table 11: Generative metrics on the Stokes problem with different constraints using the different
FFM checkpoints with different training iterations.


Dataset Stokes IC Stokes BC


#iter 10k 15k 20k 10k 15k 20k


MMSE / 10 _[−]_ [2] 1.273 0.601 0.090 0.888 0.011 0.005
SMSE / 10 _[−]_ [2] 1.693 0.177 0.127 0.868 0.015 0.003

FPD 1.863 1.031 0.076 3.101 0.009 0.010


E.4.3 A MOUNT OF C ONSTRAINT I NFORMATION


We also provide additional ablation studies on ECI sampling behavior as a regression model. Specifically, we explore the regression losses when more information is provided in the constraint. Intuitively, serving as a deterministic regression model, the ECI generation should exhibit fewer errors
and less variance with more information provided. In our experimental setup, we fixed the number
of mixing iterations to 5 and the re-sampling interval to 1. To control the information in the constraint, we varied the number of initial frames fed into the model as the value constraint from 5 to
25. As our ECI sampling guarantees the exact satisfaction of the constraint, we instead calculated
the mean MSE and standard deviation MSE based on the unconstrained region for a fair comparison
between different settings. The losses are plotted in Figure 21, in which a clear and almost linear
trend in the log scale can be distinguished for both metrics as the number of constrained frames
increases. Similarly, the visualization of the mean error and the logarithm of the standard deviation
are provided in Figure 22. The model made more accurate predictions with less uncertainty as the
number of constrained frames increased. The regions with large mean errors also matched those
with large variance (uncertainty). In this sense, our ECI sampling provides a bonus to transform the
pre-trained unconditional FFM model into a decent zero-shot uncertainty quantification model with
potentially wider applications.


29


Published as a conference paper at ICLR 2025


Figure 21: Regression MMSE and SMSE for the NS equation with different numbers of frames
fixed as the constraints. Notice a clear and almost log-linear trend for the decrease in both metrics
as the number of constrained frames increases.


Figure 22: Regression error of the mean and logarithm of the standard deviation for the NS equation
with different numbers of frames fixed as the constraints. We use the same scale for plotting different
settings.


30


