## **Automated simulation-based design via multi-fidelity active learning and** **optimisation for laser direct drive implosions**

A. J. Crilly, [1, 2,][ a)] P. W. Moloney, [1] D. Shi, [1] and E. A. Ferdinandi [1]

1) _Centre for Inertial Fusion Studies, The Blackett Laboratory, Imperial College, London SW7 2AZ,_
_United Kingdom_
2) _I-X Centre for AI In Science, Imperial College London, White City Campus, 84 Wood Lane, London W12 0BZ,_
_United Kingdom_


The design of inertial fusion experiments is a complex task as driver energy must be delivered in a precise manner to a structured target to achieve a fast, but hydrodynamically stable, implosion. Radiation-hydrodynamics
simulation codes are an essential tool in this design process. However, multi-dimensional simulations that
capture hydrodynamic instabilities are more computationally expensive than optimistic, 1D, spherically symmetric simulations which are often the primary design tool. In this work, we develop a machine learning
framework that aims to effectively use information from a large number of 1D simulations to inform design in
the presence of hydrodynamic instabilities. We use an ensemble of neural network surrogate models trained
on both 1D and 2D data to capture the space of good designs, i.e. those that are robust to hydrodynamic
instabilities. We use this surrogate to perform Bayesian optimisation to find optimal designs for a 25 kJ laser
driver. We perform hydrodynamic scaling on these designs to confirm the achievement of high gain for a 2
MJ laser driver, using 2D simulations including alpha heating effects.



**I.** **INTRODUCTION**


Inertial confinement fusion (ICF) seeks to compress
and heat deuterium–tritium (DT) fuel to thermonuclear
conditions by coupling driver energy into a spherically
imploding shell [1,2] . In laser direct drive [3] (LDD), multiple laser beams directly illuminate a capsule, generating an ablative rocket that accelerates the shell inward. Achieving ignition [4–6] and appreciable burn propagation requires a careful orchestration of target dimensions and laser pulse history to control shock timing, adiabat, and implosion velocity, while mitigating hydrodynamic instabilities [7,8] . These competing considerations
create a high-dimensional, strongly coupled design space
in which small changes to one parameter often necessitate
compensating changes in other design parameters.
Radiation–hydrodynamics (rad-hydro) simulations are
indispensable for navigating this space, but their computational cost grows steeply with dimensionality. Onedimensional (1D) simulations enable large surveys and
provide an upper bound on performance through perfect
spherical symmetry, whereas multi-dimensional (2D/3D)
simulations are needed to capture Rayleigh–Taylor instability growth that degrades fusion yield [9–11] . However,
the computational cost of 2D calculations can exceed that
of 1D by two orders of magnitude, and 3D simulations
are more expensive still.
These characteristics motivate multi-fidelity (1D, 2D
or 3D) and active-learning machine learning strategies
that extract as much information as possible from inexpensive low-dimensional simulations while judiciously deploying costly high-dimensional simulations. Surrogate
models can learn mappings from design variables to outcomes and provide predictive uncertainties for Bayesian


a) [Electronic mail: ac116@ic.ac.uk](mailto:ac116@ic.ac.uk)



optimisation [12] . Multi-fidelity extensions exploit correlations across fidelities to accelerate convergence [13,14] .
However, standard multi-fidelity formulations often assume modest computational cost ratios between the fidelities and noise models that are uniform across the fidelities. Thus for multi-fidelity ICF design, there is a
need for methods that (i) respect extreme cost disparities,
(ii) remain robust to non-uniform, fidelity-dependent
noise, and (iii) integrate seamlessly with pre-existing radhydro codes; which require High Performance Computing
(HPC) clusters.
In this work we develop an automated, simulationbased design framework that couples multi-fidelity surrogate modelling with active learning and Bayesian optimisation for LDD implosions. Our contributions are:


1. A 1D and 2D radiation-hydrodynamics simulation
pipeline allowing straightforward coupling to machine learning (ML) models. For 2D simulations
we include fixed (beam-mode) and randomised (ablator surface structure) perturbation sources to reflect shot-to-shot variability.


2. Multi-fidelity surrogate models that transfer information from large 1D ensembles to data-scarce
2D. These include a neural network ensemble with
transfer learning and calibrated predictive uncertainties, alongside Gaussian process baselines.


3. An active-learning scheme based on probabilistic threshold sampling that concentrates expensive simulations in promising regions of the design

space.


4. Bayesian optimisation at fixed fidelity using multifidelity-trained surrogates to find optimal designs
in both 1D and 2D. We define optimality via a
physics-informed objective that blends an ignition


criterion [15,16], _χ_ no _α_, with post-ignition burn propagation via areal density [17] _ρR_, enabling sub-scale
optimisation that targets ignition and high gain at
a larger hydrodynamic scale.


We demonstrate the framework on an OMEGArelevant LDD problem with the OMEGA [18] 60-beam geometry, 25kJ driver and an eight-parameter design space
(laser picket/foot/rise and layered CH/DT target). We
first amass a large 1D database to train surrogates and
narrow down the search domain. We then conduct a
targeted 2D campaign informed by active learning. We
find that transferring structure learned in 1D dramatically reduces the number of 2D simulations required to
locate robust designs. Importantly, we show that the 2Doptimised design exhibits substantially improved hydrodynamic stability compared to the 1D-optimised counterpart at the same energy scale. Finally, we assess performance under hydrodynamic scaling [19] to 2MJ (National
Ignition Facility [20] -scale) with both burn-off (no alpha
heating) and burn-on (with alpha heating) simulations
to gauge ignition margin and post-ignition burn.
The remainder of the paper is organised as follows.
Section II presents the methodology, defining the design
space, objective, and simulation models and detailing the
surrogate, active-learning, and optimisation components.
Section III presents the optimisation studies in 1D and
2D. Section IV reports the performance of the optimal
designs at 25kJ and at 2MJ with burn-off/on analyses.
Future work and conclusions are then discussed.


**II.** **METHODOLOGY**


**A.** **Design**


Design of laser direct drive ICF implosions involves
the prescription of a laser pulse (incident laser power as
a function of time) and a target (spherical layered target
dimensions) [3] . Each of the design parameters (both individually and in combination) are strongly linked to behaviours of the implosion. By tuning of the parameters,
one can alter the implosion design (implosion velocity,
shell mass, etc.) to achieve the desired conditions at stagnation. However, this is no simple task due to the complex non-linear relationships created by the many physicial processes inherent to ICF. Human designers must
consider the qualitative effect of each design decision and
use simulations as a tool to quantify these effects. In
this work we look to develop a framework that can learn
these relationships and thus autonomously move towards
an optimal design.
To proceed we must parameterise the laser pulse and
target design. The full LDD design space is very high
dimensional and therefore we look to reduce the problem down to _<_ 10 parameters. Firstly, we consider LDD
implosions with a 60-beam 25 kJ laser driver. This is relevant to the OMEGA laser facility [18], allowing comparisons between the designs presented in this work with the



2


large OMEGA experimental and simulation database.
For the pulse design, we consider a single picket, a foot
and power law rise to peak power ( _P_ peak = 25 TW). For
the power law rise we use a simplified form of the functional prescription in Gopalaswamy _et al._ [21] :


_P_ ( _t_ ) = _P_ picket ( _t_ ) + _P_ main ( _t_ ) _,_ (1a)



Where _σ_ _t_ = 25 ps and _β_ = 9/5. For the target, we use an
undoped CH ablator layer, a DT ice layer and a central
DT gas region; parameterised as two thicknesses and an
outer radius. This laser and target design has a total of
8 free parameters. We must also pick acceptable ranges
for these parameters, and for the target parameters we
must pick discrete increments such that the initial target
fits precisely on an Eulerian grid. Table I details the design space, note that all pulse duration parameters were
capped at 1 ns to prevent excessively long pulse durations.


Variable (units) Min Max Step size

_P_ picket (TW) 0 25     _P_ foot (TW) 0 12.5      _t_ delay (ps) 50 1000          _t_ foot (ps) 50 1000          _t_ rise (ps) 50 1000          _R_ outer (um) 400 650 1
∆ ablator (um) 2 20 0.5
∆ ice (um) 10 100 0.5


TABLE I: Design space used in this work (8 total
parameters, 5 laser, 3 target). See equations 1a-f for
laser pulse parameterisation.


With a design space chosen, one must define an objective function. An objective function is used to quantify
progress towards some optimal state, where the objective function is maximised. Ultimately, the goal of ICF
is to optimise fusion yield, however for sub-scale implosions, which cannot ignite, a different choice of objective
is often needed. In the literature, it is common for subignition scale implosions to optimise an ignition metric
which can be hydrodynamically scaled [19] to higher laser
energies, such as _χ_ _no α_ [15,16] . Once an ICF target has



� 2 [�]



_P_ picket ( _t_ ) = _P_ picket exp



�



_−_ [1]

2



_t −_ 3 _σ_ _t_
� _σ_ _t_



(1b)



erf( _x_ )+1
_P_ foot



_P_ main ( _t_ ) =







_P_ foot 2 for _τ <_ 0

_P_ foot [1 _−_ _ξτ_ ] _[−][β]_ for 0 _< τ <_ 1
_P_ peak for _τ >_ 1 & _t < t_ end



_,_



(1c)

_τ_ = _[t][ −]_ [(] _[t]_ [foot] [ +] _[ t]_ [dela][y] [ + 3] _[σ]_ _[t]_ [)] _,_ (1d)

_t_ rise



_x_ = _[t][ −]_ [(] _[t]_ [dela][y] [ + 3] _[σ]_ _[t]_ [)] _,_ (1e)
~~_√_~~ 2 _σ_ _t_



_ξ_ = 1 _−_ _P_ peak
� _P_ foot



_−_ [1]

_β_
_,_ (1f)

�


ignited, then burn propagation into the fuel mass begins. For burn propagation, the metric of success is the
areal density ( _ρR_ ). This can be seen through Fraley’s
formula [17] for the fraction of DT fuel, Φ, which undergoes fusion:


_ρR_
Φ = (2)

6 _._ 3 g _/_ cm [2] + _ρR_


We can therefore aim to design implosions at a lower energy scale which would ignite and achieve high gain at a
higher energy scale. The ICF optimisation problem can
be broken up into sub-problems to tune aspects of the
implosion to achieve the overarching goal [21] . However,
we will focus on the global optimisation problem (as in
Ref. [22] ) and therefore need only provide a single objective function. Guided by the need for ignition and burn
propagation at the higher energy scale, we devised an
objective which aims to smoothly combine criteria for ignition and burn propagation into a single scalar function:


_Y_ = min( _χ_ _S,no α_ _,_ 1) (3a)

+ [1] ˆ max( _ρR −_ _ρR_ _χ_ =1 _,_ 0) _,_

_ρR_



0 _._ 34

_,_
�

(3b)



0 _._ 61

1 mg

0 _._ 12 _[Y]_ _[DT]_
� � 10 [16] _M_ stag



3


**B.** **Radiation-hydrodynamics simulation**


The design code used in this work was Chimera,
a fixed-grid Eulerian radiation magnetohydrodynamics
code, details of ICF implosion modelling with Chimera
can be found in the literature [11,23–27] . To model the laser
drive, Chimera was coupled to SOLAS, a 3D laser raytracing model, the details of which will be given in a future publication. Finally, a number of additional code developments were employed to speed up simulation time,
namely implicit methods for both flux-limited thermal
conduction and multi-group flux-limited radiation diffusion, physics for which Chimera has traditionally used
explicit methods [27,28] . The atomic physics model SpK [27]

was used to generate multi-group opacity tables. To facilitate automation, control has been shifted to external
input decks and standard decks developed for laser direct
drive applications.
The purpose of 2D simulations is to capture hydrodynamic instability growth which degrades performance
over the perfect spherical symmetry of 1D. The laser
beam geometry introduces a seed for this instability
growth in the form of low mode intensity variations on
the capsule surface, known as beam mode [29,30] . Laser
speckle operates at much shorter lengthscales than beam
mode, introducing high mode sources of instability [31–33] .
Capsule defects, like surface roughness and voids, are
also key seeds for instability growth for both low and
high mode [34] . To capture some of these effects in our 2D
simulations we (a) used the OMEGA beam port geometry to introduce beam mode and (b) introduced density perturbations on the outer 2.5um of the CH ablator.
These density perturbations could have been kept constant across different simulations; however, we opted to
reseed the random perturbations to introduce noise to
the simulated outputs. This gives better correspondence
to the reality of shot-to-shot variation in experiment and
ensures our methods are robust to such noise. The level
of perturbation introduced led to _∼_ 1% RMS areal density
variations for the initial conditions.

For multi-dimensional Eulerian simulations of spherical implosions it is beneficial to run in spherical geometry
for as long as possible to prevent non-spherical grid-based
artifacts. However, multi-dimensional regular spherical
grids have small cell sizes towards r = 0, requiring small
numerical time steps for stability. A restart to other geometries can be used to eliminate this restrictive time
step requirement. For this work, an automatic restarting
scheme was devised whereby initially 2D spherical ( _r_ _−_ _θ_ )
geometry was run up until the first shock approached r
= 0. At this time, the variables for restart are dumped
to disk and a restart is initialised onto a 2D cylindrical
( _r −_ _z_ ) grid. The outer extent of the cylindrical grid
was found by identifying the outermost radius at a few
percent of critical density.
The above code optimisations allowed run times of approximately 1 and 200 core hours for 1D and 2Ds respectively. For OMEGA scale implosions, a radial resolution



_χ_ _S,no α_ = _S_



_ρR_
� 1g _/_ cm [2]



1 mg

_ρR_ _χ_ =1 = �0 _._ 12 _[Y]_ 10 _[DT]_ [16] _M_ stag



_ρR_ _χ_ =1 = �0 _._ 12 _[Y]_ 10 _[DT]_ [16]



_−_ 1 _._ 64
g _/_ cm [2] _,_ (3c)
�



ˆ
_ρR_ = 0 _._ 1 g _/_ cm [2] _,_ (3d)



_E_ L _,_ scaledup
_S_ =
� _E_ L



3
(3e)

� [1]



Where _S_ is the hydrodynamic scale factor, _Y_ _DT_ is the DT
yield, and _M_ stag is the stagnated mass. The objective
function aims to achieve ignition at scale _S_ (occurring
when [16] _χ_ _S,no α_ _∼_ 1) before switching focus to increasing
areal density above that required for ignition. In this
work, we will consider scaling laser driver energies from
25 kJ (OMEGA scale) to 2 MJ (NIF scale). This leads
to a scale factor _S ∼_ 4 _._ 3.
To calculate the quantities required for _χ_ _no α_
and _ρR_ _χ_ =1, we define the following concrete multidimensional definitions for implementation within the
radiation-hydrodynamics simulations (in any dimensionality):



_ρ_ ( _r,_ Ω _, t_ ) _d_ Ω _dr,_ (4a)
�


_ρ_ ( _⃗r, t_ ) _d_ [3] _r,_ (4b)

� _V_ _⟨P ⟩e−_ 2



1
_ρR_ =
_Y_ _DT_


1
_M_ stag = _Y_ _DT_


1
_⟨P_ _⟩_ =
_Y_ _DT_



_dt_ _[dY]_ _[DT]_
� _dt_

_dt_ _[dY]_ _[DT]_
� _dt_



� _dt_ � _P_ ( _⃗r, t_ ) _[d]_ _dtdV_ [2] _[Y]_ _[DT]_ _[d]_ [3] _[r]_ (4c)



Where _V_ _⟨P ⟩e_ _−_ 2 is the volume of fuel with pressures exceeding 2 e-foldings of the burn-averaged pressure, _⟨P_ _⟩_ .


of 500nm was used and 256 polar cells in 2D. The simulations were run on 32 and 128 CPU cores for 1D and
2D respectively.


**C.** **Surrogate modelling**


Surrogate models are a key component of machine
learning and optimisation methods. These models use
sample data to learn an approximate functional relationship between the inputs ( _X_ ) and the outputs ( _Y_ ).
Once trained on this data, the surrogate model is much
cheaper to sample from than creating new sample data,
i.e. cheaper than running a rad-hydro simulation. Additionally, surrogate models which provide an uncertainty
estimate in their prediction are particular useful, allowing Bayesian optimisation [12] .


**1.** **Gaussian Processes**


Gaussian processes (GPs) are a common surrogate
model for optimisation purposes. There have been a
number of works using GPs as surrogate models in ICF
modelling and optimisation [21,35] .
For introductory purposes, let us consider a simplified example involving two pairs of variables _x_ 1 _, y_ 1 and
_x_ 2 _, y_ 2 . We expect some relationship between _y_ and _x_,
with noise, such that _y_ = _f_ ( _x_ ) + _ϵ_ . However, we have no
functional form for _f_ ( _x_ ). To proceed, we will assert that
the joint distribution of _y_ 1 and _y_ 2 is Gaussian with mean
0 and equal variances _σ_ [2] . Now let us consider the conditional probability of _P_ ( _y_ 1 _|y_ 2 ), given that _y_ 2 now has
some known value, say _y_ _[∗]_ :


_P_ ( _y_ 1 _|y_ 2 = _y_ _[∗]_ ) _∼N_ ( _µ_ 1 _, σ_ 1 ) (5a)
_µ_ 1 = _ρy_ _[∗]_ _,_ (5b)



4


single-fidelity settings (all the training data comes exclusively 1D _or_ 2D simulations), we used the Matern
kernel [37] with anisotropic lengthscales (i.e. a lengthscale
per design parameter):



_k_ _ν_ ( **x** _,_ **x** _[′]_ ) = [2] [1] _[−][ν]_

Γ( _ν_ )



� _√_



_ν_
2 _ν ρ_ _K_ _ν_ _√_
� �



2 _ν ρ_ _,_ (7a)
�



� 1 _/_ 2



_ρ_ =



_d_
�
� _i_ =1



( _x_ _i_ _−_ _x_ _[′]_ _i_ [)] [2]

_ℓ_ [2] _i_



_._ (7b)



_σ_ 1 = �



1 _−_ _ρ_ [2] _σ,_ (5c)



Where _l_ _i_ is the _i_ -th lengthscale, _ν_ is the smoothness parameter (we use 5/2), and _K_ _ν_ ( _x_ ) is the modified Bessel
function of the second kind of order _ν_ . Fitting to the
training data yields best fit values for the lengthscales _l_
and signal variance _σ_ [2] .
However, GPs have some key limitations, a key issue
being their poor scaling with training data size. For
_O_ (10k) samples, training the GP can become excessively
slow. In practice, we found training a GP surrogate with
close to the full dataset became slower than performing
a Chimera simulation, even when using a GPU for GP

_∼_
training ( 10 min training time). This was particularly
problematic for multi-fidelity (i.e. using 1D _and_ 2D data)
GPs where we modified the kernel to introduce discrete
fidelity dependence. In these scenarios, we needed an alternative surrogate model which scaled well with training
data volume.


**2.** **Ensemble of Neural Networks**


Neural networks (NNs) are universal function approximators and can therefore act as surrogate models, with
previous examples already in the field of ICF [39–41] . Similar to GPs, NNs use training data to learn a functional
relationship between the inputs ( _X_ ) and the outputs ( _Y_ ).
In this work, we used a multi-layer perceptron (MLP) NN
architecture which involves a sequence of layers of fullyconnected “neurons”, the basic unit of a MLP. Neurons
apply a linear transformation to the incoming data before applying a non-linear activation function. Designing
a good MLP architecture involves the tuning of a modest number of hyperparameters, for example: number of
layers, number of neurons per layer, activation function,
learning rate, and regularisation parameters.
Unlike Gaussian Processes, NNs do not inherently provide any uncertainty estimate in their prediction of _Y_
given _X_ . A common method to model uncertainty with
NNs is the ensemble approach [42] . For this, a number of
NNs are trained using different random splits of the training data. This causes their predictions to vary, giving a
range of results. This is used to quantify the uncertainty
in the prediction. To ensure meaningful uncertainty predictions, we performed a calibration scaling on the ensemble predictions, as detailed in Appendix A.
An ensemble size of 25 was used for the 1D and 2D
ensemble models. The MLP model was written using



where _ρ_ is the correlation coefficient between _y_ 1 and _y_ 2 .
Within this toy model, our conditional mean and variance prediction on _y_ 1 depends only on the degree of correlation between _y_ 1 and _y_ 2 . Gaussian Processes make a
statement on this degree of correlation by proposing that:


_ρ ∝_ _k_ ( _x_ 1 _, x_ 2 ) _,_ (6)


Where _k_ ( _x_ 1 _, x_ 2 ) is the kernel function. In other words,
GPs say (depending on the choice of kernel) that data
points’ proximity in _x_ dictates their level of correlation
in _y_ . In practice, GPs handle arbitrarily many inputs
(often _x ∈R_ _[d]_ ) and produce real-valued function outputs.
The hyperparameters of the GP kernel, as well as _σ_, are
found by likelihood maximisation over the observed data,
which can be noisy. We direct the interested reader to
references which give a full background on GPs [36,37] .
For our purposes, we must make choices on the GP
kernel [38] and then fit the GP to the training data. For


PyTorch [43], which allowed its easy integration in the
Bayesian optimisation framework BOTorch [44], see Section II E. A combination of manual and automated tuning
(number of layers, neurons, batch size, learning rate) was
used to find the final architecture, shown in Fig. 1. Training of each member of the ensemble used the AdamW
gradient-based optimiser [45] to minimise the mean squared
error between prediction and truth values in the training
data. Weight-decay (value of 0.05) was used to regularise
the model and prevent over-fitting to the training data.
A learning rate schedule (starting at 0.01 and halving every 20 epochs) was found that allowed rapid training in
80-100 epochs (passes over the full training data set) for
the 1D surrogate.


**(a) MLP Architecture**


Input layer (8)


(32)



(48)



_Frozen for_





_layers_

(48)


(32)


Output layer (1)


**(b) Training, Random Forest Baseline & Transfer Learning**


**(i)** **(ii)** **(iii)**


FIG. 1. (a) Shows the final architecture chosen for the multilayer perceptron surrogate model. Each ensemble model consists of 25 instantiations of this model. The diagram shows
the number of layers, neurons, activation function and transfer learning freezing strategy. (b) Three figures showing the
performance of this surrogate model . (i) Loss history for
training of a single 1D surrogate NN. (ii) Comparison on prediction performance between a trained NN surrogate and a
random forest (RF) [46] regressor on the same validation data.
The _R_ [2] metric denotes the square of the Pearson correlation coefficient. (iii) Comparison of the ensemble model predictions of the 2D dataset (described later) before and after
transfer learning has occurred.


Another key advantage of NN surrogate models is the
ease of transfer learning [47] . In the context of this work,
this means an ensemble of NN surrogates trained on 1D
simulation data can be transferred to predict the out


5


come of 2D simulations, even with limited 2D data. The
transfer learning process proceeds as follows:


1. Train an ensemble of NN surrogates on randomised
train-test-validation (60:20:20) data splits of the 1D
data set


2. The first N layers of the NN surrogates are ‘frozen’,
such that their trained parameters will not change
in subsequent training


3. Perform transfer learning by re-training the ensemble of NN surrogates on randomised train-test
(70:30) data splits of the 2D data set


In this manner, information from the 1D (correlations
between input parameters, useful non-linear embeddings
of the inputs, etc.) will be transferred to a 2D surrogate.
In practice, it was found the freezing all but the last 2
layers produced good results when tested on validation
data.


**D.** **Active learning**


The input parameters for laser direct drive (target dimensions, laser pulse parameters) are highly correlated in
good designs. For example, shocks launched by the laser
are optimal if they can be merged together precisely [48],
this correlates the thicknesses of target layers to laser parameters like picket powers and durations. This correlation leads to good designs residing within small, complex
volumes in the design/input space. To enable effective
optimisation, we wish to populate these volumes with
more sample data rather than uniformly filling the input
space [21] . We opted for an active learning [49] approach to
achieve this.

For active learning, we will use a surrogate model
trained on uniformly sampled data to narrow down the
search space for good designs. Based on the surrogate
model predictions, new sample data will be taken (via
rad-hydro sims) and used to update the surrogate model.
In this way, we can increase the sample frequency within
regions we care about, while training the surrogate simultaneously. This also has the additional benefit that
batches of candidate points with high objective function
values can be pulled from the surrogate model and run
concurrently, i.e. in batches. In this work, we employed
the active learning algorithm described in Algorithm 1.
This shares features with canonical level set estimation [50]

algorithms.
Where Φ ( _X_ ) is the cumulative normal distribution and
_U_ ( _a, b_ ) is the random uniform distribution between _a_ and
_b_ . There are a number of hyper-parameters to be chosen, _P_ min and _Y_ [ˆ] threshold . It was found that _P_ min values between 0.25 and 0.5 gave a good balance between
exploration and exploitation. The threshold is chosen
such that designs with expected outcomes better than
the threshold are searched for.


**Algorithm 1** Active Learning: Probabilistic Threshold
Sampling

**Require:** Number of learning runs _N_ runs, batch size _N_ batch,
retrain interval _r_, threshold _Y_ [ˆ] threshold
**for** _i_ = 1 to _N_ runs **do**

**if** _i · N_ batch mod _r_ = 0 **then**

Retrain surrogate model on _X, Y_
**end if**

_M ←_ 0

**while** _M < N_ batch **do**

Generate _s_ random draws _u_ _j_ _∼U_ ( _P_ min _,_ 1)
Generate _s_ random input points _X_ _j_
Predict mean _µ_ _j_ and std _σ_ _j_ from surrogate



ˆ
Compute _P_ ( _y_ _j_ _>_ _Y_ [ˆ] threshold ) = 1 _−_ Φ � _Y_ threshold _σ_ _j_ _−µ_ _j_

Select _X_ cand _._ from _X_ _j_ if _P_ _j_ _> u_ _j_
_M ←_ number of points _X_ cand _._
If _M < N_ batch : Double random sample size _s_
**end while**
Select _N_ batch inputs ( _X_ next ) at random from _X_ cand _._
( _P_ next _, Y_ next ) _←_ Chimera( _X_ next )
Add _X_ next _, P_ next _, Y_ next to dataset
**end for**


**E.** **Bayesian optimisation**



�



6


It should be noted that we have not taken a fully
multi-fidelity Bayesian optimisation approach [52], even if
we are using multi-fidelity surrogates. This is because of

_∼_
the large difference in computational cost ( 200x) between our fidelities (1D or 2D rad-hydro sims). In traditional multi-fidelity Bayesian optimisation, the optimiser
chooses the next candidate _X as well as_ the fidelity based
on the information gain per unit cost. Due to our cost disparity, 2Ds are run very sparingly so cost weighting would
need to be used. Instead, we opted to run Bayesian optimisation at a fixed fidelity but using surrogates trained
on both fidelities. This allows the transfer of information
between fidelities via the surrogate but updates to the
database are strictly within a single fidelity.


**F.** **Orchestration**


An orchestration framework is required to efficiently
couple together the machine learning and simulator models described above. The job of orchestration software is
to: convert candidate sample points into valid simulator inputs; execute and wait for batches of simulations
to run within a HPC environment; post-process the simulations for valid training data; re-train the surrogate
model and finally query the explorer/optimiser for new
sample points. Fig. 2 summarises this workflow. While



Initial sampling and active learning was used to build
a comprehensive data set to train a sufficiently accurate
surrogate model to be used in Bayesian optimisation.
Bayesian optimisation defines an optimisation problem in
terms of maximising an acquisition function, which itself
is a function of the surrogate model mean and uncertainty
predictions, _µ_ ( _X_ ) and _σ_ ( _X_ ) respectively. This optimisation problem over the acquisition function is computationally cheaper than the ‘true’ optimisation problem
over the simulator outputs.
We used the same Bayesian optimisation strategy
throughout this work. Firstly, we restrict to simulation
batch sizes of 1 i.e. sequential optimisation. This allows
for the most well defined acquisition function and optimisation problem. Secondly, we used the log Expected
Improvement [51] acquisition function, which is defined as:


LEI( _X_ ) = log [ _σ_ ( _X_ ) ( _Z_ Φ( _Z_ ) + _ϕ_ ( _Z_ ))] (8a)


[)]
_Z_ = _[µ]_ [(] _[X]_ [)] _[ −]_ [max][(] _[Y]_ (8b)

_σ_ ( _X_ )


Where Φ ( _X_ ) and _ϕ_ ( _X_ ) are the cumulative and probability distribution functions of the standard normal distribution. This acquisition function is maximised w.r.t.
_X_ using gradient-based methods. A new simulation is
then ran at the optimal _X_ found. This produces a new
_X, Y_ pair for the dataset, the surrogate is retrained and
the process is repeated. The numerical solution to this
optimisation problem was performed by the BOTorch
library [44] and the coupling to the simulator is handled
by software described in the following section.





























FIG. 2. A schematic showing the ML and simulation workflow
used in this work. The orchestration software, _mille-feuille_,
ensures the new inputs suggested by random sampling or an
optimiser are correctly converted to input decks for the simulator. It also handles the coupling of the training database
and the surrogate used in optimisation.


the specifics of this workflow are simulator and surrogate model dependent, the software package developed
for this purpose, _mille-feuille_, is open-source [53] and aims
to abstract this workflow process.


7



Creating automated design programs in this way allows
multiple levels of parallelism to be exploited. At the lowest level, the simulator is parallelised by MPI with load
balanced domain decomposition. Within the automated
design process, batches of the simulator can be launched
simultaneously to use all the allocated HPC resource. At
the top level, multiple instances of the automated design
program can be launched as an array, learning from the
same centralised data sets.


**III.** **OPTIMISATION STUDY**


**A.** **1D design**


An extensive initial sampling 1D simulation ensemble was run due to the relatively low cost of 1D simulations. The input domain defined in Table I was
transformed to the unit-hypercube and the quasi-random
Sobol sequence [54] was used as a space filling design [36] . Of
order 12.5k 1D simulations were ran based on this Sobol
design. As expected the vast majority (97%) of these
simulations represented poor designs (quantified as objective _Y <_ 1) which are unlikely to ignite at the 2MJ
scale. However, of those simulations with _Y >_ 1 the
range of the input parameters still covered the whole design space. This is because the inputs for this dataset of
“good” designs are correlated. For example, ice and ablator thickness are anti-correlated with each other (-0.6
Pearson coefficient) and outer radius (-0.16 and -0.35 respectively), suggesting good designs lie within a certain
target mass range. For the laser pulse, picket power is
positively correlated with foot power (+0.35) and negatively with the delay between picket and foot (-0.3). This
reflects the importance of the front end of the pulse in
shock timing [48] . The distribution of objective values from
this initial sampling can be seen in Fig. 3.
The comprehensive initial sampling allowed the training of a single fidelity GP surrogate for active learning.
Since only 3% of the initial sampling lay within the region of interest ( _Y >_ 1), active learning with a threshold of 1 and _P_ _min_ = 0.25 was used to find _∼_ 1k points
within this region of interest. As expected, the active
learning sample shows similar statistical correlations as
the thresholded initial Sobol samples. The active learning step increased the population of high objective function observations, increasing surrogate model confidence
in this region.
Finally, Bayesian optimisation was performed to find
the optimal design in 1D within the provided design
space. Two approaches were taken. Firstly, a GP surrogate model was used, however training time was becoming obstructive due to the large 1D dataset. For the
GP, the training data was downselected by reducing the
design space to encompass all points with _Y >_ 1 _._ 5, this
reduced the number of training samples from _∼_ 14k to _∼_
5k. Secondly, the NN ensemble was used, for which the
whole 1D dataset was kept. Both surrogates were used



FIG. 3. (Top) History of objective values obtained
from 1D simulations, points are coloured by which sampling/optimisation method created them. (Bottom) A histogram of objective values over the full 1D database, note the
log y scale.


to optimise Log Expected Improvement. All but the last
two layers of the NNs were frozen for retraining during
the optimisation loop. The optimisations with the different surrogates converged towards similar optimal designs
( _Y_ values within 0.1 of each other), however it was the
NN surrogate which found the highest performing design
after _O_ (10) new samples.


**B.** **2D design**


It is evident that any target optimisation using only
1D simulations will produce optimistic designs, as the
designs do not need to be robust to hydrodynamic instabilities to achieve high performance in 1D. However,
there are great benefits to transferring some of the information learned from 1D simulation ensembles to optimisation using multi-dimensional radiation hydrodynamics
simulation. In particular, 1D results represent the upper
limit of performance for 2D calculations. Therefore, by
transferring information about the design space from 1D
to 2D we can reduce the search space drastically.
Therefore, our 2D design process proceeded as follows.
A subset of the inputs from 1D active learning samples were re-run as 2D simulations, we will refer to this
as our “transfer sample”. This transfer sample allowed
the training of 2D surrogate models. While both multifidelity GPs and NN ensembles were tested, it was found
that the NN ensembles performed equally well with considerably lower training times. Trained on the 1D dataset
and the 2D transfer samples, surrogate model predictions
for both 1D and 2D were compared for inputs ( _X_ values)






8


that the understanding of the design space found in 1D
is maintained in the transfer learning, in particular what
regions of design space to avoid due to low performance.







FIG. 4. Trained 1D and 2D neural network ensemble surrogate model predictions of the objective function for input
values from the 1D database. Transfer learning for the 2D surrogate only makes use of the transferred samples as a training

set.


in the 1D dataset. In Fig. 4, we can see the 2D surrogate has learnt from the transfer sample that performance is degraded relative to 1D. It is also observed that
the degradation is a strong function of input parameters, particularly ice thickness for these simulations. It is
worth repeating that the 2D data also is inherently noisy
as different random surface perturbations are applied for
each simulation. This is reflected in the large surrogate
uncertainty shown in the error bars, noting that these uncertainties have been calibrated using the method shown
in Appendix A.
Approximately 128 2D simulations were run within an
active learning loop to explore the space of good 2D designs. For the active learning, a threshold objective value
of 2 and _P_ _min_ of 0.5 were used, aiming to exploit the
space rather than to explore. Fig. 5 shows the new samples created by the active learning method. This clearly
shows the transfer learning has been effective in narrowing down the search space for 2D designs based on the 1D
dataset. The 2D surrogate directs samples towards large
objective values and shows no examples of poor design
choices (lowest objective value _∼_ 0 _._ 9). The transfer sample does not contain poor 1D designs, _Y <_ 1, so it is clear







FIG. 5. (Top) History of objective values obtained
from 2D simulations, points are coloured by which sampling/optimisation method created them. (Bottom) A histogram of objective values over the full 2D database, note the
log y scale.


Finally, Bayesian optimisation was performed using
the 2D NN ensemble surrogate only, with transfer learning performed using all available 1D and 2D data. With
a small number of runs, the algorithm found a small region of input space with robust 2D performance, with a
maximal objective function of approximately 3 _._ 0, to be
compared to 3 _._ 5 in 1D.


**IV.** **RESULTS**


In the following two sections we will provide a detailed
examination of the performance of the optimal designs
at both the design scale (25 kJ) and scaled up (2 MJ).
Table II summarises the key performance parameters for
an overview of the design performance.


**A.** **Performance of optimal designs at 25 kJ scale**


We will first consider the result of the 1D and 2D design optimisation at the design energy scale of 25 kJ.
We will assess the design performance using the objective function, Eq. (3a), as well as key ICF performance
parameters [55] (DT fusion yield, ion temperature, implosion velocity, etc.). The optimal design parameters are
given in Table III and the key performance parameters
are summarised in Table II.


9



|1D/2D|Design|χS,no α|Y|DT n Yield 14|⟨ρR⟩(g/cm2)|⟨Ti⟩(keV)|) vimp (km/s) B|
|---|---|---|---|---|---|---|---|
|1D<br>2D|_O_1_D_<br>_O_2_D_<br>_O_1_D_<br>_O_2_D_|2.0<br>1.85<br>1.65<br>1.72|3.52<br>3.40<br>2.02<br>3.05|5.49_ ×_ 10~~14~~<br>4.10_ ×_ 1014<br>2.29_ ×_ 1014<br>3.50_ ×_ 1014<br>|0.370<br>0.378<br>0.266<br>0.348|4.35<br>4.16<br>4.22<br>4.08|430<br>416<br>434<br>416|
|2D|_O_1_D_<br>_O_2_D_|-<br>-|-<br>-|5.86_ ×_ 10~~16~~<br>1.02_ ×_ 1017<br>|1.20<br>1.46|4.84<br>4.85|433<br>416|


2 MJ (Burn On) 2D _O_ 1 _D_ - - 1.02 _×_ 10 ~~[18]~~ 1.00 7.89 433 15.92
_O_ 2 _D_         -         - 2.24 _×_ 10 [19] 1.16 18.18 416 15.12


TABLE II: Performance of the optimal designs at different laser energy scales. Note that full hydrodynamic
equivalency re-tuning was not performed when hydro scaled to 2 MJ. Burn off/on refers to simulations
excluding/including alpha particle heating.



Variable (units) _O_ 1 _D_ _O_ 2 _D_

_P_ picket (TW) 3.19 4.53
_P_ foot (TW) 0.65 0.80
_t_ delay (ps) 806 757
_t_ foot (ps) 782 668
_t_ rise (ps) 1000 995
_R_ outer (um) 436 429
∆ ablator (um) 9.5 9.5
∆ ice (um) 70.5 76.0


TABLE III: Optimal designs at the 25 kJ scale derived
from 1D ( _O_ 1 _D_ ) and 2D ( _O_ 2 _D_ ) simulation based
optimisation.


The relative differences between the 1D and 2D optimal designs (denoted _O_ 1 _D_ and _O_ 2 _D_ respectively) give rise
to only small differences in performance in 1D, with areal
densities within 2% of each other, with the _O_ 1 _D_ design
having a higher _χ_ _S,_ no _α_ value. To understand these differences in the designs we will look at important implosion
characteristics related to both 1D performance and hydrodynamic stability. By inspecting the pressure gradient shown in Fig. 6, we can see the optimiser achieves a


|R<br>shell<br>IFAR|Col2|Col3|Col4|
|---|---|---|---|
|||||




is worth noting that this is not specifically optimised
for, but arises when maximising our chosen objective.
This allows these designs to achieve high areal densities
(370 - 380 mg/cm [2] ). These areal densities are considerably higher than has been achieved in experiment at
OMEGA [56] (≲ 250 mg/cm [2] ), pointing to the optimism
of 1D design calculations.
From the 1D simulations, we can also look at key metrics of hydrodynamic stability. These are the inflight
aspect ratio [57] (or IFAR) and the minimum adiabat [2] of
the DT fuel, _α_ _DT_ . These stability metrics are plotted
as a function of time in Fig. 7. Higher IFAR means the




















|Time (ns)|Col2|Col3|Col4|
|---|---|---|---|
|O1D<br>O2D|O1D<br>O2D|O1D<br>O2D|O1D<br>O2D|
|||||

|Log|| |<br>dP<br>dr|O<br>1D|
|---|---|---|
|Log|~~d~~|~~d~~|
||||
||||
||||
||||

||<br>Log|dP<br>dr||<br>O<br>2D|
|---|---|---|
|Log|~~d~~|~~d~~|
||||
||||
||||
||||






FIG. 6. Heatmaps of the logarithmic pressure gradient
against radius and time (in the Eulerian frame). The initial ablator radius, ablator-ice interface and ice-gas interface
are shown with red, green and blue dashed lines respectively.
Shown in white is the incident laser power (arbitrary scale).


reasonable degree of shock timing for both designs. It



FIG. 7. (Top) Time series of the inflight aspect ratio (IFAR)
and shell radius in dashed and solid lines respectively. (Bottom) Time series of the minimum fuel adiabat for the two
optimised designs.


imploding shell is thin in-flight and therefore more easily
disrupted by the non-linear growth of Rayleigh-Taylor.
The _O_ 1 _D_ and _O_ 2 _D_ have peak IFARs of 37 and 30 respec

tively. Another key stability metric is the shell adiabat
(defined the ratio of the total pressure to the degeneracy pressure). Low adiabat shells are more compressible leading to higher final densities at stagnation. However, low adiabats are unfavourable for hydrodynamic
stability [58,59] . Fig. 7 shows that _O_ 2 _D_ has a consistently
higher adiabat than _O_ 1 _D_ . This is achieved by using a
higher picket power (4.5 TW vs 3.2 TW), as well as a
degree of shock mistiming seen in Fig. 6.



10


The trained 1D and 2D surrogate models are also a
useful tool to visualise and understand the effect of a single design decision. We can optimise over the surrogate
model’s (mean) prediction of the objective, while holding
one input parameter at a fixed value. The results from
this investigation are shown in Fig. 9. These results show
some intuitive findings when comparing the predictions
for 1D and 2D. For example, in 2D decreasing picket
power below a threshold rapidly decreases performance.
Low picket power creates a low adiabat implosion which
is more prone to hydrodynamic instability growth. In a
similar vein, thinner ice is more disrupted by the growth
of ablator surface perturbations - the key source of degradation in our 2D simulations. Therefore, we can see from
these trends that the 2D surrogate has found regions of
input space which are robust to the level hydrodynamic
instabilities that was seeded. Amplifying the level of ablator density perturbation applied would push the trends
further, heading towards smaller capsules, with thicker
ice and higher picket power/adiabat.












|3<br>2<br>0 5|Col2|
|---|---|
|0<br>5<br><br>2<br>3|10<br>15<br>20<br>2|


FIG. 8. Bang time conditions (mass density _ρ_ and electron
temperature _T_ _e_ ) from 2D Chimera simulations of the optimised 1D and 2D designs (top and bottom respectively).


As expected from analysing the stability in 1D, multidimensional Chimera simulations show that the _O_ 1 _D_ design degrades far more than the _O_ 2 _D_ design in the presence of hydrodynamic instabilities. This is particularly
evident in the bangtime density and temperature distribution as shown in Fig. 8. The DT shell in the _O_ 1 _D_ design
is destroyed by the growth of hydrodynamic instabilities,
whereas the _O_ 2 _D_ design keeps the shell relatively intact.
The OMEGA beam mode (averaged into 2D _r, θ_ geometry) is particularly evident in the _O_ 1 _D_ fuel morphology.
It is clear the _O_ 2 _D_ design (while similar in many design
parameters to _O_ 1 _D_ ) is more robust to hydrodynamic instabilities.




|3<br>2<br>1<br>5|Col2|
|---|---|
|5<br>1<br>2<br>3|10<br>15<br>2|


FIG. 9. Neural network ensemble surrogate model predictions of the optimal performance at a given value of design
parameter. Each subplot shows the variation of 1 of the 8
design parameters, where the other 7 design parameters have
been optimised over. The 1D and 2D surrogate predictions
are shown in red and blue respectively. Calibrated error bars
are shown.


11



**B.** **Performance of optimal designs at 2 MJ scale**


In this work, the design objective was devised such
that the optimal targets hydrodynamically scaled to a
2 MJ laser driver would ignite and produce high gain.
In particular, via a multi-fidelity approach, we aimed to
find targets that were robust to hydrodynamic instabilities seeded at the ablator surface. In this section, we
investigate the scaled design performance via direct simulation at the 2 MJ scale with alpha heating physics included. We used the particle Monte Carlo burn package
in Chimera as described in Tong _et al._ [25] . This model
was extended to include the additional effect of fuel depletion via a burn fraction advected around with the DT
material which was used to reduce the reaction rate.
Hydrodynamic scaling by scale factor _S_ involves the
following dimensional scalings [19] :


_R →_ _S × R,_

_t →_ _S × t,_


_E →_ _S_ [3] _× E,_


Where _R_, _t_ and _E_ are the spatial, temporal and energetic scales. However, due to non-hydrodynamic physics
(e.g. radiation and thermal transport) naively applying
this scaling to initial conditions does not lead to hydrodynamically equivalent implosions. Nora _et al._ [19] note
additional changes to the laser pulse and target dimensions are required to restore hydrodynamic equivalency,
mainly exchange of ablator mass for DT ice. Here we
will not perform this re-optimisation but instead apply
hydro-scaling to the 2D simulations at a restart time
approaching stagnation, making the assumption that a
hydro-equivalent implosion can be achieved up to this
time.
First, both designs have sufficient ignition margin to
ignite in 2D at the 2 MJ energy scale. However, the
yield amplification due to alpha heating (defined as the
ratio of neutron yields burn-on to burn-off) are very different with values of 17 and 217 for the _O_ 1 _D_ and _O_ 2 _D_ designs respectively. From the burn-off areal densities, one
might expect ignited designs to achieve burn fractions of
16% and 19% respectively, based on Eq. (2). However
in the burn-on simulations, burn fractions of 0.9% and
15% were achieved. This shows canonical 1D burn propagation was not achieved in the _O_ 1 _D_ design due to large
asymmetries in the confining shell.
Fig. 10 shows the temporal evolution of the burn characteristics for the two designs. Comparing burn-off and
-on results for the same design shows higher burn rates
and temperatures but lower areal densities. The lower
areal densities are due to the rapid expansion driven by
increased hotspot pressure from alpha heating. Now contrasting the two designs, the strongly burning _O_ 2 _D_ design
shows super-exponential growth the burn rate, shown by
the solid red curve which has positive second derivative
in the rise on the log-linear scale. Burn-averaged ion
temperature also grows rapidly during this time up to a



FIG. 10. Time series of key burn parameters: burn rate (DT
fusion reactions per unit time), burn-averaged ion temperature and areal density (arithmetic average in solid angle). The
_O_ 1 _D_ and _O_ 2 _D_ design results are shown in the top and bottom
subplots respectively. Burn-on and -off are indicated by solid
and dashed lines.


peak of 22 keV. During the fall of the burn ( _t >_ 15 _._ 1
ns), burn-averaged ion temperature shows a second increase as shock heating of the remaining low density,
free-falling shell [60] produces large ion temperatures (10s
keV). This resembles the hohlraum re-heating [61] phenomena observed on the National Ignition Facility for ignition experiments. However this free-falling DT heating
does not contribute significantly to increased burn. The
_O_ 1 _D_ burn-on simulation shows less pronounced increases
in ion temperature and burn rate reflecting the truncated burn propagation. This can be attributed to poor
hotspot confinement from thin regions in the shell. This
allows the hotspot to form aneurysms through holes in
the shell [62,63] .

The differing degree of burn propagation leads to very
different conditions at the time of peak neutron production, as shown in Fig. 11. The strongly burning _O_ 2 _D_
design has become a near uniform ball of burning and expanding DT. High localised burn up fraction, Φ _∼_ 20%,
leads to reduced reaction rate, _R_ _DT_, in the core of the
burning plasma, where temperatures exceed 30 keV. In
contrast, the _O_ 1 _D_ design shows a largely perturbed shell
into which burn has not propagated. Hotspot fuel has
blown out through holes in the shell showing it has lost










12














|∈ T<br>r r<br>N(T, u) = 0 with pr<br>r<br>en the solution is an op<br>erator learning method<br>ors [27], Graph Kernel<br>designed to approxima<br>rest x ∈ R2 in the P<br>0<br>e PDE solution u has<br>g the DT interface x(t).|Col2|
|---|---|
|||

|f there exists a unique<br>= u(T ). This is the fr<br>r<br>ks (DeepONets) [30, 44<br>rnel Networks [47], wh<br>ider an infinitesimal La<br>the DT interface at tim<br>m of a parameterized<br>xistence of a parametri|Col2|
|---|---|
|||


FIG. 11. Bang time conditions (mass density _ρ_, electron temperature _T_ _e_, volumetric DT reaction rate _R_ _DT_ and burn up
fraction Φ) from 2D burn-on Chimera simulations at 2 MJ energy scale of the optimised 1D and 2D designs (left and right
columns respectively).



confinement before burn of the dense fuel could be established. This results in the _O_ 1 _D_ design having far lower
performance than the _O_ 2 _D_ design, as discussed above.
It is clear from these direct numerical simulations at
the 2 MJ scale that the objective function used (Eq. (3a))
and the multi-fidelity methodology was able to design
high gain designs at a higher energy scale than simulated.
The optimised 2D design allowed for a more robust burn
due to its better stability leading to a more uniformly
confining shell.


**V.** **FUTURE WORK**


In this paper we describe a framework for performing
multi-fidelity optimisation around the Chimera simulation code. This provides many avenues for future work,
using Chimera’s full capabilities as a radiation magnetohydrodynamics code. For example, additional fidelities
can be introduced such as 3D simulation [23], the modelling
of cross-beam energy transfer [64] or the use of experimental data. It is also important to introduce further sources
of perturbations, such as DT ice roughness [65,66] or capsule stalk [67], to ensure optimised designs are robust to instabilities derived from experimentally realistic perturba


tions. There are also magnetically driven inertial fusion
schemes (such as MagLIF) for which optimal designs at
higher driver energies are an active area of research [68,69] .
We have operated with a relatively small design space
of 8 parameters. Scaling to larger design spaces is necessary to explore a greater variety of possible designs.
Some key additional parameters being considered for
next generation inertial fusion designs relate to target
layers (dopants, foams) [70,71] and laser parameters (beam
size, zooming, bandwidth) [72] . However, the curse of
dimensionality means that larger design spaces require
larger numbers of sample points to train the surrogate.
Additionally, the global optimisation of the acquisition
function becomes increasingly difficult in higher dimensions. This will require new optimisation strategies, such
as the splitting into smaller sub-problems [21] or using
gradient-based optimisation methods directly from the
simulator [73,74] .


**VI.** **CONCLUSIONS**


In this work we show that machine learning models
can use simulation data from multiple fidelities to optimise designs in the context of laser direct drive inertial


13



fusion. In particular, we show that a large number of 1D
simulations, with inherently no hydrodynamic instability
growth, could be used to narrow down the search space
for 2D simulations. It was found that an ensemble of
neural network based surrogate models was effective at
transferring this information between fidelities. This surrogate model also enabled active learning and Bayesian
optimisation methods to automatically tune the laser and
target design parameters. These methods were robust to
noise introduced to the 2D simulation results via randomised ablator density perturbations.

The optimal 2D design was more hydrodynamically
stable than the optimal 1D design and showed only weak
degradation in performance at the 25 kJ energy scale.
Hydrodynamically scaled simulations for a 2 MJ laser
driver including alpha heating physics showed large differences in the achieved yield amplification, 17 and 217
for the _O_ 1 _D_ and _O_ 2 _D_ designs respectively. The _O_ 1 _D_ design failed to propagate burn into the dense fuel due
to loss of confinement from hydrodynamic instability
growth. In contrast, _O_ 2 _D_ design was highly performant
at the 2 MJ scale. This demonstrates that our devised objective function and multi-fidelity design framework correctly identified a high-gain scaled-up design, robust to
the presence of hydrodynamic instabilities.


**ACKNOWLEDGEMENTS**


This research received support through Schmidt Sciences, LLC, the International Atomic Energy Authority
(IAEA) AI for Fusion Coordinated Research Project and
the Imperial College Research Fellowship program.

This work made extensive use of the Imperial College London RCS HPC systems and the
ARCHER2 [75] UK National Supercomputing Service
(https://www.archer2.ac.uk).
The authors have benefited from many useful discussions with individuals at I-X, the Australian National
University and Data61. We also thank Dr Brian D Appelbe for his helpful review of this work.


**Appendix A: Ensemble Model Uncertainty Calibration**


While an ensemble of ML models can give a distribution of predictions at a point, it is not guaranteed that
the statistics of these predictions is representative of the
data it was trained/tested on. Often the minimisation of
a mean squared error loss ensures the mean prediction is
close to the data, however the ensemble variance might
be miscalibrated.

Following Ledda _et al._ [76], we aimed to test our ensemble
model calibration by computing the frequency at which
test data lies within a given confidence interval of the



ensemble model. This is defined mathematically as:



1
_f_ _α_ =
_N_ _D_



� _I_ [ _Y_ _i_ _> µ_ ( _X_ _i_ ) _−_ _z_ _α_ _σ_ ( _X_ _i_ ) & (A1)


_i∈D_



_Y_ _i_ _< µ_ ( _X_ _i_ ) + _z_ _α_ _σ_ ( _X_ _i_ )] _,_



1 + _α_
_z_ _α_ = Φ _[−]_ [1] � 2



(A2)
�



Where _f_ _α_ is the frequency of test data _D_ in confidence
interval _α_, _X_ _i_ _, Y_ _i_ are data from test data _D_ which is
size _N_ _D_, _µ_ ( _X_ ) and _σ_ ( _X_ ) are the ensemble model predictions of mean and standard deviation at input _X_, _I_ is
the indicator function, and Φ _[−]_ [1] is the inverse cumulative
probability function of the normal distribution.
Performing this analysis with both our 1D and 2D surrogate ensembles revealed that the trained models were
typically ‘over-confident’ in their predictions, i.e. the error bars of the ensemble was too small. Fig. 12 shows
this as the uncalibrated curves lie below the perfect calibration line. It was found that applying a constant scale







FIG. 12. Plot of expected frequency vs observed frequency,
_f_ _α_, for both 1D and 2D ensemble surrogate models, as described in Section II C 2. For each case, 10 random resamplings (75/25 split) of the complete dataset as used to
create testing data. The calibrated curves are created by applying a constant multiplier to _σ_ ( _X_ ) when computing _f_ _α_ .


_∼_
factor ( 3.5) on the model uncertainties regained close
to perfect calibration. This calibration constant ( _c_ _v_ ) was
then included into our ensemble model predictions as follows:


_Y_ _i_ ( _X_ ) _→_ _[√]_ ~~_c_~~ _v_ ~~_Y_~~ _i_ ( _X_ ) + (1 _−_ _[√]_ ~~_c_~~ _v_ ~~)~~ _Y_ [¯] ( _X_ ) (A3)


Where _Y_ _i_ ( _X_ ) is the _i_ -th models prediction at input _X_
and _Y_ [¯] ( _X_ ) is the mean prediction over the ensemble. This
transformation ensures that the model mean is unaffected
but the variances are scaled by _c_ _v_ .


**REFERENCES**


1 Stefano Atzeni and J¨urgen Meyer-ter Vehn. _The Physics of In-_
_ertial Fusion:_ _BeamPlasma Interaction, Hydrodynamics, Hot_
_Dense Matter_ . Oxford University Press, Oxford, 2004.
2 John Lindl. Development of the indirect-drive approach to inertial confinement fusion and the target physics basis for ignition
and gain. _Physics of plasmas_, 2(11):3933–4024, 1995.
3 RS Craxton, KS Anderson, TR Boehly, VN Goncharov,
DR Harding, JP Knauer, RL McCrory, PW McKenty, DD Meyerhofer, JF Myatt, et al. Direct-drive inertial confinement fusion:
A review. _Physics of Plasmas_, 22(11), 2015.
4 H Abu-Shawareb, R Acree, P Adams, J Adams, B Addis,
R Aden, P Adrian, BB Afeyan, M Aggleton, L Aghaian, et al.
Lawson criterion for ignition exceeded in an inertial fusion experiment. _Physical Review Letters_, 129(7):075001, 2022.
5 A. B. Zylstra, A. L. Kritcher, O. A. Hurricane, D. A. Callahan,
J. E. Ralph, D. T. Casey, A. Pak, O. L. Landen, B. Bachmann,
K. L. Baker, L. Berzak Hopkins, S. D. Bhandarkar, J. Biener,
R. M. Bionta, N. W. Birge, T. Braun, T. M. Briggs, P. M. Celliers, H. Chen, C. Choate, D. S. Clark, L. Divol, T. D¨oppner,
D. Fittinghoff, M. J. Edwards, M. Gatu Johnson, N. Gharibyan,
S. Haan, K. D. Hahn, E. Hartouni, D. E. Hinkel, D. D. Ho, M. Hohenberger, J. P. Holder, H. Huang, N. Izumi, J. Jeet, O. Jones,
S. M. Kerr, S. F. Khan, H. Geppert Kleinrath, V. Geppert Kleinrath, C. Kong, K. M. Lamb, S. Le Pape, N. C. Lemos, J. D. Lindl,
B. J. MacGowan, A. J. Mackinnon, A. G. MacPhee, E. V. Marley, K. Meaney, M. Millot, A. S. Moore, K. Newman, J.-M. G.
Di Nicola, A. Nikroo, R. Nora, P. K. Patel, N. G. Rice, M. S. Rubery, J. Sater, D. J. Schlossberg, S. M. Sepke, K. Sequoia, S. J.
Shin, M. Stadermann, S. Stoupin, D. J. Strozzi, C. A. Thomas,
R. Tommasini, C. Trosseille, E. R. Tubman, P. L. Volegov, C. R.
Weber, C. Wild, D. T. Woods, S. T. Yang, and C. V. Young. Experimental achievement and signatures of ignition at the national
ignition facility. _Phys. Rev. E_, 106:025202, Aug 2022.
6 A. L. Kritcher, A. B. Zylstra, D. A. Callahan, O. A. Hurricane, C. R. Weber, D. S. Clark, C. V. Young, J. E. Ralph,
D. T. Casey, A. Pak, O. L. Landen, B. Bachmann, K. L. Baker,
L. Berzak Hopkins, S. D. Bhandarkar, J. Biener, R. M. Bionta,
N. W. Birge, T. Braun, T. M. Briggs, P. M. Celliers, H. Chen,
C. Choate, L. Divol, T. D¨oppner, D. Fittinghoff, M. J. Edwards, M. Gatu Johnson, N. Gharibyan, S. Haan, K. D. Hahn,
E. Hartouni, D. E. Hinkel, D. D. Ho, M. Hohenberger, J. P.
Holder, H. Huang, N. Izumi, J. Jeet, O. Jones, S. M. Kerr, S. F.
Khan, H. Geppert Kleinrath, V. Geppert Kleinrath, C. Kong,
K. M. Lamb, S. Le Pape, N. C. Lemos, J. D. Lindl, B. J.
MacGowan, A. J. Mackinnon, A. G. MacPhee, E. V. Marley,
K. Meaney, M. Millot, A. S. Moore, K. Newman, J.-M. G.
Di Nicola, A. Nikroo, R. Nora, P. K. Patel, N. G. Rice, M. S.
Rubery, J. Sater, D. J. Schlossberg, S. M. Sepke, K. Sequoia,
S. J. Shin, M. Stadermann, S. Stoupin, D. J. Strozzi, C. A.
Thomas, R. Tommasini, C. Trosseille, E. R. Tubman, P. L. Volegov, C. Wild, D. T. Woods, and S. T. Yang. Design of an inertial fusion experiment exceeding the lawson criterion for ignition.
_Phys. Rev. E_, 106:025201, Aug 2022.
7 R Betti and OA Hurricane. Inertial-confinement fusion with
lasers. _Nature Physics_, 12(5):435–448, 2016.
8 O. A. Hurricane, D. A. Callahan, D. T. Casey, P. M. Celliers,
C. Cerjan, E. L. Dewald, T. R. Dittrich, T. D¨oppner, D. E.
Hinkel, L. F. B. Hopkins, J. L. Kline, S. Le Pape, T. Ma, A. G.
Macphee, J. L. Milovich, A. Pak, H.-S. Park, P. K. Patel, B. A.
Remington, J. D. Salmonson, P. T. Springer, and R. Tommasini.
Fuel gain exceeding unity in an inertially confined fusion implosion. _Nature_, 506:343–348, 2014.
9 Brian M Haines, Daniel S Clark, Christopher R Weber, M John
Edwards, Steven H Batha, and John L Kline. Cross-code comparison of the impact of the fill tube on high yield implosions on
the national ignition facility. _Physics of Plasmas_, 27(8), 2020.
10 D. S. Clark, C. R. Weber, J. L. Milovich, A. E. Pak, D. T.



14


Casey, B. A. Hammel, D. D. Ho, O. S. Jones, J. M. Koning,
A. L. Kritcher, M. M. Marinak, L. P. Masse, D. H. Munro, M. V.
Patel, P. K. Patel, H. F. Robey, C. R. Schroeder, S. M. Sepke,
and M. J. Edwards. Three-dimensional modeling and hydrodynamic scaling of national ignition facility implosions. _Physics of_
_Plasmas_, 26(5):050601, 2019.
11 K McGlinchey, BD Appelbe, AJ Crilly, JK Tong, CA Walsh, and
JP Chittenden. Diagnostic signatures of performance degrading
perturbations in inertial confinement fusion implosions. _Physics_
_of Plasmas_, 25(12):122705, 2018.
12 Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical
bayesian optimization of machine learning algorithms. _Advances_
_in neural information processing systems_, 25, 2012.
13 Marc C Kennedy and Anthony O’Hagan. Bayesian calibration
of computer models. _Journal of the Royal Statistical Society:_
_Series B (Statistical Methodology)_, 63(3):425–464, 2001.
14 Paris Perdikaris, Maziar Raissi, Andreas Damianou, Neil D
Lawrence, and George Em Karniadakis. Nonlinear information
fusion algorithms for data-efficient multi-fidelity modelling. _Pro-_
_ceedings of the Royal Society A: Mathematical, Physical and En-_
_gineering Sciences_, 473(2198):20160751, 2017.
15 R Betti, AR Christopherson, BK Spears, R Nora, A Bose,
J Howard, KM Woo, MJ Edwards, and J Sanz. Alpha heating and burning plasmas in inertial confinement fusion. _Physical_
_review letters_, 114(25):255003, 2015.
16 AR Christopherson, R Betti, S Miller, V Gopalaswamy,
OM Mannion, and D Cao. Theory of ignition and burn propagation in inertial fusion implosions. _Physics of Plasmas_, 27(5),
2020.
17 G. S. Fraley, E. J. Linnebur, R. J. Mason, and R. L. Morse. Thermonuclear burn characteristics of compressed deuterium-tritium
microspheres. _The Physics of Fluids_, 17(2):474–489, 1974.
18 TR Boehly, DL Brown, RS Craxton, RL Keck, JP Knauer,
JH Kelly, TJ Kessler, SA Kumpan, SJ Loucks, SA Letzring, et al.
Initial performance results of the omega laser system. _Optics_
_communications_, 133(1-6):495–506, 1997.
19 R. Nora, R. Betti, K. S. Anderson, A. Shvydky, A. Bose, K. M.
Woo, A. R. Christopherson, J. A. Marozas, T. J. B. Collins, P. B.
Radha, S. X. Hu, R. Epstein, F. J. Marshall, R. L. McCrory, T. C.
Sangster, and D. D. Meyerhofer. Theory of hydro-equivalent
ignition for inertial fusion and its applications to omega and the
national ignition facility. _Physics of Plasmas_, 21(5):056316, 2014.
20 George H Miller, Edward I Moses, and Craig R Wuest. The
national ignition facility. _Optical Engineering_, 43(12):2841–2853,
2004.
21 V Gopalaswamy, A Lees, R Ejaz, CA Thomas, TJB Collins,
KS Anderson, W Ebmeyer, and R Betti. Automated and highly
parallelized bayesian optimization scheme for direct drive fusion
experiments on omega. _Physical Review Research_, 7(1):013009,
2025.
22 PW Hatfield, SJ Rose, and RHH Scott. The blind implosionmaker: Automated inertial confinement fusion experiment design. _Physics of Plasmas_, 26(6):062706, 2019.
23 J. P. Chittenden, B. D. Appelbe, F. Manke, K. McGlinchey, and
N. P.L. Niasse. Signatures of asymmetry in neutron spectra and
images predicted by three-dimensional radiation hydrodynamics
simulations of indirect drive implosions. _Physics of Plasmas_,
23(5), 2016.
24 C. A. Walsh, J. P. Chittenden, K. McGlinchey, N. P.L. Niasse,
and B. D. Appelbe. Self-Generated Magnetic Fields in the Stagnation Phase of Indirect-Drive Implosions on the National Ignition Facility. _Physical Review Letters_, 118(15):1–5, 2017.
25 J.K. Tong, K. McGlinchey, B.D. Appelbe, C.A. Walsh, A.J.
Crilly, and J.P. Chittenden. Burn regimes in the hydrodynamic
scaling of perturbed inertial confinement fusion hotspots. _Nu-_
_clear Fusion_, 59(8):086015, jun 2019.
26 AJ Crilly, BD Appelbe, OM Mannion, CJ Forrest, JP Knauer,
DJ Schlossberg, EP Hartouni, AS Moore, and JP Chittenden.
Neutron backscatter edges as a diagnostic of burn propagation.
_Physics of Plasmas_, 29(6):062707, 2022.


27 AJ Crilly, NPL Niasse, AR Fraser, DA Chapman, KM McLean,
SJ Rose, and JP Chittenden. Spk: A fast atomic and microphysics code for the high-energy-density regime. _arXiv preprint_
_arXiv:2211.16464_, 2022.
28 A stabilized runge–kutta–legendre method for explicit supertime-stepping of parabolic and mixed equations. _Journal of Com-_
_putational Physics_, 257:594–626, 2014.
29 Stanley Skupsky and Kotik Lee. Uniformity of energy deposition
for laser driven fusion. _Journal of Applied Physics_, 54(7):3662–
3671, 1983.
30 V Gopalaswamy, R Betti, JP Knauer, A Lees, D Patel,
AR Christopherson, IV Igumenshchev, D Cao, KS Anderson,
A Shvydky, et al. Using statistical modeling to predict and understand fusion experiments. _Physics of Plasmas_, 28(12), 2021.
31 S Skupsky and RS Craxton. Irradiation uniformity for highcompression laser-fusion experiments. _Physics of Plasmas_,
6(5):2157–2163, 1999.
32 VN Goncharov, S Skupsky, TR Boehly, JP Knauer, P McKenty,
VA Smalyuk, RPJ Town, OV Gotchev, R Betti, and DD Meyerhofer. A model of laser imprinting. _Physics of Plasmas_,
7(5):2062–2068, 2000.
33 Dhrumir Patel, JP Knauer, D Cao, R Betti, Ryan Nora, A Shvydky, V Gopalaswamy, A Lees, Siddharth Sampat, WR Donaldson, et al. Effects of laser bandwidth in direct-drive highperformance dt-layered implosions on the omega laser. _Physical_
_Review Letters_, 131(10):105101, 2023.
34 JN Shiau, EB Goldman, and CI Weng. Linear stability analysis of laser-driven spherical implosions. _Physical Review Letters_,
32(7):352, 1974.
35 Jingyi Wang, N Chiang, Andrew Gillette, and J Luc Peterson. A
multifidelity bayesian optimization method for inertial confinement fusion design. _Physics of Plasmas_, 31(3), 2024.
36 Thomas J Santner, Brian J Williams, William I Notz, and Brain J
Williams. _The design and analysis of computer experiments_,
volume 1. Springer, 2003.
37 Christopher KI Williams and Carl Edward Rasmussen. _Gaussian_
_processes for machine learning_, volume 2. MIT press Cambridge,
MA, 2006.
38 David Duvenaud. The kernel cookbook: Advice on
covariance functions. _URL_ _https://www._ _cs._ _toronto._
_edu/duvenaud/cookbook_, 2014.
39 Brian K Spears, Scott Brandon, Dan T Casey, John E Field,
Jim A Gaffney, Kelli D Humbird, Andrea L Kritcher, Michael KG
Kruse, Eugene Kur, Bogdan Kustowski, et al. Predicting fusion
ignition at the national ignition facility with physics-informed
deep learning. _Science_, 389(6761):727–731, 2025.
40 Rahman Ejaz, Varchas Gopalaswamy, A Lees, C Kanan, D Cao,
and R Betti. Deep learning-based predictive models for laser
direct drive at the omega laser facility. _Physics of Plasmas_, 31(5),
2024.
41 Jim A Gaffney, Kelli Humbird, Andrea Kritcher, Michael Kruse,
Eugene Kur, Bogdan Kustowski, Ryan Nora, and Brian Spears.
Data-driven prediction of scaling and ignition of inertial confinement fusion experiments. _Physics of Plasmas_, 31(9), 2024.
42 Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using
deep ensembles. _Advances in neural information processing sys-_
_tems_, 30, 2017.
43 Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia
Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. _Advances in neural in-_
_formation processing systems_, 32, 2019.
44 Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel
Daulton, Ben Letham, Andrew G Wilson, and Eytan Bakshy.
Botorch: A framework for efficient monte-carlo bayesian optimization. _Advances in neural information processing systems_,
33:21524–21538, 2020.
45 Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.



15


46 Leo Breiman. Random forests. _Machine learning_, 45(1):5–32,
2001.
47 Kelli D Humbird, Jayson Luc Peterson, BK Spears, and Ryan G
McClarren. Transfer learning to model inertial confinement
fusion experiments. _IEEE Transactions on Plasma Science_,
48(1):61–70, 2019.
48 David H Munro, Peter M Celliers, Gilbert W Collins, David M
Gold, Luiz B Da Silva, Steven W Haan, Robert C Cauble,
Bruce A Hammel, and Warren W Hsing. Shock timing technique
for the national ignition facility. _Physics of Plasmas_, 8(5):2245–
2250, 2001.
49 Tom Blau, Iadine Chades, and Cheng Soon Ong. Machine learning for biological design. In _Synthetic Biology:_ _Methods and_
_Protocols_, pages 319–344. Springer, 2024.
50 Alkis Gotovos. Active learning for level set estimation. Master’s thesis, Eidgen¨ossische Technische Hochschule Z¨urich, Department of Computer Science,, 2013.
51 Sebastian Ament, Samuel Daulton, David Eriksson, Maximilian
Balandat, and Eytan Bakshy. Unexpected improvements to expected improvement for bayesian optimization. _Advances in Neu-_
_ral Information Processing Systems_, 36:20577–20612, 2023.
52 Jose Pablo Folch, Robert M Lee, Behrang Shafei, David Walz,
Calvin Tsay, Mark van der Wilk, and Ruth Misener. Combining
multi-fidelity modelling and asynchronous batch bayesian optimization. _Computers & Chemical Engineering_, 172:108194, 2023.
53 Aidan Crilly. mille-feuille: A Bayesian Optimisation wrapper for
HPC scale simulations: https://github.com/aidancrilly/millefeuille, 2025.
54 I Sobol. The distribution of points in a cube and the accurate
evaluation of integrals (in russian) zh. _Vychisl. Mat. i Mater._
_Phys_, 7:784–802, 1967.
55 JD Lindl, SW Haan, OL Landen, AR Christopherson, and
R Betti. Progress toward a self-consistent set of 1d ignition capsule metrics in icf. _Physics of Plasmas_, 25(12), 2018.
56 S. P. Regan, R. Epstein, B. A. Hammel, L. J. Suter, J. Ralph,
H. Scott, M. A. Barrios, D. K. Bradley, D. A. Callahan, C. Cerjan, G. W. Collins, S. N. Dixit, T. Doeppner, M. J. Edwards,
D. R. Farley, S. Glenn, S. H. Glenzer, I. E. Golovkin, S. W.
Haan, A. Hamza, D. G. Hicks, N. Izumi, J. D. Kilkenny, J. L.
Kline, G. A. Kyrala, O. L. Landen, T. Ma, J. J. MacFarlane,
R. C. Mancini, R. L. McCrory, N. B. Meezan, D. D. Meyerhofer, A. Nikroo, K. J. Peterson, T. C. Sangster, P. Springer,
and R. P. J. Town. Hot-spot mix in ignition-scale implosions on
the nif. _Physics of Plasmas_, 19(5):056307, 2012.
57 Paul McKenna, David Neely, Robert Bingham, and Dino
Jaroszynski. _Laser-plasma_ _interactions_ _and_ _applications_ .
Springer, 2013.
58 Hideaki Takabe, K Mima, L Montierth, and RL Morse. Selfconsistent growth rate of the rayleigh–taylor instability in an ablatively accelerating plasma. _The Physics of fluids_, 28(12):3676–
3682, 1985.
59 R Betti, VN Goncharov, R ´aL McCrory, and C ´aP Verdon.
Growth rates of the ablative rayleigh–taylor instability in inertial
confinement fusion. _Physics of Plasmas_, 5(5):1446–1454, 1998.
60 R. Betti, K. Anderson, V. N. Goncharov, R. L. McCrory, D. D.
Meyerhofer, S. Skupsky, and R. P. J. Town. Deceleration phase
of inertial confinement fusion implosions. _Physics of Plasmas_,
9(5):2277–2286, 2002.
61 MS Rubery, MD Rosen, N Aybar, OL Landen, L Divol,
CV Young, C Weber, J Hammer, JD Moody, AS Moore, et al.
Hohlraum reheating from burning nif implosions. _Physical Re-_
_view Letters_, 132(6):065104, 2024.
62 Omar A Hurricane, DA Callahan, DT Casey, EL Dewald,
TR Dittrich, T D¨oppner, S Haan, DE Hinkel, LF Berzak Hopkins, O Jones, et al. Inertially confined fusion plasmas dominated
by alpha-particle self-heating. _Nature Physics_, 12(8):800–806,
2016.
63 PT Springer, OA Hurricane, JH Hammer, R Betti, DA Callahan,
EM Campbell, DT Casey, CJ Cerjan, D Cao, E Dewald, et al.
A 3d dynamic model to assess the impacts of low-mode asym

metry, aneurysms and mix-induced radiative loss on capsule performance across inertial confinement fusion platforms. _Nuclear_
_Fusion_, 59(3):032009, 2018.
64 Philip Moloney. _Multidimensional modelling of Cross-Beam En-_
_ergy Transfer for direct-drive Inertial Confinement Fusion_ . PhD
thesis, Imperial College London, 2024.
65 A Nikroo, J Bousquet, R Cook, BW McQuillan, R Paguio, and
M Takagi. Progress in 2 mm glow discharge polymer mandrel
development for nif. _Fusion science and technology_, 45(2):165–
170, 2004.
66 H Huang, K Engelhorn, K Sequoia, A Greenwood, W Sweet,
L Carlson, F Elsner, and M Farrell. Metrology feasibility study
in support of the national direct-drive program. _Fusion Science_
_and Technology_, 73(2):98–106, 2018.
67 Maria Gatu Johnson, Patrick J Adrian, KS Anderson, BD Appelbe, JP Chittenden, AJ Crilly, D Edgell, CJ Forrest, Johan A
Frenje, V Yu Glebov, et al. Impact of stalk on directly driven inertial confinement fusion implosions. _Physics of Plasmas_, 27(3),
2020.
68 DE Ruiz, PF Schmit, DA Yager-Elorriaga, MR Gomez, MR Weis,
CA Jennings, AJ Harvey-Thompson, PF Knapp, SA Slutz,
DJ Ampleford, et al. Exploring the parameter space of maglif
implosions using similarity scaling. ii. current scaling. _Physics of_
_Plasmas_, 30(3), 2023.
69 Andrew Alexander, Laura Robin Benedetti, Indrani Bhattacharyya, Jared Bowen, June Cabatu, Virgil Cacdac, Chhavi
Chhavi, Chiatai Chen, Karen Chen, Dan Clark, et al. Affordable, manageable, practical, and scalable (amps) high-yield and



16


high-gain inertial fusion. _arXiv preprint arXiv:2504.10680_, 2025.
70 SA MacLaren, DD-M Ho, OA Hurricane, EL Dewald, DA Martinez, RE Tipton, JE Pino, CV Young, HW Xu, CW Kong, et al.
A pushered capsule implosion as an alternate approach to the ignition regime for inertial confinement fusion. _Physics of Plasmas_,
28(12), 2021.
71 RW Paddock, TS Li, E Kim, JJ Lee, H Martin, RT Ruskov,
S Hughes, SJ Rose, CD Murphy, RHH Scott, et al. Energy gain of
wetted-foam implosions with auxiliary heating for inertial fusion
studies. _Plasma Physics and Controlled Fusion_, 66(2):025005,
2023.
72 DH Froula, C Dorrer, A Cola¨ıtis, DH Edgell, RK Follett, EM Hill,
IV Igumenshchev, AL Milder, JP Palastro, RC Shah, et al. A
future of inertial confinement fusion without laser-plasma instabilities. _Physics of Plasmas_, 32(5), 2025.
73 Rika Antonova, Jingyun Yang, Krishna Murthy Jatavallabhula,
and Jeannette Bohg. Rethinking optimization with differentiable
simulation from a global perspective. In _Conference on robot_
_learning_, pages 276–286. PMLR, 2023.
74 Archis S Joglekar. Generative neural reparameterization for
differentiable pde-constrained optimization. _arXiv preprint_
_arXiv:2410.12683_, 2024.
75 George Beckett, Josephine Beech-Brandt, Kieran Leach, Z¨oe
Payne, Alan Simpson, Lorna Smith, Andy Turner, and Anne
Whiting. Archer2 service description, December 2024.
76 Emanuele Ledda, Giorgio Fumera, and Fabio Roli. Dropout injection at test time for post hoc uncertainty quantification in
neural networks. _Information Sciences_, 645:119356, 2023.


